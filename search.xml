<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Dubbo的容错机制和负载均衡]]></title>
    <url>%2F2017%2F11%2F24%2Fcluster%26loadbalance%2F</url>
    <content type="text"><![CDATA[前面介绍dubbo中提到过这两个内容，但我觉得有必要再分出来写一下 容错机制机制在集群调用失败时，Dubbo提供了多种容错方案，缺省为failover重试。可以自行拓展集群容错策略 原理 Invoker是Provider的一个可调用Service的抽象，Invoker封装了Provider地址及Service接口信息 Directory代表多个Invoker，可以把它看作List，但与Invoker不同的是，它的值可能是动态变化的，比如注册中心推送变更 Cluster将Directory中的多个Invoker伪装成一个Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个 Router负责从多个Invoker中按路由规则选出子集，比如读写分离，应用隔离等 LoadBalance负责从多个Invoker中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选 分类 Failover Cluster 失败自动切换，若出现失败，重试其他服务器。可通过retries=”2”来设置重试次数（不含第一次） &lt;dubbo:method name=&quot;findFoo&quot; retries=&quot;2&quot; /&gt; Failfast Cluster 快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录 Failsafe Cluster 失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。 Failback Cluster 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。 Forking Cluster 并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作。但需要浪费更多服务资源。可通过forks=”2” 来设置最大并行数。 Broadcast Cluster 广播调用所有提供者，逐个调用，任意一台报错则报错，通常用于通知所有提供者更新缓存或日志等本地资源信息。配置 提供方&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt; 消费方&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt; 负载均衡机制在集群负载均衡时，Dubbo提供了多种均衡策略，缺省为random随机调用可以自行拓展负载均衡策略 负载均衡策略 Random LoadBalance。随机。按权重设置随机概率 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重 RoundRobin LoadBalance.轮循,按公约后的权重设置轮循比率 存在慢的提供者累计请求的问题，比如：第二台机器很慢，但没挂，当请求遇到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 解决办法：结合权重，把第二台的权重设置低一点 LeastActive LoadBalance.最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。 使慢的提供者收到更少请求，因为越慢的提供者的调用前后技术差会越大。 ConsistentHash LoadBalance.一致性Hash，相同参数的请求总是发到同一提供者 当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其他提供者，不会引起剧烈变动 缺省值对第一个参数Hash，如果要修改，请配置&lt;dubbo:parmeter key=&quot;hash.arguments&quot; value=&quot;0.1&quot;/&gt; 缺省用160份虚拟节点，如果要修改，请配置&lt;dubbo:parmeter key=&quot;hash.nodes&quot; value=&quot;320&quot;/&gt;配置服务端服务级别&lt;dubbo:service interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 客户端级别&lt;dubbo:reference interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 服务端方法级别123&lt;dubbo:service interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot; / &gt;&lt;/dubbo:service&gt; 客户端方法级别123&lt;dubbo:reference interface=&quot;...&quot;&gt; &lt;dubbo:method name=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt;&lt;/dubbo:reference&gt;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring]]></title>
    <url>%2F2017%2F11%2F24%2FSpring%2F</url>
    <content type="text"><![CDATA[Spring是一个开放源代码的设计层面框架，他解决的是业务逻辑层和其他各层的松耦合问题，因此它将面向接口的编程思想贯穿整个系统应用. 关键字 一个IOC和AOP架构。 容器 一站式架构 Spring的核心IOC（控制反转）概述传统的java开发模式，要想用一个对象，我们一般是new一个对象或者通过反射获得一个对象，而用了spring框架之后，spring底层利用工厂模式为我们创建我们所需要的对象，我们不需要自己手动去创建，直接调用spring为我们提供的对象即可。 使用 如何实例化一个对象 使用类构造器` 使用静态工厂方法 12// 调用Bean2Factory的getBean2方法得到bean2&lt;bean id=&quot;bean2&quot; class=&quot;cc.misaniy.spring.Bean2Factory factory-method=&quot;getBean2&quot;/&gt; 使用实例化工厂方法 123//先创建工厂实例bean3Factory,再通过工厂实例创建目标bean实例&lt;bean id=&quot;bean3Factory&quot; class=&quot;cc.misaniy.spring.Bean3Factory&quot; /&gt;&lt;bean id=&quot;bean3&quot; factory-bean=&quot;bean3Factory&quot; factory-method=&quot;getBean3&quot; /&gt; 当使用spring的时候我们不需要关心通过何种方式实例化一个对象，spring通过控制反转机制（IOC）为我们创建一个对象 原理 bean的生命周期 instantiate bean 对象实例化 properties 封装属性 如果Bean实现BeanNameAware执行setBeanName 如果Bean实现BeanFactoryAware或者ApplicationContextAware设置工厂setBeanFactory或者上下文对象setApplicationContext 如果存在类实现BeanPostProcessor(后处理Bean),执行postProcessBeforeInitialization，BeanPostProcessor接口提供钩子函数,用来动态扩展修改Bean（程序自动调用后处理Bean） 如果Bean实现InitializingBean执行afterPropertiesSet 调用 执行初始化方法init 如果存在类实现BeanPostProcessor(处理Bean)，执行postProcessAfterInitialization 执行业务处理 如果Bean实现DisposableBean执行destroy 调用指定销毁方法customerDestroy bean的作用域 singleton:当一个bean的作用域为singleton,那么Spring IOC容器中只会存在一个共享的bean实例，并且所有对bean的请求，只要id与该bean定义相匹配，则只会返回bean的同一实例 prototype:Prototype作用域的bean会导致在每次对该bean请求（将其注入到另一个bean中，或者以程序的方式调用容器的getBean（）方法）时都会创建一个新的Bean实例。根据经验，对所有有状态的bean应该使用prototype作用域，而对无状态的bean则应该使用singleton作用域 request：在一次HTTP请求中，一个bean定义对应一个实例；即每次HTTP请求将会有各自的bean实例，它们依据某个bean定义创建而成。该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP session 中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global session：在一个全局的HTTP session 中，一个bean定义对应一个实例。典型情况下，仅在使用portlet context的时候有效。该作用域仅在基于web的Spring ApplicationContext情形下有效。 BeanFactory接口和ApplicationContext接口的区别 ApplicationContext 接口继承BeanFactory接口，Spring核心工厂是BeanFactory，BeanFactory采取延迟加载，第一次getBean时才会初始化Bean，ApplicationContext是会在加载配置文件时初始化Bean。 ApplicationContext是对BeanFactory的扩展，它可以进行国际化处理、时间传递和bean自动装配以及各种不同应用层的Context实现。 DI（依赖注入）概述Spring使用java bean对象的set方法或者带参数的构造方法为我们在创建所需对象时将其属性自动设置所需要的值的过程就是依赖注入的过程 依赖注入的方式 构造器注入 通过元素完成注入 set方法注入 通过元素完成注入【开发中常用方式】 AOP（面向切面编程）概述在面向对象编程（OOP）思想中，我们将事物纵向抽象成一个个的对象。而在OOP编程思想中，我们将一个个对象某些类型的方面横向抽象成一个切面，对这个切面进行一些如权限验证、事务管理、记录日志等公用操作处理的过程就是面向切面编程。 各种概念 切面(Aspect) 一个关注点的模块化，这个关注点可能会横切多个对象。事务管理是J2EE应用中的一个关于横切关注点的很好的例子。在Spring AOP中，切面可以使用通用类（基于模式的风格）或者在普通类中以@Aspect注解（@AspectJ风格）来实现。 连接点（Joinpoint） 在程序执行过程中某个特定的点，比如某方法调用的时候或者处理异常的时候。在Spring AOP中，一个连接点总是代表一个方法的执行。通过声明一个org.aspect.lang.JoinPoint类型的参数可以使通知（Advice）的主题部分获得连接点信息。 通知（Advice） 在切面的某个特定的连接点(Joinpoint)上执行的动作。通知有各种类型，其中包括”around”、”before”、”after”等通知。许多AOP框架，包括Spring，都是以拦截器坐通知模型，并维护一个连接点为中心的拦截器链。 切入点（Pointcut） 匹配连接点（Jointpoint）的断言。通知和一个切入点表达式关联，并在满足这个切入点的连接点上运行（例如，当执行某个特定名称的方法时）。切入点表达式如何和连接点匹配是AOP的核心：Spring缺省使用AspectJ切入点语法。 引入（Introduction） 也被称为内部类型声明（inter-type declaration）。声明额外的方法或者某个类型的字段。Spring允许引入新的接口（以及一个对应的实现）到任何被代理的对象。例如，你可以使用一个引入来使bean实现isModified接口，以便简化缓存机制。 目标对象（Target Object） 被一个或者多个切面（aspect）所通知(advise)的对象。也有人把它叫做被通知对象（advised）。既然Srping AOP是通过运行时代理实现的，这个对象永远是一个被代理（proxied）对象。 AOP代理（AOP Proxy） AOP框架创建的对象，用来实现切面契约（aspect contract）（包括通知方法执行等功能）。在Spring中，AOP代理可以是JDK动态代理或者CGLIB代理。注意：Spring2.0最新引入的基于模式（schema-based）风格和@AspectJ注解风格的切面声明，对于使用这些风格的用户来说，代理的创建是透明的。 织入（Weaving） 把切面（aspect）连接到其他的应用程序类型或者对象上，并创建一个被通知（advised）对象。这些可以在编译时（例如使用AspectJ编译器），类加载时和运行时完成。Spring和其他纯JAVA AOP 框架一样，在运行时完成织入。 底层实现动态代理 jdk：默认。面向接口的 cglib：面向实现类的 应用场景管理事务编程式事务 通过编程的方式管理事务 比较灵活，难维护 声明式事务 将业务代码和事务分离，注解和XML配置 XML配置 123456789101112131415161718192021222324252627&lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;configLocation&quot; value=&quot;classpath:hibernate.cfg.xml&quot;/&gt; &lt;property name=&quot;configurationClass&quot; value=&quot;org.hibernate.cfg.AnnotationConfiguration&quot; /&gt;&lt;/bean&gt;&lt;!-- 定义事务管理器（声明式事务） --&gt;&lt;bean id=&quot;transactionManger&quot; class=&quot;org.springframework.orm.hibernate3.HibernateTransactionManager&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot; /&gt;&lt;/bean&gt;&lt;!-- 配置DAO --&gt;&lt;bean id=&quot;userDaoTarget&quot; class=&quot;com.blusky.spring.dao.UserDaoImpl&quot;&gt; &lt;property name=&quot;sessionFactory&quot; ref=&quot;sessionFactory&quot;&gt;&lt;/bean&gt;&lt;bean id=&quot;userDao&quot; class=&quot;org.springframework.transaction.interceptor.TransactionProxyFactoryBean&quot;&gt; &lt;!-- 配置事务管理 --&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;transactionManager&quot; /&gt; &lt;property name=&quot;target&quot; ref=&quot;userDaoTarget&quot;/&gt; &lt;property name=&quot;proxyInterfaces&quot; value=&quot;com.bluesky.spirng.dao.GeneratorDao&quot; /&gt; &lt;!-- 配置事务属性 --&gt; &lt;property name=&quot;transactionAttributes&quot;&gt; &lt;props&gt; &lt;prop key=&quot;*&quot;&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;props&gt; &lt;/property&gt;&lt;/bean&gt; 注解 比较灵活，好维护 spring如何管理事务 事务管理器Spring并不直接管理事务，而是提供了多种事务管理器，他们将事务管理的职责委托给Hibernate或JTA等持久化机制所提供的相关平台框架的事务来实现。 基本的属性 传播行为：当事务方法被另一个事务方法调用时，必须指定事务应该如何传播 Spring定义了7种传播行为 常用的两种传播行为 SUPPORTS：如果存在一个事务，支持当前事务。如果没有事务，则非事务的执行 REQUIRED：默认的事务传播行为：如果没有事务，就新建一个事务，如果有，就加入当前事务 隔离级别：定义了一个事务可能收到其他并发事务影响的程度 并发引起的问题 脏读 脏读发生在一个事务读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读 不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了更新。 幻读 幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入了一些数据时。在随后的查询中，第一个事务就会发现多了一些原本不存在的记录 不可重复读和幻读的区别 不可重复读的重点是修改 幻读的重点在于新增或者删除 隔离级别 9种 一般使用数据库默认的隔离级别就可以 回滚规则 定义了那些异常会导致事务回滚而哪些不会 默认情况下，事务只有遇到运行期异常时才会回滚，而在遇到检查型异常时不会回滚 可以声明事务在遇到特定的检查型异常时像运行期异常那样回滚。同样，你还可以声明事务遇到特定的异常不回滚，即使这些异常是运行期异常 事务超时 为了使应用程序很好地运行，事务不能运行太长的时间。因为事务可能涉及对后端数据库的锁定，所以长时间的事务会不必要的占用数据库资源。事务超时就是事务的一个定时器，在特定时间内事务如果没有执行完毕，那么就会自动回滚，而不是一直等待其结束。 事务只读 如果事务值对祸端的数据库进行操作，数据库可以利用事务的只读特性来进行一些特定的优化 管理日志 自定义一个注解，拦截Controller 1234567891011121314151617@Target(&#123;ElementType.PARAMETER,ElementType.METHOD&#125;)@Retention（RetentionPolicy.RUNTIME）@Documentedpublic @interface SystemControllerLog&#123; String description() default &quot;&quot;;&#125;package com.annotation;import java.lang.annotation.*;// 自定义注解 拦截service@Target(&#123;ElementType.PARAMETER,ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface SystemServiceLog&#123; String description() default &quot;&quot;;&#125; 创建一个切点类 ··· package com.annotation; import com.model.Log; import com.model.User; import com.service.LogService; import com.util.DateUtil; import com.util.JSONUtil; import com.util.SpringContextHolder; import com.util.WebConstants; import org.aspectj.lang.JoinPoint; import org.aspectj.lang.annotation.*; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.stereotype.Component; import org.springframework.web.context.request.RequestContextHolder; import org.springframework.web.context.request.ServletRequestAttributes; import javax.annotation.Resource; import javax.servlet.http.HttpServletRequest; import javax.servlet.http.HttpSession; import java.lang.reflect.Method; /** 切点类 @author tiangai @since 2014-08-05 Pm 20:35 @version 1.0*/@Aspect@Componentpublic class SystemLogAspect { //注入Service用于把日志保存数据库 @Resource private LogService logService; //本地异常日志记录对象 private static final Logger logger = LoggerFactory.getLogger(SystemLogAspect. class); //Service层切点 @Pointcut(“@annotation(com.annotation.SystemServiceLog)”) public void serviceAspect() { } //Controller层切点 @Pointcut(“@annotation(com.annotation.SystemControllerLog)”) public void controllerAspect() { } /** 前置通知 用于拦截Controller层记录用户的操作 @param joinPoint 切点*/@Before(“controllerAspect()”)public void doBefore(JoinPoint joinPoint) { HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); HttpSession session = request.getSession(); //读取session中的用户 User user = (User) session.getAttribute(WebConstants.CURRENT_USER); //请求的IP String ip = request.getRemoteAddr(); try { //*========控制台输出=========*// System.out.println(&quot;=====前置通知开始=====&quot;); System.out.println(&quot;请求方法:&quot; + (joinPoint.getTarget().getClass().getName() + &quot;.&quot; + joinPoint.getSignature().getName() + &quot;()&quot;)); System.out.println(&quot;方法描述:&quot; + getControllerMethodDescription(joinPoint)); System.out.println(&quot;请求人:&quot; + user.getName()); System.out.println(&quot;请求IP:&quot; + ip); //*========数据库日志=========*// Log log = SpringContextHolder.getBean(&quot;logxx&quot;); log.setDescription(getControllerMethodDescription(joinPoint)); log.setMethod((joinPoint.getTarget().getClass().getName() + &quot;.&quot; + joinPoint.getSignature().getName() + &quot;()&quot;)); log.setType(&quot;0&quot;); log.setRequestIp(ip); log.setExceptionCode( null); log.setExceptionDetail( null); log.setParams( null); log.setCreateBy(user); log.setCreateDate(DateUtil.getCurrentDate()); //保存数据库 logService.add(log); System.out.println(&quot;=====前置通知结束=====&quot;); } catch (Exception e) { //记录本地异常日志 logger.error(&quot;==前置通知异常==&quot;); logger.error(&quot;异常信息:{}&quot;, e.getMessage()); }} /** 异常通知 用于拦截service层记录异常日志 @param joinPoint @param e*/@AfterThrowing(pointcut = “serviceAspect()”, throwing = “e”)public void doAfterThrowing(JoinPoint joinPoint, Throwable e) { HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); HttpSession session = request.getSession(); //读取session中的用户 User user = (User) session.getAttribute(WebConstants.CURRENT_USER); //获取请求ip String ip = request.getRemoteAddr(); //获取用户请求方法的参数并序列化为JSON格式字符串 String params = “”; if (joinPoint.getArgs() != null &amp;&amp; joinPoint.getArgs().length &gt; 0) { for ( int i = 0; i &lt; joinPoint.getArgs().length; i++) { params += JSONUtil.toJsonString(joinPoint.getArgs()[i]) + &quot;;&quot;; } } try { /*========控制台输出=========*/ System.out.println(&quot;=====异常通知开始=====&quot;); System.out.println(&quot;异常代码:&quot; + e.getClass().getName()); System.out.println(&quot;异常信息:&quot; + e.getMessage()); System.out.println(&quot;异常方法:&quot; + (joinPoint.getTarget().getClass().getName() + &quot;.&quot; + joinPoint.getSignature().getName() + &quot;()&quot;)); System.out.println(&quot;方法描述:&quot; + getServiceMthodDescription(joinPoint)); System.out.println(&quot;请求人:&quot; + user.getName()); System.out.println(&quot;请求IP:&quot; + ip); System.out.println(&quot;请求参数:&quot; + params); /*==========数据库日志=========*/ Log log = SpringContextHolder.getBean(&quot;logxx&quot;); log.setDescription(getServiceMthodDescription(joinPoint)); log.setExceptionCode(e.getClass().getName()); log.setType(&quot;1&quot;); log.setExceptionDetail(e.getMessage()); log.setMethod((joinPoint.getTarget().getClass().getName() + &quot;.&quot; + joinPoint.getSignature().getName() + &quot;()&quot;)); log.setParams(params); log.setCreateBy(user); log.setCreateDate(DateUtil.getCurrentDate()); log.setRequestIp(ip); //保存数据库 logService.add(log); System.out.println(&quot;=====异常通知结束=====&quot;); } catch (Exception ex) { //记录本地异常日志 logger.error(&quot;==异常通知异常==&quot;); logger.error(&quot;异常信息:{}&quot;, ex.getMessage()); } /==========记录本地异常日志==========/ logger.error(“异常方法:{}异常代码:{}异常信息:{}参数:{}”, joinPoint.getTarget().getClass().getName() + joinPoint.getSignature().getName(), e.getClass().getName(), e.getMessage(), params); } /** * 获取注解中对方法的描述信息 用于service层注解 * * @param joinPoint 切点 * @return 方法描述 * @throws Exception */ public static String getServiceMthodDescription(JoinPoint joinPoint) throws Exception { String targetName = joinPoint.getTarget().getClass().getName(); String methodName = joinPoint.getSignature().getName(); Object[] arguments = joinPoint.getArgs(); Class targetClass = Class.forName(targetName); Method[] methods = targetClass.getMethods(); String description = &quot;&quot;; for (Method method : methods) { if (method.getName().equals(methodName)) { Class[] clazzs = method.getParameterTypes(); if (clazzs.length == arguments.length) { description = method.getAnnotation(SystemServiceLog. class).description(); break; } } } return description; } /** * 获取注解中对方法的描述信息 用于Controller层注解 * * @param joinPoint 切点 * @return 方法描述 * @throws Exception */ public static String getControllerMethodDescription(JoinPoint joinPoint) throws Exception { String targetName = joinPoint.getTarget().getClass().getName(); String methodName = joinPoint.getSignature().getName(); Object[] arguments = joinPoint.getArgs(); Class targetClass = Class.forName(targetName); Method[] methods = targetClass.getMethods(); String description = &quot;&quot;; for (Method method : methods) { if (method.getName().equals(methodName)) { Class[] clazzs = method.getParameterTypes(); if (clazzs.length == arguments.length) { description = method.getAnnotation(SystemControllerLog. class).description(); break; } } } return description; } } ··· 把Controller的代理权交给cglib&lt;aop:aspectj-autoproxy proxy-target-clss=&quot;true&quot; /&gt; 在controller里面使用 ··· @RequestMapping(value=”/delete”) //此处为记录AOP拦截Controller记录用户操作 @SystemControllerLog(description=”删除用户”)public String del(Criteria criteria,String id,Model model,HttpSession session) ···]]></content>
      <categories>
        <category>框架集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL主从复制的搭建]]></title>
    <url>%2F2017%2F11%2F17%2Fcopymysql%2F</url>
    <content type="text"><![CDATA[搭建步骤修改MySQL主机上的MySQL的配置文件vim/etc/my.cnf 在/etc/my.cnf配置文件的[mysqlID]下添加如下内容： 在MySQL主机上重启MySQL服务service mysqlId restart 进入MySQL数据库可以查看刚才配置的server-id进入MySQL数据库mysql-uroot-proot查看server_idshow variables like ‘server_id’ 由于这台是主机，可以查看主机的状态show master status 搜全备机能够进行数据同步GRANT REPLICATION SLAVE ON . TO ‘root’@’192.168.1.103’ IDENTIFIED BY’root’; MySQL备机上，也修改MySQL配置文件并重启，但server_id不能相同在备机执行一下SQL语句，表示到主机中去同步数据1234567change master tomaster_host=&apos;192.168.1.103&apos;,master_port=3306,master_user=&apos;root&apos;,master_password=&apos;root&apos;,master_log-file=&apos;mysql-bin.000001&apos;,master_log_pos=154; master_log_file:表示要去同步的log日志，就是我们配置的log-bin，也就是show master status;查看的结果master_log_pos：表示从哪一行开始同步 开启同步start slave;可以查看备机的从服务器的状态show slave status\G;如果Slave_IO_Running、Slave_SQL_Running都为Yes表示成功其中任何一个为NO都不行]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库基础小结]]></title>
    <url>%2F2017%2F10%2F31%2Fdatebase%2F</url>
    <content type="text"><![CDATA[数据库是入门Coder的基础。工具用太久，有时候甚至忘了一些基础，小结一下。 基本语法聚合函数 AVG COUNT MIN MAX SUM GROUP_CONCAT 返回由属于一组的列值连接组合而成的结果 表连接内连接From table1 a**inner join** table2 b **on** a.id=b.cid &gt; 亦可写成`From table1 a , table2 b where a.id=b.cid` &gt; 在连接条件中还有（&gt;、&lt;、&lt;&gt;、&gt;=、&lt;=、！&gt;和!&lt;） 外连接 左外连接 From table1 a **left join** table2 b **on** a.id=b.cid 左连接显示左表全部行 右外连接 From table1 a **right join** table2 b **on** a.id=b.cid 右连接显示右表全部行 全连接 From table1 a **full join** table2 b **on** a.id=b.cid 交叉连接（CROSS JOIN）也称笛卡尔积：不带WHERE条件子句，它将会返回被连接的两个表的笛卡尔积，返回结果的行数等于两个表行数的乘积,如果带WHERE，返回的是匹配的行数。 不带Where From table1 cross join table2 &gt;亦可写成`From table1 ,table2` 带Where From table1 a cross **join table2** b where a.id=b.cid 注意：cross join 后加条件只能用where ，不用用on 自连接连接的表都是同一个表，同样可以由内连接，外连接各种组合 From table1 a ,table1 b where a.id = b.id 子查询 为了给住查询（外部查询）提供数据而首先执行的查询（内部查询）被叫做子查询。也就是说，先执行子查询，根据子查询的结构，再执行主查询 IN、NOT IN、EXIST、NOT EXIST、=、&lt;&gt; 子查询的效率低于连接诶查询 事务事务的四个属性 原子性 事务是由一个或一组相互关联的SQL语句组成，这些语句被认为是一个不可分割的单元 一致性 对于数据库的修改是一致的，即多个用户查的数据是一样的。一致性主要由mysql的日志机制处理，他记录数据的变化，为事务恢复提供跟踪记录。 隔离性 每个事务都有自己的空间，和其他发生在系统中的事务隔离开来，而且事务的结果只在他完全被执行时才能看到 持久性 提交了这个事务之后对数据的修改更新就是永久的。当一个事务完成，数据库的日志已经被更新时，持久性即可发挥其特有的功效，在mysql中，如果系统崩溃或数据戒指被破坏，通过日志，系统能够恢复在重启前进行的最后一次成功更新，可以反应系统崩溃时处于执行过程的事物的变化 事务的四种隔离级别READ UNCOMMITTED(未提交读)事务A对数据做的修改，即使没有提交，对于事务B来说也是可见的，这种问题叫脏读 READ COMMITTED（提交读）事务A对数据做的修改，提交之后会对事务B可见。 &gt; 举例：事务B开启时独到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，会独到最新的数据2 REPEATABLE READ(可重复读)事务A对数据做的修改，提交之后，对于先于事务A开启的事务是不可见的。 &gt; 举例：事务B开启时读到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，仍然独到数据1 SERIALIZABLE（可串行化）可串行化是最高的隔离级别。这种隔离级别强制要求所有事物串行执行，在这种隔离级别下，读取的每行数据都加锁，会导致大量的锁征用问题，性能最差 &gt; 随着隔离级别的增高，并发性能也会降低 mysql中的事务 事务的实现是基于数据库的存储引擎的。不同的存储引擎对事务的支持成都不一样 mysql支持的存储引擎中支持事务的InnoDB 事务的隔离级别是通过锁实现的，而事务的原子性、一致性和持久性是通过事务日志实现的 事务日志 redo——保障了事务的持久性和一致性 在InnoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志现形”。当事务提交之后，在BUFFER POOL中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态，未完成的事务，可以继续提交，也可以选择回滚，这是基于恢复的策略而定 undo——保障了事务的原子性 undo log主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。每个事务的回滚，只会当回滚当前事务做的操作，并不会印象到其他的事务做的操作 索引索引概述 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，他们包含着数据表里所有文件的引用指针。可以类比为书的目录，可以加快数据库的查询速度。 索引是创建在数据表对象上的。由表中的一个字段或多个字段生成的键组成，这些键存储在数据结构（B-树或哈希表）中，通过mysql可以快速有效的查找与键值相关联的字段 根据存储类型，分为B型树索引和哈希索引 InnoDB和MyISAM支持BTREE类型索引，默认 Memoery支持HASH类型的索引 索引分类普通索引最基本的索引，没有任何限制。MyIASM中默认的BTREE类型的索引，经常用 唯一索引与普通索引类是，不同的是：索引的列必须唯一，但允许有空值 主键索引它是一种特殊的唯一索引，不允许有空值 全文索引 FULLTEXT索引仅仅可用于MYISAM表 他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加 注意：大容量的表，使用全文索引虽然速度更快，但是生产全文检索是一个非常消耗时间消耗磁盘空间的做法 组合索引 所谓多列索引，是指在创建索引的时候，锁关联的字段不是一个字段，而是多个字段。 虽然可以通过所关联的字段进行查询，但是只有查询条件中使用了所关联字段中的第一个字段，多列索引才会被使用 应用场景什么情况适合创建索引 经常被查询的字段，即在WHERE子句中出现的字段 在分组的字段，即在GROUP BY子句中出现的字段 存在依赖关系的子表和父表之间的联合查询，即主键或外键字段 设置唯一完整性约束的字段 什么情况不适合创建索引 在查询中很少使用的字段 拥有许多重复值的字段 索引失效 使用or关键字的时候索引失效，要想使用or,又想使用索引，只能将or条件中的每个列都加上索引 对于多列索引，不是使用的第一部分，则不会使用索引 like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引 如果mysql估计使用全表扫描要比使用索引快，则不使用索引 对索引列进行运算导致索引失效(+，-，*，/，！等) 独立的列（对列变量需要计算（聚合运算、类型转换等）） 在JOIN操作中（需要从多个数据表提取数据时），MYSQL只在主键和外键的数据类型相同时才能使用索引，否则即使建立了索引也不会使用 不使用NOT IN和&lt;&gt;操作，不会使用索引将进行全表扫描，NOT IN 可以NOT EXISTS代替，ID&lt;&gt;3则可以用id&gt;3 or id &lt; 3来代替 索引不会包含有NULL值的列，只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 查看索引和优化索引 查看索引show status like &#39;Handler_read%&#39; 优化索引 索引不会包含有NULL值的列，因此数据库设计时不要让字段的默认值为NULL 使用短索引 例如：如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是唯一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 索引列排序 MySQL查询只使用一个索引，因此如果WHERE子句中已经使用了索引的话，那么order by 中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引 不要在列上进行运算 存储过程、存储函数什么是 一组预编译的SQL语句集 优点 只需要一次创建过程，以后在程序中就可以调用该过程任意次 允许更快执行，如果某操作需要执行大量SQL语句或重复执行，存储过程比SQL语句执行的要快 减少网络流量，例如一个需要数百行的SQL代码的操作有一条执行语句完成，不需要在网络中发送数百行代码 更好的安全机制，对于没有权限执行存储过程的用户，也可以授权他们执行存储过程 缺点 可移植性差，多个类型的数据库 学习成本高 如果存储过程中有复杂运算的话，会增加一些数据库服务端的处理成本，对于集中式数据库可能会导致系统可扩展性问题。 如何用存储过程1234CREATE PROCEDURE Proc()BEGINSELECT * tb_person;END; 存储函数123CREATE FUNCTION Query_score(classID INT,studentID INT)RETURNS INT RETURN(SELECT grade FROM tb_score WHERE cID=classID AND sID=studentID); 两者之间的区别 存储过程的功能更加复杂，而存储函数的功能针对性更强； 存储过程可以返回参数（通过OUT|INPUT），而函数只能返回单一值或者表对象； 存储过程作为一个独立的部分来执行，而函数可以可以作为查询语句的一部分来调用，由于函数可以返回一个表对象，因此它可以在查询语句中位于FROM关键字之后； 存储过程是通过关键字CALL来调用，作为一个独立的执行部分。而存储函数则可作为SELECT语句的一部分调用，嵌入到SQL语句中； 当存储过程和函数被执行的时候，SQLManager会到procedure cache中去取相应的查询语句，如果在procedure cache里没有相应的查询语句，SQLManager就会对存储过程和函数进行编译。 应用场景 复杂的数据处理用存储过程，如有些报表处理 多条件多表联合查询，并做分页处理，用存储过程也比较适合 优化SQL语句优化 对查询进行优化，尽量避免全表查询，首先考虑在where以及order by 涉及的列上建立索引。 应尽量避免在where子句中字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描 应尽量避免在where子句中使用!=或&lt;&gt;操作符，否则将导致引擎放弃使用索引而进行全表扫描。 应尽量避免在where子句中使用or来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，可以使用union all来代替 in 和not in 也要慎用，否则会导致全表扫描，对连续的数据用between 代替，也可以用exists 代替in like ‘%a%’这种，索引失效，可以使用全文索引代替 对于多张大数据量（几百条就算）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差 少使用* 日期使用mysql自带的，少使用字符串存储，字符串的比较复杂；IP也使用int类型，不要使用字符串 尽可能的使用varchar/nvarchar代替char/nchar，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对娇小的字段内搜索效率显然要高些。 数据库优化主从复制（读写分离）使用spring可以实现读写分离 分库分表 分表 垂直分表 将表按照功能模块、关系密切程度划分出来，部署到不同的库上，比如我们会建立定义数据库workDB、商品数据库payDB、用户数据库userDB、日志数据库logDB等，分别用于存储项目数据定义表、商品定义表、用户数据表、日志数据表等 水平分表 指定自己的规则：1、求余。2、哈希。3、时间 分库 分库就是把一张表的数据分成N多个区块，这些区块可以在同一个磁盘上，也可以在不同的磁盘上 应用场景 表太多，海量数据，各项业务划分清除，低耦合的话，用垂直拆分比较号 表不多，单表数据量大的话，选水平拆分比较号 存储引擎查看mysql支持的存储引擎（SHOW ENGINES） myisam和innodb的区别 存储结构 MyISAM每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，拓展名指出文件类型。.frm文件存储表定义。数据文件的拓展名为.MYD(MYDate)。索引文件的拓展名是.MYI(MYIndex) InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB 存储空间 MyISAM可被压缩，存储空间较小。支持三种不同的存储格式：静态表（默认，但是主义数据末尾不能有空格，会被去掉）、动态表、压缩表 innoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于告诉缓冲数据和索引 可移植性、备份及恢复 MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个进行操作。 innoDB免费的方案可以是拷贝数据文件、备份binlog，或用mysqldump，在数据量达到几十GB的时候就相对痛苦了。 事务支持 MyISAM强调的是性能，每次查询具有原子性，其执行速度比InnoDB类型要快，但是不提供事务支持。 innoDB提供事务支持，外部键等高级数据库功能。具有事务（commit），回滚（rollback）和崩溃修复能力（crash recovery capablities）的事务安全（transaction=safe(ACID compliant)）型表 表锁差异 MyISAM只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据 innoDB支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的性能，但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的 AUTO-INCREMENT MyISAM可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 innoDBInnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引。如果是组合索引也必须是组合索引的第一列。 全文索引 MyISAM支持FULLTEXT类型的全文索引。 innoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好 表主键 MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。 innoDB如果没有设定主键或空孔唯一索引，就会自动生成一个6字节的主键（用户不可见），数据是主索引的一部分，附加索引保存的是主索引的值。 表的具体行数 MyISAM保存表的总行数，如果select count(*) from table会直接取出该值 innoDB没有保存表的总行数，如果使用select count(*) from table；会遍历整个表，消耗相当大，但是在加了where条件后，myisam和innodb处理的方式都一样。 CRUD操作 MyISAM如果执行大量的select，MyISAM是更好的选择 innoDB如果你的数据执行大量的insert或update，出于性能方面的考虑，应该使用InnoDB表。DELETE从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在InnoDB上如果要清空保存有大量数据的表，最好使用truncate table这个命令 外键 MyISAM不支持 innoDB支持 锁表锁行锁（InnoDB）共享锁又成为读锁，多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改 &gt; SELECT ... LOCK IN SHARE MODE 排他锁又称为写锁。排它锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行数据和修改。排他锁指的是一个事务在一行数据加上排它锁后，其他事务不能再加其他的锁。updata,delete,isnert都会自动给设计到的数据加上排他锁，select语句默认不会加任何锁类型 &gt; SELECT ... FRO UPDATE 乐观锁 乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行就爱南侧，如果发现冲突了，则返回用户错误的信息，让用户决定如何去做 实现方式 版本（Version）记录机制 一般是通过为数据库表增加一个数字类型的”version”字段来实现。当读取数据时，将version字段的值一同独处，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比较。如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据 时间戳（timestamp） 悲观锁它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守状态（悲观），因此，在整个数据处理过程中，将数据处于锁定状态。 &gt; SELECT ... FOR UPDATE 或 LOCK IN SHARE MODE select语句完整的执行顺序from → where → group by → having →表达式 → group by → select 输出]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hibernate]]></title>
    <url>%2F2017%2F10%2F26%2Fhibernate%2F</url>
    <content type="text"><![CDATA[Hibernate是一个开放源代码的对象关系映射框架，它对JDBC进行了非常轻量级的对象封装，它将POJO与数据库表建立映射关系，是一个全自动的orm框架，hibernate可以自动生成SQL语句，自动执行，使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库。 执行流程 读取配置Configuration 创建sessionFactory 打开session 开启事务Transaction 进行持久化操作（CRUD） 提交事务 关闭session延迟加载延迟加载机制是为了避免一些无谓的性能开销而提出来的。所谓延迟加载就是当在真正需要数据的时候，才真正执行数据加载操作。在Hibernate中提供了对实体对象的延迟加载以及对集合的延迟加载，另外在Hibernate3中还提供了对属性的延迟加载。缓存 一级缓存 hibernate支持两个级别的缓存，默认只支持一级缓存 每个session内部自带一个一级缓存 每个Session被关闭时，其对应的一级缓存自动清除二级缓存独立于session，默认不开启查询方式本地SQL查询，Criteria、HQL状态 瞬时态不纯在持久化标识OID，尚未与Hibernate Session关联对象，被认为处于瞬时态，失去引用将被JVM回收 持久态存在持久化标识OID，与当前session有关联，并且相关联的session没有关闭，并且事务未提交 游离态存在持久化标识OID，但没有与当前session关联，游离状态改变hibernate不能检测到 优化 使用双向一对多关联，不使用单向一对多 灵活使用单向一对多关联 不用一对多，用多对一取代 配置对象缓存，不使用集合缓存 一对多集合使用Bag,多对多集合使用Set 继承类使用显式多台 表字段要少，表关联不要怕多，有二级缓存撑腰GET和LOAD的区别Session.get方法，查询立即执行，返回Customer类对象Session.load方法，默认采用延迟加载数据方式，不会立即查询，返回Customer类子类对象（动态生成代理对象）]]></content>
      <categories>
        <category>框架集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis]]></title>
    <url>%2F2017%2F10%2F25%2Fmybatis%2F</url>
    <content type="text"><![CDATA[MyBatis是一个持久层框架，避免了jdbc代码和手工设置参数以及抽取结果集，可以使用简单的xml或者注解方式来配置映射基本体，将接口和java中的pojo映射成数据库中的记录 执行流程 和Hibernate的对比 Mybatis和Hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写SQL语句，不过mybatis可以用过XML或注解方式灵活配置要运行的sql语句，并将java对象和SQL语句映射生成最终执行的SQL，最后将SQL执行的结果再映射生成JAVA对象 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一旦需求变化要求成功输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大 Hibernate对象/关系映射能力强，数据库无关性号，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的缺点是学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 使用MyBatis的mapper接口调用时有哪些要求 Mapper接口方法名和mapper.xml中定义的每个sql的ID相同 Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql的parameterType类型相同 Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType类型相同 Mapper.xml文件中的namespace即是mapper接口的类路径。 SqlMapConfig.xml中配置有哪些内容SqlMapConfig.xml中配置的内容和顺序如下 properties(属性) settings(配置) typeAliases(类型别名) typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境集合属性对象） enviroment（环境子属性对象） transactionManager(事务管理) dataSource（数据源） mappers（映射器） 一级缓存和二级缓存一级缓存Mybatis首先去缓存中查询结果集，如果没有则查询数据库，如果又则从缓存取出返回结果就不查数据库。Mybatis内部存储缓存使用一个HashMap。key为hashCode+sqlId+Sql语句。value为查询出来映射生成的java对象。 二级缓存Mybatis的二级缓存即查询缓存，它的作用域是一个mapper的namespace，即在同一个namespace中查询sql可以从缓存中获取数据。二级缓存是可以跨SqlSession的。]]></content>
      <categories>
        <category>框架集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[springmvc]]></title>
    <url>%2F2017%2F10%2F24%2Fspringmvc%2F</url>
    <content type="text"><![CDATA[Spring MVC属于SpringFrameWork的后续产品，已经融合在Spring Web Flow里面。Spring 框架提供了构建 Web 应用程序的全功能 MVC 模块。使用 Spring 可插入的 MVC 架构，从而在使用Spring进行WEB开发时，可以选择使用Spring的SpringMVC框架或集成其他MVC开发框架，如Struts1，Struts2等。 执行流程 用户发送请求到前端控制器DispatcherServlet DispatcherServlet收到请求调用HandlerMapping处理器映射器 处理器映射器找到具体的处理器，生成处理器对象及处理器拦截器（如果有则生成）一并返回给DispatcherServlet。 DispatcherServlet调用HandlerAdapter处理器适配器 HandlerAdapter经过适配调用具体的处理器（Controller，也叫后端控制器） Controller执行完成返回ModelAndView HandlerAdpater将Controller执行结果ModelAndView返回给DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet根据View进行渲染视图（即将模型数据填充至视图中） DispatcherServlet响应用户常用的注解 @Controller 标记在一个类上，一个springmvc的controller对象 @RequestMapping 处理请求地址映射的注解，可用于类或方法上，用于类上，表示类中的所有响应方法都是以该地址作为父路径的。 @Resource和@Autowired 都是bean注入使用 @Autowired为spring提供的注解，需要导包 @Autowired是按照类型注入对象的，如果想按照名称来配置，可以结合@Qualifier注解使用 @Resource是按照名称自动注入 @PathVariable 用于将请求路径URL中的模版变量映射到功能方法的参数上，即取出URL模版中的变量作为参数 @RequestParm 在Springmvc后台控制层获取参数 @ResponseBody 将controller的方法返回的对象，通过适当的htppmessageconverter转化为指定格式后，写入到response对象的body数据区 @Component 通用注解 @Repository 注解dao层 @ModelAttribute和@SessionAttribute @ModelAttribute该Controller的所有方法在调用前，先执行，应用在basecontroller中 @SessionAttribute将值放在session域中]]></content>
      <categories>
        <category>框架集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Struts2]]></title>
    <url>%2F2017%2F10%2F21%2FStruts2%2F</url>
    <content type="text"><![CDATA[Struts2是一个基于MVC设计模式的Web应用框架，它本质上相当于一个servlet，在MVC设计模式中，Struts2作为控制器(Controller)来建立模型与视图的数据交互。 执行流程 加载类（FilterDispatcher） 读取配置（struts2配置文件中的Action） 派发请求（客户端发送请求） 调用Action（FilterDispatcher从Struts配置文件中读取与之对应的Action） 启动拦截器（WebWork拦截器链自动对请求应用通用功能，如验证） 处理业务（回调Action的excute（）方法） 返回响应（通过execute方法将信息返回到FilterDispatcher） 查找响应（FilterDispatcher根据配置查找响应的是什么信息如：SUCCESS、ERROER、将跳转到哪个jsp页面） 响应用户（jsp→客户浏览器端显示） Struts2标签库（相比struts1的标签库，struts2是大大加强了，对数据的操作功能很强大） 与SpringMVC的区别 SpringMVC的入口是一个Servlet前端控制器，而Struts2入口是一个Filter过滤器 SpringMVC是基于方法开发，传递参数是通过方法形参，可以设计为单例或多例（建议单例） Struts2采用值栈存储请求和响应的数据，通过OGNL存储数据，springmvc通过参数解析器将request对象内容进行解析成方法形参，将响应数据和页面封装成ModelAndView对象，最后又将模型数据通过request对象传输到页面。jsp视图解析器默认使用jstl。]]></content>
      <categories>
        <category>框架集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集合小结]]></title>
    <url>%2F2017%2F10%2F17%2Fcollection%2F</url>
    <content type="text"><![CDATA[计算机科学中，集合是一组可变数量的数据项(也可能是0个)的组合，这些数据项可能共享某些特征，需要以某种操作方式一起进行操作。 单例集合List特点：元素有放入顺序，元素可重复 ArrayList 长度可变的数组 有索引结构，使用索引在数组中搜索和读取数组是很快的，也就是查找快 数组这种数据结构将对象放在连续的位置中，因此增加、删除的速度比较慢 存数类型是Object，允许为null 初始大小（不设置大小的时候默认是10） 不是线程安全的，异步的 LinkedList 链表的数据结构 存储的类型是Object,允许为null 不是线程安全的，得自己实现线程安全 在创建List时构造一个同步的List List list = Collections.synchronizedList(new LinkedList(...)); Vector 底层是数组结构 同步的，线程安全 执行效率低 Vector缺省情况下自动增长原来一倍的数组长度 在集合中保存大量的数据那么使用Vector有一些有事，因为你可以通过设置集合的变化大小来避免不必要的资源开销 初始大小是10 Set特点：元素无放入顺序，且不可重复（实际存储顺序是HashCode决定的，固定的） HashSet 基于hashmap实现，底层使用hashmap保存元素 set元素中的值存放在hashmap的key中，value存放了一个模拟值persent 由于hashmap中的key是不能重复的，所以hashset就利用这个特性实现了set中的值不会重复 hashset是如何保证元素的唯一性 基于哈希表实现的。哈希表是存储一系列哈希值的表，而哈希值是由对象的hashcode()方法产生的。 确保元素唯一性的两个方法：hashcode()和equals()方法 当调用add（）方法向集合中存入对象的时候，先比较对象的哈细致有没有一样的，如果都不一样就直接存入。如果有与之相同的哈希值，则要继续比较两个对象是否是同一个对象，调用对象的equals方法TreeSet 基于treeMap实现的 有序的二叉树多例集合 HashTable 父类是Dictionary 实现一个key-value映射的哈希表。任何非空对象都可作为key或者value 同步的，线程安全 内部通过单链表解决冲突问题 效率低 有contains方法 初始大小是11 扩容时，hashTable采用的是2*old+1 hash值的计算使用的直接是对象的hashcode方法 迭代器，HashTable用Enumeration来遍历数据 只提供了遍历Vector和Hashtable类型集合元素的功能 这种类型的集合对象通过调用elements()方法获取一个Enumeration对象，然后Enumeration对象再调用一下方法来对集合中元素进行遍历。 hasMoreElements():判断Enumeration对象中是否还有数据nextElement():获取Enumeration对象中的下一个数据 HashMap 弗雷是AbstractMap 哈希表，允许key和value值为空 方法是同步的，线程不安全 为什么不安全 首先如果多个线程同时使用put方法添加元素，而且假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖 如果多个线程同时检测到元素个数超过数组大小*loadFactor，这样就会发生多个线程同时对Node数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋值给table，也就算说其他线程的都会丢失，并且各自线程put的数据也丢失 解决方案 HashtableConcurrentHashMapSynchronized Map 效率高 有containsvalue()和containsKey()方法 初始大小是16 扩容时，hashMap采用的是2*old hash值的计算没有直接使用对象的hashcode方法，为对象key的hashcode值与对象value的hashcode值按位与或操作 迭代器 hashMap的工作原理 当两个对象的hashcode值相同时会发生什么？(hashmap中的碰撞探测(collision detection)及解决方案) 因为hashcode值相同，因此他们的bucket位置相同，碰撞就会发生 因为hashMap使用链表存储对象，这个Entry(包含有监支队的Map.Entry对象)会存储在链表中 如果有两个键的hashcode相同，你如何取对象？ 当我们调用get()方法，hashMap会使用建对象的hashcode找到bucket位置，然后获取值对象 找到bucket位置之后，会调用keys,equals()方法去找到链表中正确的节点，最终找到要找的值对象 如果HashMap的大小超过了负载银子(load factor)定义的容量，怎么办？ 默认的负载银子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其他集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中，这个过程叫做rehashing，因为它调用hash方法找到新的bucket位置 重新调整HashMap大小存在什么问题吗？ 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，他们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将呀un苏放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件发生了，那么就死循环了。 为什么String、Integer这样的wrapper类适合作为键 String、Integer这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的。也是final的，而且已经重写了equals()和hashcode()方法了。其他的wrapper类也有这个特点。不可变形是必须，因为为了要计算hashCode(),就要防止键值改变，如果键值在放入时和获取是i返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性是必要的，因为为了要计算hashCode()，就要防止兼职改变。如果键值在放入和获取时返回不同的hashCode就不能从HashMap中找到想要的对象。不可变性还有其他优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法非常重要。如果两个不想等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 ConcurrentHashMap CHM不但是线程安全的，而且比HashTable和synchronizedMap的性能要好 相对于HashTable和synchronizedMap锁住了整个Map,CHM只锁住部分Map. CHM允许并发的读操作，同时通过同步锁在写操作时保证数据完整性 如何实现 CHM引入了分割，并提供了HashTable支持的所有的功能 在CHM中，支持多线程对Map做读操作，并且不需要任何的blocking CHM将MAP分割成不同的部分，在执行更新操作的时候只锁住了一部分 根据默认的并发级别，Map被分割成16个部分，并且由不同的锁控制。这样的话可以同时最多有16个写线程进行操作 但由于一些更新操作，如put()，remove(),putAll(),clear()只锁住操作的部分，所以检索操作不能保证返回的是最新的结果 在迭代遍历CHM时，keySet返回的是iterator是弱一致性和fail-safe的，可能不会返回某些最近的改变，并且在遍历过程中，如果已经遍历的数组上的内容变化了，不会抛出ConcurrentModificationException的异常 CHM默认的并发级别是16，但可以在创建CHM时通过构造函数改变 CHM不允许null的键值 应用场景 CHM适用于读数量超过写 当写数量大于读时，CHM的性能低于Hashtable的synchronized Map的。这是因为当锁住了整个Map时，读操作要等待对统一部分执行写操作的线程结束 CHM适用于cache,在程序启动时初始化，之后可以被多个请求线程访问。]]></content>
      <categories>
        <category>基本集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java7的新特性Paths,Files]]></title>
    <url>%2F2017%2F09%2F25%2FJAVA7PathsFiles%2F</url>
    <content type="text"><![CDATA[在使用lucene 5的时候，发现在lucene4时file的地方用到了Path,发现这是JAVA7的新特性，于是查找相关文档，发现在IO方面,java7新增了Paths,Files工具类，发现异常强大，小结一下。 PathsPath是用来表示文件路径和文件，可以有多种方法来构造一个Path对象来表示一个文件路径或一个文件 在Paths类里有两个static方法 12345public static Path get(String first,String...more) &#123; return FileSystems.getDefault().getPath(first,more);&#125;public static Path get(Url url) 得到三种构造方式(以源文件d:/demo.txt为例) 1234Path path1 = Paths.get(&quot;d:/&quot;,&quot;demo.txt&quot;);Path path2 = Paths.get(&quot;d:/demo.txt&quot;);Path path3 = Paths.get(URI.create(&quot;file:///d:/demo.txt&quot;));Path path4 = FileSystems.getDefault().getPath(&quot;d:/&quot;,&quot;demo.txt&quot;); File和Path、File和URI之间的转换 12345File file = new File(&quot;d:/demo.txt&quot;);Path path = file.toPath();File file2 = path.toFile();URI uri = file.toURI(); 读取文件属性 1234567Path path = Paths.get(url);path.getFileName();path.getParent();//根目录path.getRoot();//目录级数(D:\xxx\xxx\xxx\demo.txt 4)path.getNameCount(); 创建一个文件 123Path path = Paths.get(&quot;C:\demo.txt&quot;);if(Files.exists(path)) Files.createFile(path); Files.newBufferWriter写入文件 1234BufferedWriter writer = Files.newBufferedWriter(Paths.get(&quot;D:\\demo.txt&quot;),Charset.forName(&quot;UTF-8&quot;)); writer.write(&quot;测试中文&quot;); writer.flush(); writer.close(); Files.newBufferWriter读取文件 123456BufferedReader reader = Files.newBufferedReader(Paths.get(&quot;D:\\demo.txt&quot;), Charset.forName(&quot;UTF-8&quot;)); String str = null; while((str = reader.readLine())!=null)&#123; System.out.println(str); &#125; reader.close(); 遍历文件夹,这里只遍历当前目录，不遍历子目录 12345678910111213Path path = Paths.get(&quot;D:\\dir&quot;); DirectoryStream&lt;Path&gt; paths = Files.newDirectoryStream(path); for(Path p : paths)&#123; System.out.println(p.getFileName()); &#125; DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(Paths.get(&quot;D:\\dir&quot;)); Iterator&lt;Path&gt; ite = stream.iterator(); while (ite.hasNext())&#123; Path path = ite.next(); System.out.println(path.getFileName()); &#125; 要遍历子目录，在java7前需要用递归，而java7的Files提供了walkFileTree()方法，这个在另一篇文章写到 Files 创建目录和文件 123Files.createDirectories(Paths.get(&quot;D://dir&quot;));if(!Files.exists(Paths.get(&quot;D://dir&quot;))) Files.createFile(Paths.get(&quot;D://dir/demo.txt&quot;)) 文件复制 123456789101112//Files.copy(Source,Target,CopyOptions) //StandardCopyOption //REPLACE_EXISTING 如果存在替换 //COPY_ATTRIBUTES 复制 //ATOMIC_MOVE Move the file as an atomic file system operation.//Files.copy(Source,OutputStream)//Files.copy(InputStream,Target,CopOption)Files.copy(Paths.get(&quot;C://Source.txt&quot;,Paths.get(&quot;D://Target.txt&quot;,StandardCopyOption.COPY_ATTRIBUTES))); 读取文件属性 123456789Path path = Paths.get(url);//最后一次修改时间 System.out.println(Files.getLastModifiedTime(path)); System.out.println(Files.size(path));//是否为一个连接 System.out.println(Files.isSymbolicLink(path)); System.out.println(Files.isDirectory(path));//指定属性，*表全部 System.out.println(Files.readAttributes(path,&quot;*&quot;));]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JavaWeb]]></title>
    <url>%2F2017%2F09%2F17%2Fjavaweb%2F</url>
    <content type="text"><![CDATA[Java Web，是用Java技术来解决相关web互联网领域的技术总和。web包括：web服务器和web客户端两部分。Java在客户端的应用有java applet，不过使用得很少，Java在服务器端的应用非常的丰富，比如Servlet，JSP和第三方框架等等。 Http协议 是一个基于请求与响应模式的、无状态的、应用层的协议，基于TCP的连接方式 请求： 请求行、消息报头、请求正文 响应：状态行、消息报头、响应正文 状态码 1XX：提示信息——表示请求已解说，继续处理 2XX：成功——表示请求已被成功接收、理解、接受 3XX：重定向——要完成请求必须进行更进一步的操作 4XX：客户端错误——请求有语法错误或请求无法实现 5XX：服务器端错误——服务器未能实现合法的请求 TomcatServletServlet是一个web容器，我们通常用的servlet是httpservlet，而httpservlet又是继承于genericservlet，而genericservlet又实现了servlet接口 生命周期 实例化——init() 初始化 提高服务——service() 销毁 不可用 应用场景 在调用service()方法中，因为我们继承了httpservlet，其实就是对应了doGet(),doPost()方法。 常用的MVC框架，strus,spring这些框架都是基于servlet发展二来的，比如struts1的核心控制器是ActionServlet，而MVC的前端控制器是dispatchServlet Servlet的由于是单例的，所有请求都使用一个实例，所以如果有全局变量被多线程使用的时候，就会出现线程安全问题。解决方案如下： 实现singleThreadModel接口，这样对于每次请求都会创建一个新的servlet实例，这样会消耗服务器内存，且已经过时。 通过加锁(synchronized)来避免线程安全问题。这个时候虽然是单例，但是对于多线程的访问，每次只能有一个请求进行方法体内执行，只有执行完毕后，其他线程才允许访问，降低吞吐量。 避免使用全局变量，使用局部变量可以避免线程安全问题，强烈推荐使用此方法来解决servlet线程安全问题 12345678&lt;servlet&gt; &lt;servlet-name&gt;唯一性 &lt;servlet-class&gt;提高的服务&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt; &lt;url-pattern&gt;&lt;/servlet&gt; Listener一个Java类，用来监听其他的Java的状态的变化 生命周期应用场景 监听其他对象的变化 应用在图形化界面中比较多FilterFilter是一个过滤器，用来在请求前和响应后进行数据的处理生命周期 实例化 初始化——init 进行过滤——doFilter 销毁——destroy 释放资源 应用场景 编码转换 123456789101112131415//实现Filter类public class CharactorFilter implements Filter &#123; //定义转换字符集类型 String encoding = utf-8; public void DoFilter(ServletRequest request,ServletResponse response,FilterChain chain) throws Exception&#123; if(encoding!=null)&#123; //设置request字符编码 request.setCharacterEncoding(encoding); //设置response字符编码 response.setContentType(&quot;text/html;charset=&quot;+encoding); &#125; chain.doFilter(request,response); &#125;&#125; web.xml 12345678&lt;filter&gt; &lt;filter-name&gt; &lt;filter-class&gt;上文过滤器的全限定名&lt;/fitler&gt;&lt;filter&gt; &lt;filter-name&gt; &lt;url-pattern&gt;&lt;/filter&gt; 安全验证 123456789101112//实现Filter类public LoginFilter implements Filter&#123; public void doFilter(ServletRequest request,ServletResponse response,FilterChain chain) throws Exception&#123; HttpServletRequest httpRequest = (HttpRequest) request; HttpServletResponse httpResponse = (HttpResponse) response; HttpSession session = httpRequest.getSession(); if(session.getAttribute(&quot;username&quot;)!=null)&#123; chain.doFilter(request,response); &#125; &#125; 重复提交的判断 1234567891011121314public class HttpRequestContentFilter implements FIlter &#123; private Servlet Context context; public void init(FilterConfig filterConfig) throws Exception&#123; context = filterConfig.getServletContext(); &#125; public void doFilter(ServletRequest request,ServletResponse response,FilterChain chain) throws Exception&#123; //在ThreadLocal中共享本次请求、响应对象 HttpRequestContext.setHttpRequestContext((HttpServletRequest)request,(HttpServletResponse)response,context); chain.doFilter(request,response); &#125;&#125;]]></content>
      <categories>
        <category>Web集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux误删除了备份的数据库]]></title>
    <url>%2F2017%2F09%2F13%2FOracleBankDelete%2F</url>
    <content type="text"><![CDATA[延迟加载异常：failed to lazily initialize a collection of role: com.misaniy.bos.domain.base.Courier.fixedAreas, could not initialize proxy - no Session reason：误删除了备份的数据库resolve：12345678910111213141516sqlplus /nolog//使用数据库命令模式connect system/root as sysdba//连接数据库SQL&gt;shutdown normal//关闭数据库oracle服务SQL&gt;startup mount//重新启动Oracle服务SQL&gt;alter database open;//打开数据库//SQL&gt;alter database datafile 5 offline drop 若出现错误SQL&gt;alter database open;//重新更改数据库的openSQL&gt;startup]]></content>
      <categories>
        <category>报错集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[could not initialize proxy - noSession]]></title>
    <url>%2F2017%2F09%2F13%2Flazily%2F</url>
    <content type="text"><![CDATA[failed to lazily initialize a collection of role: com.misaniy.xxx, could not initialize proxy - no Session延迟加载异常 解决方案有两种其一，在web.xml中配置Spring的OpenSessionInViewFilter，确保服务器端的逻辑执行完后再关闭session，这是针对hibernate的支持类12345678910111213&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.jpa.support.OpenEntityManagerInviewFilter&lt;/filter-class&gt; &lt;!-- 如果你的sessionFactory不是叫sessionFactory，需要配置如下--&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt; &lt;param-value&gt;&#123;Your Session Factory Name&#125;&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 其二，上面方法是hibernate的支持类，如果你配置的不是sessionFactory,比如我用的SPRING DATA JPA，就用如下方法1234@JSON(serialize = false)public XXX getXXX()&#123; return XXX;&#125; 在Bean类找到你的延迟加载的数据，没有使用到就使用该注解]]></content>
      <categories>
        <category>报错集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Oracle和tomcat端口冲突]]></title>
    <url>%2F2017%2F09%2F12%2FPortException%2F</url>
    <content type="text"><![CDATA[Oracle XE http与tomcat端口冲突8080 reason：oracle与tomcat端口8080冲突，我们可以修改任意一个端口； resolve：修改oracle123sqlplus system/rootSQL&gt;call dbms_xdb.sethttpport(&apos;8082&apos;); 修改tomcat，这里由于用了maven,所以直接安装tomcat7插件Maven —&gt; build plugin —-&gt;tomcat712345678&lt;plugin&gt;&lt;gourpId&gt;org.apache.tomact.maven&lt;/groupId&gt;&lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;&lt;version&gt;2.2&lt;/version&gt;&lt;configuration&gt; &lt;port&gt;8081&lt;/port&gt; &lt;path&gt;/&lt;/path&gt;&lt;/configuration&gt; Run As —&gt; Maven InstallRun As —&gt; Maven Build… tomcat7:run]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mybatis的分页插件PageHelper]]></title>
    <url>%2F2017%2F08%2F10%2FPageHelper%2F</url>
    <content type="text"><![CDATA[逆向工程生成的代码是不支持分页处理的，如果想进行分页需要自己编写mapper，这样就失去逆向工程的意义了。为了提高开发效率可以是要弄个mybatis的分页插件PageHelper. 简介该插件目前支持Oracle，MySQL，MariaDB,SQLite,Hsqldb,PostgreSQL六种数据库分页. 使用方法 把PageHelper依赖的jar包添加到工程中。官方提供的代码对逆向工程支持的不好，使用民间修改版pagehelper-fix. 在Mybatis配置xml中配置拦截器插件 12345&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;&gt; &lt;property name=&quot;dialect&quot; value=&quot;mysql&quot;/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 代码 12345678910//获取第1页，10条内容，默认查询总数countPageHelper.startPage(1,10);//分页处理List&lt;E&gt; list = ;//取分页信息PageInfo&lt;E&gt; pageInfo = new PageInfo(list);pageInfo.getTotal();pageInfo.getPages();pageInfo.getPageNum();pageInfo.getPageSize();]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅谈数据库优化]]></title>
    <url>%2F2017%2F05%2F15%2Fsql%2F</url>
    <content type="text"><![CDATA[数据库优化分为硬优化、软优化。硬优化主要是指针对数据库本身的优化（表和库），软优化主要就是针对sql语句之类的优化。 软优化查询条件尽量加索引- 这个不是必须加的，但很大成都上能够解决一定查询效率的问题。但是需要主义的是不要在索引字段上做计算、函数等操作。 少部分关键字的使用需要减少- 避免使用Like - exit not exit来代替not in - 字段名代替* - id&gt;=3代替id&gt;2 减少子查询的使用- 复杂业务简单化，不使用子查询 - 先生成临时表，再做关联查询 其他- 减少对数据库的重复操作，能合并就合并，能一条sql就不要分多条 - 其他有的再补充 硬优化拆表 在很多情况下，某一张表某些字段经常被查询到，那么我们就有必要拆分表 正如这张图，在id\name\attr7\attr8\attr9常用的情况下，把表拆分成两张表再关联起来 分表 单表数据太多，查询的效率极慢的情况下我们可以把表里的数据分成几张表来存储 就如上图，我们每次查询一月份的数据的时候都要在全年度的表里去查，数据量太大，查询了会很慢，所以我们将每个月份的数据单独提出来做成表，这样查询就相对比较快了 读写分离 读写分离，主要针对数据库访问的量很大，导致数据库运行可能宕机的问题。原理就是添加几个数据库，形成集群将访问量平均分配到每个数据库上 如上图，我们只需要修改主库数据，通知到每个分类，从库更新之后，每次查询我们都将查询分配到每个从库就可以了]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[全文检索和Lucene]]></title>
    <url>%2F2017%2F04%2F27%2FFull-Context%2F</url>
    <content type="text"><![CDATA[SQL语句的like会搜索大量不相关的内容，不走索引，且，存在资源浪费。这时我们就需要用到全文检索。 全文检索和Lucene全文检索了解Lucene之前，我们需要清楚全文检索的概念。 生活中的数据分为三种。 结构化数据：具有固定格式或有限长度的数据，如数据库，元数据等。 非结构化数据：不定长度或无固定格式的数据，如邮件，word文档等。 半结构化数据：根据需要可以按结构化数据处理，也可抽取出纯文本按非结构化数据来处理。 对应的搜索分为两种。 对结构化数据的搜索：sql语句、windows搜索文件名、类型、修改时间等 对非结构化数据的搜索：windwos搜索文件内容、linux的grep,搜索引擎的搜索等 对非结构化数据搜索即对全文数据的搜索分为两种： 顺序扫描法：假设寻找某个字符串的文件，就是一个文档一个文档读，然后每个文档从头读到尾，Linux下的grep就是这种方式，小数据量可以使用，但对于大量数据，就很慢了。 全文检索：将非结构化数据中的一部分信息提取出来，重新组织，使其变为结构化数据，我们称之为索引，而这种先建立索引，再搜索的过程就叫全文检索。 对应的创建索引方式分三种。 索引：加快数据搜索的一种数据结构 I/O流：对于本地文件创建索引。 爬虫：模拟访问URL，获取网页数据，搜索引擎使用。 SQL搜索：对于存放在数据库的数据使用。 如何创建索引 源文档Document 文档中包括一个一个域(Field)[file_name,file_path,file_size,file_content等] 分词组件Tokenizer得到词元Token 将源文档分词 去除标点 去除停词 处理组件LinguisticProcessor得到词Term 变为小写 缩减位词根 转变为词根 索引组件Indexer 用词Term创建字典 对字典按字母顺序排序 合并相同的词Term成为倒排索引(Posting List) 倒排索引：从字符串到文件的映射是文件到字符串映射的反向过程，所以这种索引称为倒排索引 如何对索引进行搜索 用户输入查询语句 对查询语句进行分析处理 搜索索引，得到符合语法树的文档 根据得到的文档和查询语句的相关性排序 LuceneLucene实现全文检索 获得原始文档 创建文档对象 分析文档 创建索引 查询索引库 常用域Field 创建索引库12 索引库的维护索引库的添加123456789//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//Document.add(Field)Document document = new Document();document.add(new TextField(&quot;name&quot;,&quot;文档&quot;.Store.YES));document.add(new TextFiled(&quot;content&quot;,&quot;内容&quot;,Store.YES));IndexWriter.addDocument(document);IndexWriter.close(); 索引库的删除 删除全部 123456//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//删除全部索引indexWriter.deleteAll();indexWriter.close(); 指定条件删除 12345678//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//QueryQuery query = new TermQuery(new Term(&quot;name&quot;,&quot;文档&quot;));//指定条件删除indexWriter.deleteDocuments(query);indexWriter.close(); 索引库的修改123456789//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//修改后的DocumentDocument document = new Document();document.add(new TextField(&quot;name&quot;,&quot;新文档&quot;,Store.YES));//updateDocumentindexWriter.updateDocument(new Term(&quot;content&quot;,&quot;文档&quot;),document);indexWriter.close(); 查询索引Query的子类查询TermQuery 精确查找123456789101112131415//IndexSearcherIndexSearcher indexSearcher = new IndexSearcher(Directory.open(FSDirectory.open(new File(Path))));Query query = new TermQuery(new Term(FieldName,keyStr));TopDocs topDocs = indexSearcher.searcher(query,100);//topDocs.scoreDocs存储了document的idfor(ScoreDoc scoreDoc : topDocs.scoreDocs)&#123; //scoreDoc.doc就是document的id Document document = indexSearcher.doc(scoreDoc.doc); document.get(keyStr);&#125;indexSearcher.getindexReader.close(); MatchAllDocsQuery 所有文档NumericRangeQuery 数值范围查找BooleanQuery 组合条件查找IndexSearcher搜索方法queryparser 查询QueryParserMulitFieldQueryParser TopDocs]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用Redis实现缓存]]></title>
    <url>%2F2017%2F04%2F13%2FRedis%2F</url>
    <content type="text"><![CDATA[实现缓存的工具有很多，现在比较流行的是Redis。Redis(remote dictionary server)缓存存放在内存，数据库存放在磁盘，访问内存更快。频繁访问而不频繁修改的数据放在缓存里，能带来更好的体验和更小的服务器压力。 Redis安装Redis即便是高并发也是单线程的，不适合保存内容大的数据。 Linux下安装由于Redis是C语言开发的，安装Redis前需要安装c语言的编译环境。如果没有gcc需要在线安装。Yum install gcc-c++解压后编译make安装make install PREFIX=/usr/local/redis 连接Redis 启动Redis服务 前端启动 bin]./redis-server 后台启动 把redis3.0/redis.conf复制到redis/bin 目录下 修改配置文件daemonize yes 启动服务bin]./redis-server redis.conf 启动客户端 bin]./redis-cli默认连接localhost:6379 bin]./redis-cli -h 192.168.25.130 -p 6379 -h:Path -p:Port Redis五中数据类型 String:key-value(做缓存) get/incr/decr key set key value Hash:key-fields-values(做缓存) hget key field hset key field value hincrby key field int List:有顺序可重复 （类似堆） rpush key values往右添加 lpush key values 往左添加（倒序） lrange 0 -1 查询所有 lpop 从左取 rpop 从右取 Set:无顺序，不能重复 sadd key values smembers key srem key 删除 SortedSet(zset):有顺序，不能重复 zadd key n valueszadd zset1 2 a 5 b 1 c 6 d zrange key 0 -1 c a b d zrevrange key 0 -1 倒序查看 Key命令 expire key time 设置失效时间 ttl key 查看有效期 persist key 解除失效（持久化） Redis的持久化方案Redis所有数据都是保存在内存中的。 Rdb:快照形式。定期把内存中当前时刻的数据保存到磁盘。Redis默认支持的持久化方案。一直开启 Aof:Append Only File。把所有对Redis数据库操作的命令，增删改命令保存到一个文件中。数据库恢复时，把所有命令执行一遍就可恢复。开启后会降低一点性能 在Redis.conf配置文件中配置。 默认RBD123save 900 1save 300 10save 60 10000 - 备份文件 dump.rdb - AOF.每秒执行一次 `appendonly yes` Redis集群搭建架构细节 所有Redis节点彼此互联(PING-PONG机制)，内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连，不需要中间proxy层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 redis-cluster把所有物理节点映射到[0-16383]slot上，cluster负责维护node&lt;-&gt;slot&lt;-&gt;value Redis集群中内置了16384个哈希槽，当需要在Redis集群中防止一个key-value时，redis先对key使用crc16算法算出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，redis会根据节点数量大致均等的将哈希槽映射到不同的节点 集群搭建Redis集群中至少应该有三个节点，为了保证集群的高可用，需要每个节点有一个备份机。所以至少需要6台服务器。 搭建伪集群 复制单机版redis复制5份redis-cluster 修改每一个redis节点的端口port以及启用集群cluster-enabled yes 启动每一个节点./redis-server redis.conf 创建集群 安装Ruby的运行环境yum install ruby 安装Ruby的包管理器yum install rubygems 安装脚本gem install redis-3.0.0.gem 复制redis-3.0.0/src/redis-trib.rb到redis 使用ruby脚本搭建集群./redis-trib.rb create --replicase 1 192.168.XX.XX:7001 1992.168.XX.XX:7002 ...... 创建关闭集群的脚本 redis-cluster]vim shutdown-all.sh 123redis01/redis-cli -h address -p 7001 shutdownredis01/redis-cli -h address -p 7002 shutdownredis01/redis-cli -h address -p 7003 shutdown chmod u+x shutdown-all.sh这里这个redis-cli可以是任意一个redis集群的客户端 集群的使用方法Redis-cli 连接集群redis-cluster]redis01/redis-cli -p 7002 -c-c:代表连接的redis集群 Jedis连接单机版1234Jedis jedis = new Jedis(ipaddress,port);jedis.set(key,value);String result = jedis.get(key);System.out.print(result); 连接单机版使用连接池1234567JedisPool jedisPool = new JedisPool(ipaddress,port);Jedis jedis = jedisPool.getResource();jedis.set(key,value);String result = jedis.get(key);System.out.print(result);jedis.close();jedisPool.close(); 连接集群版1234567891011121314//第一步：使用JedisCluster对象，需要一个Set&lt;HostAndPort&gt;参数，Redis节点的列表Set&lt;HostAndPort&gt; nodes = new HashSet&lt;&gt;();nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));JedisCluster jedisCluster = new JedisCluster(nodes);//第二部：直接使用JedisCluster对象操作redis,在系统中举例存在。jedisCluster.set(key,value);String result = jedisCluster.get(key);//关闭JedisCluster对象。jedisCluster.close();]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Solr安装与介绍]]></title>
    <url>%2F2017%2F04%2F02%2Fsolr%2F</url>
    <content type="text"><![CDATA[Solr是基于Lucene的全文搜索服务器 Solr的安装和配置Solr配置到tomcat 把Solr的war包赋值到tomcat的webapp下并解压 把Solr/example/lib/ext目录下的jar包添加到solr工程中. 配置SolrHome和SolrCore Solr/example/solr文件就是一个标准SolrHome，复制出来命名solrHome solrHome中的collection1就是一个SolrCore solrCore下有一个目录conf，conf下的solrconfig.xml可以配置相关信息。 solrconfig.xml env-entry-value:配置solrhome的绝对路径 Lib:solr服务依赖的拓展包 dataDir:配置索引库存放路径 requestHandler:查询时使用的url 打开Schema.xml可以看到Solr默认的FieldType class:Solr提供的包solr.TextField，solr.TextField允许用户通过分析器来定制索引和查询，分析器包括一个分词器(tokenizer)和多个过滤器(filter) positionIncrementGap:可选属性，定义在同一个文档中此类型数据的空白间隔，避免短语匹配错误，此值相当于Lucene的短语查询设置slop值，根据经验设置位100 analyzer 搜索分析器 solr.StandardTokenizerFactory 标准分词器 solr.StopFilterFactory 停用词过滤器 solr.LowerCaseFilterFactory 小写过滤器 索引分析器 solr.StandardTokenizerFactory 标准分词器 solr.StopFilterFactory 停用词过滤器 solr.SynonymFilterFactory 同义词过滤器 Field 定义 name 域名 type FieldType indexed 是否索引 stored 是否存储 multiValued 是否存储多个值 uniqueKey 默认定义唯一主键key为id域 &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; copyField 复制域 将多个Field复制到一个Field中，进行统一检索 比如，输入关键字搜索name/description 1234567&lt;!-- 定义name/description/keywords的Field --&gt;&lt;field name=&quot;keywords&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot;/&gt;&lt;field name=&quot;name&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;description&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;false&quot; /&gt;&lt;!-- 只搜索keywords就相当于搜索了name和description&lt;copyField source=&quot;name&quot; dest=&quot;keywords&quot;&gt;&lt;copyField source=&quot;description&quot; dest=&quot;keywords&quot;&gt; dynamicField(动态字段)自定义Field为:product_title_t&lt;dynamicField name=&quot;*_t&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; 配置中文分析器 将IKAnalyzer2012FF_ul.jar添加到solr/WEB-INF/lib 赋值IKAnalyzer的配置文件和自定义词典到solr的classpath下 在schema.xml中添加一个自定义的filedType 123&lt;fieldType name=&quot;text_ik&quot; class=&quot;solr.TextFiled&quot;&gt; &lt;analyzer class=&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;/&gt;&lt;/fieldType&gt; 定义field，指定field的type属性为text_ik Solr管理索引库维护索引 添加/更新文档 使用Dataimport批量导入数据 导入solr-dataimporthandler.jar、solr-dataimporthandler-extras.jar、mysql数据库jar包 配置solrconfig.xml，添加一个requestHandler 12345&lt;requestHandler name=&quot;/dateimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 创建一个data-config.xml，保存到solrcore\conf\目录下 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;dataConfig&gt;&lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver url=&quot;jdbc:mysql://localhost:3306/database&quot; user=&quot;root&quot; password=&quot;root&quot;/&gt;&lt;document&gt; &lt;entity name=&quot;product&quot; query=&quot;SELECT pid,name,catalog_name,price,description,picture FROM products&quot;&gt; &lt;field column=&quot;pid&quot; name=&quot;id&quot;/&gt; &lt;field column=&quot;name&quot; name=&quot;product_name&quot;/&gt; &lt;field column=&quot;catalog_name&quot; name=&quot;product_catalog_name&quot;/&gt; &lt;field column=&quot;description&quot; name=&quot;product_price&quot;/&gt; &lt;field column=&quot;picture&quot; name=&quot;product_picture&quot;/&gt; &lt;/entity&gt;&lt;/document&gt;&lt;/dataConfig&gt; 重启tomcat Excute导入数据 删除文档- 删除指定ID的索引 123&lt;delete&gt; &lt;id&gt;1&lt;/id&gt;&lt;/delete&gt; - 删除查询到的索引数据&lt;*:*表示全部&gt; 123&lt;delete&gt; &lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt; 查询索引 q- 查询字符串 域名:条件 fq- filter query 过滤查询 sort- 排序 start,rows- 分页查询 fl- 返回指定字段内容，用逗号或空格分割 df- default Feild 默认域 wt- writer type 指定输出格式 hl- 是否高亮 使用SolrJ管理索引库使用客户端操作Solr比较繁琐低效，于是有SolrJ通过JAVA来访问Solr客户端 添加/更新文档 solrJ、slf4j-log4j12、jul-to-slf4j、jcl-over-slf4j1234567891011SolrServer solerServer = new HttpSolrServer(&quot;https://localhost:8080/solr&quot;);SolrInputDocument document = new SolrInputDocument();//这里FieldName在schema.xml已定义//当id已存在，相当于更新document.addField(FieldName,Value);solrServer.add(document);solrServer.commit(); 删除文档12345678SolrServer solrServer = new HttpSolrServer(&quot;http://localhost:8080/solr&quot;);//根据条件删除solrServer.deleteByQuery(&quot;*:*&quot;);solrServer.deleteById(id);solrServer.commit(); 查询文档查询所有12345678910111213141516SolrServer solrServer = new HttpSolrServer(&quot;https://localhost:8080/solr&quot;);SolrQuery query = new SolrQuery();query.setQuery(&quot;*:*&quot;);QueryResponse queryResponse = solrQuery.query(query);SolrDocumentList solrDocumentList = queryResponse.getResults();//查询到的数量solrDocumentList.getNumFound();//遍历查询结果for(SolrDocument solrDocument : solrDocumentList) &#123; solrDocument.get(fieldName);&#125; 复杂查询123456789101112131415161718192021222324252627282930313233343536373839404142SolrServer solrServer = new HttpSolrServer(&quot;http://localhost:8080/solr&quot;);SolrQuery query = new SolrQuery();//查询条件query.setQuery(KeyStr);//过滤条件query.setFilterQueries();//排序条件query.setSort(FieldNmae,ORDER.asc);//分页处理query.setStart(num);query.setRows(num);//结果中域的列表query.setFields(FiledNames);//默认搜索域query.set(FieldName);//高亮显示query.setHighlightField(true);//高亮显示前缀query.setHighlightSimplePre();//高亮显示后缀query.setHighlightSimplePost();//执行查询QueryResponse queryResponse = solrServer.query(query);//取查询结果SolrDocumentList solrDocumentList = queryResponse.getResults();//遍历查询结果for(SolrDocument solrDocument:solrDocumentList)&#123; //取高亮显示 String productName=&quot;&quot;; Map&lt;String,Map&lt;String,List&lt;String&gt;&gt;&gt; highlighting = queryResponse.getHighlighting(); List&lt;String&gt; list = highlighting.get(solrDocument.get(&quot;id&quot;)).get(product_name); //判断是否有高亮内容 if(list!=null)&#123; productName=list.get(0); &#125;else&#123; productName=(String)solrDocument.get(&quot;product_name&quot;); &#125;&#125;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[shiro]]></title>
    <url>%2F2017%2F03%2F16%2Fshiro%2F</url>
    <content type="text"><![CDATA[shiro是apache的一个开源的安全框架，支持认证一个或者多个数据源，可以对数据进行会话管理，对用户进行认证和授权，实现角色的细粒度权限控制，并且支持缓存技术。 核心组件安全管理器SecurityManager对外提供安全管理的各种服务 认证这个组件负责手机principals和credentials，并将它们提交给应用系统。如果提交的credentials跟应用系统中提供的credentials吻合，就能够继续访问，否则需要重新提交principals和credentials，或者直接终止访问。 授权身份验证通过后，由这个组件对登录人员进行访问控制的筛查，比如“who can do what”，或者“who can do which actions”。Shiro采用“基于Realm”的方法，即用户（又称Subject）、用户组、角色和permission的聚合体 sessionManager这个组件保证了异构客户端的访问。配置简单。它是基于POJO/J2EE的，不跟任何的客户端或者协议绑定 运行原理 Application Code应用程序代码，就是我们自己的编码，如果在程序中需要进行权限控制，需要调用Subject的API Subject主体，代表了当前的用户。所有的Subject都绑定到SecurityManager，与Subject的所有交互都委托给SecurityManager，可以将Subject当作一个门面，而真正执行者是SecurityManger SecurityManager安全管理器，所有与安全有关的技术都会与SecurityManage交互，并且它管理所有的Subject。 Realm域。Shiro是从Realm来获取安全数据（用户，角色，权限），就是说SecurityManager要验证用户身份，那么它需要从Realm得到用户响应的角色/权限进行验证用户是否能进行操作；可以把Realm看成DateSource，即安全数据源 四种权限控制 url权限控制 方法注解权限控制 代码级别权限控制 页面标签权限控制 和其他技术对比有什么优势]]></content>
      <categories>
        <category>框架集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis集群的搭建及使用]]></title>
    <url>%2F2017%2F03%2F01%2Frediscluster%2F</url>
    <content type="text"><![CDATA[小结一下，让自己以后参考这个步骤来搭建，避免出错 下载并解压 编译安装make &amp;&amp; make install 将redis-trib.rb复制到/usr/local/bin目录下 创建redis节点 首先在第一台机器上 /root/software/redis-3.0.0 目录下创建redisCluster目录mkdir redisCluster 在redisCluster目录下，创建名为7000,7001,7002的目录，并将redis.conf拷贝到这三个目录中 123mkdir 7000 7001 7002&lt;br&gt;cp redis.conf redisCluster/7000cp redis.conf redisCluster/7001cp redis.conf redisCluster/7002 分别修改这三个配置文件，修改如下内容 12345678prot 7000 //端口7000,7002,7003bind 本机ip //默认IP为127.0.0.1，需要修改为其他节点极其可访问的IP，否则创建集群时无法访问对应的端口，无法创建集群deamonize yes //redis后台启动pidfile /var/run/redis_7000.pid //pidfile文件对应7000,7001,7002cluster-enabled yes //开启集群 把注释#去掉cluster-config-file nodes_7000.conf //集群的配置 配置文件首次启动自动生成 7000，7001，7002cluster-node-timeout 15000 //请求超时 默认15秒，可自行设置appendonly yes //aof日志开启 有需要就开启，它会每次写操作都记录一条日志 接着在另外一台机器上的操作重复以上三步，只是把目录改为7003、7004、7005，对应的配置文件也按照这个规则修改即可 启动各个节点 123456789第一台机器上执行redis-server redisCluster/7000/redis.confredis-server redisCluster/7001/redis.confredis-server redisCluster/7002/redis.conf另外一台机器上执行redis-server redisCluster/7003/redis.confredis-server redisCluster/7004/redis.confredis-server redisCluster/7005/redis.conf 创建集群 Redis官方提供了redis-trib.rb这个工具，就在解压目录的src目录中，第三步中已将它复制到/usr/local/bin目录中，可以直接在命令行中适用了。该工具又ruby实现，需要安装ruby.安装命令如下 12yum -y install ruby ruby-devel rubygems rpm-buildgem install redis 使用下面这个命令即可安装 1redis-trib.rb create --replicas 1 192.168.31.245:7000 192.168.31.245:7001 192.168.31.245:7002 192.168.31.210:7003 192.168.31.210:7004 192.168.31.210:7005]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[dubbo小结第二弹]]></title>
    <url>%2F2017%2F01%2F24%2Fdubbo2%2F</url>
    <content type="text"><![CDATA[dubbo小结第二弹~ 是什么 阿里提供的开源的SOA服务化治理的技术框架。主要提供了服务注册、RPC服务调用、调用均衡、服务监控和服务failover(故障切换)等功能 透明化的远程方法调用，就像调用本地方法一样调用远程方法，只需简单配置，没有任何API侵入 软负载均衡及容错机制，可在内网替代F5等硬件负载均衡器，降低成本，减少单点。 服务自动注册与发现，不再需要写服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者 为什么用 当服务越来越多的时候，服务url配置管理变得非常困难，F5硬件服务器的单点压力也越来越多 增加一个动态的服务注册中心，动态的注册和发现服务，使服务的位置透明 通过在消费方获得服务方地址列表，实现软负载均衡和failover(故障切换)，降低对硬件的依赖，减少成本 当近一步发展，服务间的依赖关系变得错综复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述 自动画出应用间的依赖关系图，帮助架构师理清关系 接着，服务的调用量越来越大，服务的容量问题就暴露出来，这个服务需要多少机器支撑，什么时候该加服务器 将服务现在每天的调用量、响应时间都统计出来，作为容量规划的参考标准 动态的调整权重，在线上，将某个机器的权重一直增大，在加大的过程中记录响应时间的变化，直到响应时间达到阀值，记录此时的访问量，再以此访问量乘以机器量反推总容量 怎么用的如何通信 容器负责启动、加载、运行服务提供者 服务提供者在启动时，向注册中心注册自己的服务 服务消费者在启动时，向注册中心订阅自己所需要的服务 注册中心返回服务提供者列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者 服务消费者，从提供者列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用 服务提供者和消费者，在内存中累计调用次数和调用时间。 协议dubbo协议 采用NIO复用单一长连接，并使用线程池并发处理请求，减少握手和并发处理请求，性能较好（建议使用） 为什么用异步长连接：因为消费者多而提供者少，采用hession会压垮，通过长连接，减少握手。 为什么消费者多而提供者少：根据测试，每条连接最大为7M，千兆网卡为128M，理论上一个服务提供者需要20个消费者才能压满。 在大文件传输得时候，单一链接会成为瓶颈 假设每次请求数据包500kb,假设网络为千兆网卡，就是128MB，每条连接最大7MB，单个服务提供者每秒处理事务为（128m/500k）=262.单个消费者调用单个服务提供者的每秒处理事务最大为7mb/500=14。如果能接收，可以使用。 rmi协议 可与原生RMI互操作，基于TCP协议，堵塞式短链接和JDK标准序列化方式 偶尔会连接失败，需要重建存根 hession协议 可与原生hession协议互操作，基于http协议 需要hession.jar的支持，http短链接的开销比较大 注册中心和监控中心 注册中心 zookeeper注册中心 redis注册中心 multicast注册中心 simple注册中心 区别服务器挂了怎么办 注册中心挂了 注册中心挂了不影响服务的使用，但是不能重新发布服务 dubbo挂了 重启dubbo使用dubbo存在的问题 增加提供服务版本号和消费服务版本号 123&lt;dubbo:service interface=&quot;com.xxx.xxxService&quot; ref=&quot;xxxService&quot; version=&quot;1.0&quot; /&gt;&lt;dubbo:reference id=&quot;xxxService&quot; interface=&quot;com.xxx.xxxService&quot; version=&quot;1.0&quot;/&gt; 服务超时问题 服务请求超时→网络抖动：系统分析师的问题 调用的版本不对 提供者的服务被禁止 这是一种人为的控制，通过监控中心我们可以对具体的服务，以及它的权重进行控制，当我们将一个具体的服务禁止之后消费者就调不到相关的服务，此时就会出现超时的问题，解决方案，取消禁止即可，注意这里有一定时间的缓存，实际操作的时候应该注意 服务保护 服务保护的原则上是避免发生类似雪崩效应，尽量将异常控制在服务周围，不要扩散开。说到雪崩效应，还得提下dubbo自身的重试机制。默认3此，当失败时会进行重试，这样在某个时间点出现性能问题，然后调用房再连续重复调用。很容易引起雪崩，建议的话还是根据业务情况规划号如何进行异常处理，何时进行重试。服务保护的话，考虑服务的dubbo线程池类型（fix线程池的话考虑线程池大小）、数据库连接池、dubbo连接数限制是否都符合 注册中心的分组group和服务的不同实现group 这两个东西完全不同的概念，使用的时候不要弄混了。registry上可以配置group，用于区分不同分组的注册中心，比如在同一个注册中心下，有一部分注册信息是要给开发环境用的，有一部分注册信息是要给测试环境用的，可以分别用不痛的group区分开，目前对这个理解还不透彻，大致就是用于区分不同环境。service和reference上也可以配置group，这个用于区分同一个接口的不同实现，只有在reference上指定与service相同的group才会被发现 dubbo的序列化 dubbo远程调用服务原理是通过序列化、反序列化实现的 dubbo是采用Hessian（比jdk自带的反序列化高效）进行反序列化的，该反序列化创建对象时，会取最少的构造方法创建对象，构造方法参数设置默认值，基本类型设置为响应基本类型的默认值，不是基本类型设置为null。通过新增午餐构造方法解决了问题 守护线程]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式文件系统FastDFS]]></title>
    <url>%2F2017%2F01%2F11%2FFastDFS%2F</url>
    <content type="text"><![CDATA[传统图片上传不能有效应对集群的方式，单独搭建一个图片服务器。可以使用分布式文件系统FastDFS方式。实现服务器的高可用 什么是FastDFSFastDFS是用C语言编写的一款开源的分布式文件系统。FastDFS位互联网量身定制，充分考虑了冗杂备份，负载均衡，线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务集群提供文件上传、下载等服务 FastDFS架构 FastDFS架构包括Tracker server和Storage server。客户端请求Tracker server进行文件上传、下载，通过tracker server调度最终由Storage server完成文件上传和下载。 Tracker server作用是负载均衡和调度，通过Tracker server在文件上传时可以根据一些策略找到Storage server提供文件上传服务。可以将Tracker称为追踪服务器或调度服务器。 Storage server作用是文件存储，客户端上传的文件最终存储在Storage服务器上，Storage server没有实现自己的文件系统而是利用操作西戎的文件系统来管理文件。可以将storage称为存储服务器. 服务端两个角色： Tracker：管理集群，tracker也可以实现集群。每个tracker节点地位平等。 Storage:实际保存文件。Storage氛围多个组，每个组之间保存的文件是不同的。每个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有主从概念。 文件上传的流程图客户端上传文件后存储服务器将文件ID返回给客户端，此文件ID用于以后访问该文件的索引信息。文件索引信息包括：组名，虚拟磁盘路径，数据两级目录，文件名。 组名：文件上传后所在的storage组名称，在文件上传成功后有storage服务器返回，需要客户端自行保存。 虚拟磁盘路径:storage配置的虚拟路径，与磁盘选项store_path*对应。如果配置了store_path0则是M00，如果配置了store_path1则是M01,以此类推。 数据两级目录：storage服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件。 文件名：与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含：源存储服务器IP地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。 文件下载 FastDFS使用上传文件123456789101112//加载配置文件，配置文件内容就是tracker服务的地址ClientGlobal.init(AbsoluteAddress);//创建一个TrackerClient对象TrakcerClient trackerClient = new TrackerClient();//使用TrackerClient对象创建连接，获得一个TrackerServer对象TrackerServer trackerServer = trackerClient.getConnection();//创建一个StorageServer的引用StorageServer storageServer = null;//创建一个StorageClient对象，提供两个Server对象StorageClient storageClient = new StorageClient(trackerServer,storageServer);//使用StorageClient对象上传图片String[] result = storageClient.upload_file(&quot;D:/a.jpg&quot;,&quot;jpg&quot;,null); 使用工具类上传1234//加载配置文件FastDFSClient fastDFSClient = new FastDFSClient(AbsoluteAddress);//上传文件String file = fastDFSClient.uploadFile(AbsoluteAddress); 整合SpringSpringMvc.xml设定文件上传解析器1234567&lt;!-- 定义文件上传解析器 --&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- 设定默认编码 --&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;!-- 设定文件上传的最大值,这里设5MB --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;5242888&quot;/&gt; &lt;/bean&gt;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis小结]]></title>
    <url>%2F2016%2F11%2F28%2Fredis2%2F</url>
    <content type="text"><![CDATA[redis是C语言写的，存储key-value类型的非关系型数据库，可基于内存也可持久化的日志型数据库，适合作为缓存使用 数据类型String（字符串）特点最简单的类型，key-value，二进制安全的 应用场景任何场景都使用：商品编号、订单号采用String的递增特性生成（incr） hash(散列)特点 field和value之间的映射，即键值对的集合，所以特别适合存储对象 每个hash最多可以存储2^32-1键值对（40多亿） hash对应的value内部实际是一个hashmap，这里有两种实现方式 hash的成员比较少时，Redis为了节省内存会采用一对数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap 当成员数量增大时会自动转成真正的HashMap 应用场景首页广告 常用命令hget、hset、hgetall list(列表)特点 链表类型，主要功能是push、pop、获取一个范围的所有值等 List类型是按照插入顺序排序的字符串链表。比如使用LPUSH命令在list头插入一个元素，使用RPUSH命令在list的尾插入一个元素。当这两个命令之一作用于一个空的key时，一个新的list就创建出来了 List的最大长度是2^32-1个元素 list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构 应用场景 商品评论列表 最新消息排行 消息队列，可以利用Lists的PUSH操作，将任务存在List中，然后工作线程再用POP操作将任务取出进行执行 常用命令lpush、rpush、(插入)lpop、rpop、（返回并删除）lrange、（获取start到stop）BLPOP(返回并删除第一个元素，堵塞并直到有可用，不建议使用) set（集合）特点 set是String的无序集合 集合成员是唯一的，这就意味着集合中不能出现重复的数据 Redis集合是 通过哈希表实现的 支持随机获取元素 支持集合间的取差集、交集与并集操作 set的内部实现是一个value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的集合应用场景微博应用中，每个人的好友存在一个集合（set）中，这样求两个人的共同好友的操作可能就只需要用求交集命令即可 zset(sorted set)（有序集合）特点 有序集合和集合一样是String类型的元素的集合，且不允许重复的成员 不同的是每个元素都会关联一个double类型的分数，Redis正是通过分数来为集合中的元素排序，scope越低排名越靠前。 zet的内部使用HashMap和跳跃表（SkipList）来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序一句是HashMap里存的score，使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单应用场景以某个条件为群暗中，比如按顶的次数排序需要精准设定过期时间的应用 持久化RDB 方式 快照形式是直接把内存中的数据保存到一个dump文件中，定时保存 特点 半持久方式 不定期的通过异步方式保存到磁盘上 在redis.conf中进行配置 格式：save N M 含义：表示在N秒之内，redis至少发生M次修改则redis抓快照到磁盘 原理 当redis需要坐持久化时，redis会fork一个子进程；子进程将数据写到磁盘上的一个临时RDB文件中；当子进程完成写临时文件后，将原来的RDB替换掉，这样的好处就是可以copy-on-write（写时拷贝） 优点 只包含一个文件，对于文件备份比较实用 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其他存储介质上 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要坐的是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了 相比于AOF机制，如果数据集很大，RDB的启动效率会更高 缺点 如果你想保证数据的高可用性，即最大限度的避免是数据丢失，那么RDB不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失 由于RDB是通过fork子进程来协助完成数据持久化工作的。因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟AOF 方式 把所有对REDIS的服务器进行修改的命令都存到一个文件里，命令的集合 特点 全持久化方式 每一个写命令都通过write函数追加到appendonly.aof中 配置方式 修改appendonly yes 默认是no 文件刷新三种方式 appendfsync appendfsync always（每提交一个修改命令都调用fsync刷新到AOF文件，非常非常慢，但也非常安全） appendfsync everysec（每秒都调用fsnyc刷新AOF文件，很快，但可能会都市1秒内的数据） 推荐使用 appendfsync no（依靠OS进行刷新，redis不主动刷新，这样最快，但安全性就差） 原理 以日志的形式记录服务器所处理的每一个写、删除操作，查询操作不会记录，以文本的方式进行记录，可以打开文本看到详细的操作记录 优点 较高的数据安全性，即数据持久性 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容 如果日志过大，redis可以自动启用rewrite机制 AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作 缺点 对于相同的数据集而言，AOF文件通常要大于RDB文件。RDB在恢复大数据时的速度比AOF快 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RBD一样高效。比较 牺牲一些性能，换取更高的一致性（AOF） 写操作频繁的时候，不启用备份来换取更高的性能，待手动运行save的时候，再做备份（RDB） Redis的漏洞漏洞介绍Redis默认情况下，会绑定在0.0.0.0：6379，这样会将Redis服务暴露到公网上，如果没有开启认证的情况下，可以导致任意用户在可以访问目标服务器的情况下未授权访问Redis以及读取Redis的数据。攻击者在未授权访问Redis的情况下可以利用Redis的相关方法，可以成功在Redis服务器上写入公钥，进而可以使用对应私钥直接登录目标服务器 如何入侵 Redis可能执行过FLUSHALL方法，整个Redis数据库被清空 在Redis数据库中新建了一个名为crackit（网上流传的命令指令）的键值对，内容为一个SSH公钥。 在/root/.ssh文件夹下新建或者修改了authrized_keys文件，内容为Redis生成的db文件，包含上述公钥修复 禁止一些高危命令 修改Redis.conf文件，禁用远程修改DB文件地址 rename-command FLUSHALL “” rename-command CONFIG “” rename-command EVAL “” 以低权限运行Redis服务 为Redis服务创建单独的用户和家目录，并且配置禁止登录 为Redis添加密码验证 修改redis.conf文件，添加requirepass password 禁止外网访问Redis 修改redis.conf文件，添加或修改 bind 127.0.0.1 使得Redis服务只在当前主机可用 做Log监控，及时发现攻击]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dubbo]]></title>
    <url>%2F2016%2F11%2F09%2FDubbo%2F</url>
    <content type="text"><![CDATA[Dubbo是阿里的开源分布式服务架构，可通过高性能的RPC实现服务的输出和输入功能。 系统通信如何实现远程通信 WebService：效率不高基于soap协议。 使用Restful形式的服务：http+json。很多项目中应用。如果服务太多，服务之间调用关系混乱，需要治疗服务。 dubbo。使用rpc协议进行远程调用，直接使用socket通信。传输效率高，并且可以统计出系统之间的调用关系、调用次数。 规模架构 单一应用架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 此时，用于简化增删改差工作量的数据访问框架(ORM)是关键。 垂直应用架构 当访问量逐渐增大，单一应用增加极其带来的加速度越来越小，将应用拆成无不想干的几个应用，以提升效率 此时，用于加速前端页面开发的Web框架(MVC)是关键 分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 此时，用于提高业务服用及整合的分布式服务架构(RPC)是关键。 流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。 此时，用于提高极其利用率的**资源调度和治理中心(SOA)是关键。 Dubbo 节点角色说明： Provider:暴露服务的服务提供方 Consumer:调用远程服务的服务消费方 Registry:服务注册与发现的注册中心 Monitor:统计服务的调用次数调和调用时间的监控中心 Container:服务运行容器 调用关系说明： 0.服务容器(Container)负责启动、加载、运行服务提供方(Provider) 1.服务提供方(Provider)在启动时，向注册中心(Registry)注册自己提供的服务。 2.服务消费方(Consumer)在启动时，向注册中心(Registry)订阅自己所需的服务。 3.注册中心(Registry)返回服务提供方(Provider)地址列表给消费者，如果有变化，注册中心将基于长连接推送变更数据给消费方(Consumer). ４。服务消费方(Consumer)，从提供者地址列表中，基于软负载均衡算法，选一台提供方(Provider)进行调用，如果调用失败，再选另一台调用. 5.服务消费者(Consumer)和提供方(Provider)，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心(Monitor)使用方法 Spring配置Dubbo采用全Spring配置方式，透明化接入应用，对应用没有任何API侵入，只需用Spring加载Dubbo的配置即可，Dubbo基于Spring的Schema拓展进行加载。 单一工程中spring的配置 1234&lt;bean id=&quot;xxxService&quot; class=&quot;com.xxxServiceImpl&quot; /&gt;&lt;bean id=&quot;xxxAction&quot; class=&quot;com.xxx.xxxAction&quot;&gt; &lt;property name=&quot;xxxService&quot; ref=&quot;xxxService&quot;&gt;&lt;/bean&gt; 远程服务 将上述local.xml配置拆分成两份，将服务定义部分放在服务提供方remote-provier.xml,将服务引用部分放在服务消费方remote-consumer.xml并在provider增加暴露服务配置&lt;dubbo:service&gt;,在consumer增加引用服务配置&lt;dubbo:reference&gt; 发布服务1234&lt;!-- 和本地服务一样实现远程服务 --&gt;&lt;bean id = &quot;xxxService&quot; class = &quot;com.xxx.xxxServiceImpl&quot;/&gt;&lt;!-- 增加暴露远程服务配置 --&gt;&lt;dubbo:service interface=&quot;com.xxx.xxxService&quot; ref=&quot;xxxService&quot;&gt; 调用服务123456&lt;!-- 增加引用远程服务配置 --&gt;&lt;dubbo:reference id =&quot;xxxService&quot; interface=&quot;com.xxx.xxxService&quot;/&gt;&lt;!-- 和本地服务一样使用远程服务&gt;&lt;bean id =&quot;xxxAction&quot; class=&quot;com.xxx.xxxAction&quot;&gt; &lt;property name=&quot;xxxService&quot; ref=&quot;xxxService&quot;&gt;&lt;/bean&gt; 注册中心(Registry)注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力娇小，使用dubbo2.3.3以上版本，建议使用zookeeper注册中心zookeeper是Apache Hadoop的紫霞公募，是一个属性的目录服务，支持变更推送，适合作为Dubbo服务的注册中心 Zookeeper 安装 安装jdk 上传并解压zookeeper 将conf文件夹下的zoo-sample.cfg复制一份，改名位zoo.cfg 修改zoo.cfg的dataDir属性，指定zookeeper的真实目录 修改zoo.cfg的clientport属性，指定该服务器的zookeeper端口 启动zookeeper:zookeeper/bin/zkServer.sh start 关闭zookeeper:zookeeper/bin/zkServer.sh stop 查看zookeeper:zookeeper/bin/zkServer.sh status 框架整合Dubbo监控中心 https://github.com/alibaba/dubbo下载dubbo将dubbo-admin打成war包 1mvn package -Dmaven.test.skip=true 将war包复制到tomcat/webapps/中。修改/WEB-INF/dubbo.properties。包括地址和用户密码]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库优化]]></title>
    <url>%2F2016%2F10%2F29%2Foptimize%2F</url>
    <content type="text"><![CDATA[浅析数据库的优化 SQL语句优化 对查询进行优化，尽量避免全表查询，首先考虑在where以及order by 涉及的列上建立索引。 应尽量避免在where子句中字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描 应尽量避免在where子句中使用!=或&lt;&gt;操作符，否则将导致引擎放弃使用索引而进行全表扫描。 应尽量避免在where子句中使用or来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，可以使用union all来代替 in 和not in 也要慎用，否则会导致全表扫描，对连续的数据用between 代替，也可以用exists 代替in like ‘%a%’这种，索引失效，可以使用全文索引代替 对于多张大数据量（几百条就算）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差 少使用* 日期使用mysql自带的，少使用字符串存储，字符串的比较复杂；IP也使用int类型，不要使用字符串 尽可能的使用varchar/nvarchar代替char/nchar，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对娇小的字段内搜索效率显然要高些。 数据库优化主从复制（读写分离）使用spring可以实现读写分离 分库分表 分表 垂直分表 将表按照功能模块、关系密切程度划分出来，部署到不同的库上，比如我们会建立定义数据库workDB、商品数据库payDB、用户数据库userDB、日志数据库logDB等，分别用于存储项目数据定义表、商品定义表、用户数据表、日志数据表等 水平分表 指定自己的规则：1、求余。2、哈希。3、时间 分库 分库就是把一张表的数据分成N多个区块，这些区块可以在同一个磁盘上，也可以在不同的磁盘上 应用场景 表太多，海量数据，各项业务划分清除，低耦合的话，用垂直拆分比较号 表不多，单表数据量大的话，选水平拆分比较号]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[存储引擎]]></title>
    <url>%2F2016%2F10%2F26%2FStorageEngine%2F</url>
    <content type="text"><![CDATA[MySQL中的数据用各种不同的技术存储在文件（或者内存）中。这些技术中的每一种技术都使用不同的存储机制、索引技巧、锁定水平并且最终提供广泛的不同的功能和能力 查看mysql支持的存储引擎（SHOW ENGINES） myisam和innodb的区别 存储结构 MyISAM每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，拓展名指出文件类型。.frm文件存储表定义。数据文件的拓展名为.MYD(MYDate)。索引文件的拓展名是.MYI(MYIndex) InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB 存储空间 MyISAM可被压缩，存储空间较小。支持三种不同的存储格式：静态表（默认，但是主义数据末尾不能有空格，会被去掉）、动态表、压缩表 innoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于告诉缓冲数据和索引 可移植性、备份及恢复 MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个进行操作。 innoDB免费的方案可以是拷贝数据文件、备份binlog，或用mysqldump，在数据量达到几十GB的时候就相对痛苦了。 事务支持 MyISAM强调的是性能，每次查询具有原子性，其执行速度比InnoDB类型要快，但是不提供事务支持。 innoDB提供事务支持，外部键等高级数据库功能。具有事务（commit），回滚（rollback）和崩溃修复能力（crash recovery capablities）的事务安全（transaction=safe(ACID compliant)）型表 表锁差异 MyISAM只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据 innoDB支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的性能，但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的 AUTO-INCREMENT MyISAM可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 innoDBInnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引。如果是组合索引也必须是组合索引的第一列。 全文索引 MyISAM支持FULLTEXT类型的全文索引。 innoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好 表主键 MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。 innoDB如果没有设定主键或空孔唯一索引，就会自动生成一个6字节的主键（用户不可见），数据是主索引的一部分，附加索引保存的是主索引的值。 表的具体行数 MyISAM保存表的总行数，如果select count(*) from table会直接取出该值 innoDB没有保存表的总行数，如果使用select count(*) from table；会遍历整个表，消耗相当大，但是在加了where条件后，myisam和innodb处理的方式都一样。 CRUD操作 MyISAM如果执行大量的select，MyISAM是更好的选择 innoDB如果你的数据执行大量的insert或update，出于性能方面的考虑，应该使用InnoDB表。DELETE从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在InnoDB上如果要清空保存有大量数据的表，最好使用truncate table这个命令 外键 MyISAM不支持 innoDB支持]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[事务Transaction]]></title>
    <url>%2F2016%2F10%2F23%2Ftransaction%2F</url>
    <content type="text"><![CDATA[Transaction是指作为单个逻辑工作单元执行的一系列操作，要么完全地执行，要么完全地不执行。 事务的四个属性 原子性 事务是由一个或一组相互关联的SQL语句组成，这些语句被认为是一个不可分割的单元 一致性 对于数据库的修改是一致的，即多个用户查的数据是一样的。一致性主要由mysql的日志机制处理，他记录数据的变化，为事务恢复提供跟踪记录。 隔离性 每个事务都有自己的空间，和其他发生在系统中的事务隔离开来，而且事务的结果只在他完全被执行时才能看到 持久性 提交了这个事务之后对数据的修改更新就是永久的。当一个事务完成，数据库的日志已经被更新时，持久性即可发挥其特有的功效，在mysql中，如果系统崩溃或数据戒指被破坏，通过日志，系统能够恢复在重启前进行的最后一次成功更新，可以反应系统崩溃时处于执行过程的事物的变化 事务的四种隔离级别READ UNCOMMITTED(未提交读)事务A对数据做的修改，即使没有提交，对于事务B来说也是可见的，这种问题叫脏读 READ COMMITTED（提交读）事务A对数据做的修改，提交之后会对事务B可见。 &gt; 举例：事务B开启时独到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，会独到最新的数据2 REPEATABLE READ(可重复读)事务A对数据做的修改，提交之后，对于先于事务A开启的事务是不可见的。 &gt; 举例：事务B开启时读到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，仍然独到数据1 SERIALIZABLE（可串行化）可串行化是最高的隔离级别。这种隔离级别强制要求所有事物串行执行，在这种隔离级别下，读取的每行数据都加锁，会导致大量的锁征用问题，性能最差 &gt; 随着隔离级别的增高，并发性能也会降低 mysql中的事务 事务的实现是基于数据库的存储引擎的。不同的存储引擎对事务的支持成都不一样 mysql支持的存储引擎中支持事务的InnoDB 事务的隔离级别是通过锁实现的，而事务的原子性、一致性和持久性是通过事务日志实现的 事务日志 redo——保障了事务的持久性和一致性 在InnoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志现形”。当事务提交之后，在BUFFER POOL中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态，未完成的事务，可以继续提交，也可以选择回滚，这是基于恢复的策略而定 undo——保障了事务的原子性 undo log主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。每个事务的回滚，只会当回滚当前事务做的操作，并不会印象到其他的事务做的操作]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[锁Lock]]></title>
    <url>%2F2016%2F10%2F19%2FLock%2F</url>
    <content type="text"><![CDATA[数据库是一个多用户使用的共享资源。当多个用户并发地存取数据时，在数据库中就会产生多个事务同时存取同一数据的情况。若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据库的一致性。加锁是实现数据库并发控制的一个非常重要的技术。当事务在对某个数据对象进行操作前，先向系统发出请求，对其加锁。加锁后事务就对该数据对象有了一定的控制，在该事务释放锁之前，其他的事务不能对此数据对象进行更新操作。 表锁行锁（InnoDB）共享锁又成为读锁，多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改 &gt; SELECT ... LOCK IN SHARE MODE 排他锁又称为写锁。排它锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行数据和修改。排他锁指的是一个事务在一行数据加上排它锁后，其他事务不能再加其他的锁。updata,delete,isnert都会自动给设计到的数据加上排他锁，select语句默认不会加任何锁类型 &gt; SELECT ... FRO UPDATE 乐观锁 乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行就爱南侧，如果发现冲突了，则返回用户错误的信息，让用户决定如何去做 实现方式 版本（Version）记录机制 一般是通过为数据库表增加一个数字类型的”version”字段来实现。当读取数据时，将version字段的值一同独处，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比较。如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据 时间戳（timestamp） 悲观锁它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守状态（悲观），因此，在整个数据处理过程中，将数据处于锁定状态。 &gt; SELECT ... FOR UPDATE 或 LOCK IN SHARE MODE]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库之索引]]></title>
    <url>%2F2016%2F10%2F17%2FIndex%2F</url>
    <content type="text"><![CDATA[索引是对数据库表中一列或多列的值进行排序的一种结构，使用索引可快速访问数据库表中的特定信息。 索引概述 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，他们包含着数据表里所有文件的引用指针。可以类比为书的目录，可以加快数据库的查询速度。 索引是创建在数据表对象上的。由表中的一个字段或多个字段生成的键组成，这些键存储在数据结构（B-树或哈希表）中，通过mysql可以快速有效的查找与键值相关联的字段 根据存储类型，分为B型树索引和哈希索引 InnoDB和MyISAM支持BTREE类型索引，默认 Memoery支持HASH类型的索引 索引分类普通索引最基本的索引，没有任何限制。MyIASM中默认的BTREE类型的索引，经常用 唯一索引与普通索引类是，不同的是：索引的列必须唯一，但允许有空值 主键索引它是一种特殊的唯一索引，不允许有空值 全文索引 FULLTEXT索引仅仅可用于MYISAM表 他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加 注意：大容量的表，使用全文索引虽然速度更快，但是生产全文检索是一个非常消耗时间消耗磁盘空间的做法 组合索引 所谓多列索引，是指在创建索引的时候，锁关联的字段不是一个字段，而是多个字段。 虽然可以通过所关联的字段进行查询，但是只有查询条件中使用了所关联字段中的第一个字段，多列索引才会被使用 应用场景什么情况适合创建索引 经常被查询的字段，即在WHERE子句中出现的字段 在分组的字段，即在GROUP BY子句中出现的字段 存在依赖关系的子表和父表之间的联合查询，即主键或外键字段 设置唯一完整性约束的字段 什么情况不适合创建索引 在查询中很少使用的字段 拥有许多重复值的字段 索引失效 使用or关键字的时候索引失效，要想使用or,又想使用索引，只能将or条件中的每个列都加上索引 对于多列索引，不是使用的第一部分，则不会使用索引 like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引 如果mysql估计使用全表扫描要比使用索引快，则不使用索引 对索引列进行运算导致索引失效(+，-，*，/，！等) 独立的列（对列变量需要计算（聚合运算、类型转换等）） 在JOIN操作中（需要从多个数据表提取数据时），MYSQL只在主键和外键的数据类型相同时才能使用索引，否则即使建立了索引也不会使用 不使用NOT IN和&lt;&gt;操作，不会使用索引将进行全表扫描，NOT IN 可以NOT EXISTS代替，ID&lt;&gt;3则可以用id&gt;3 or id &lt; 3来代替 索引不会包含有NULL值的列，只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 查看索引和优化索引 查看索引show status like &#39;Handler_read%&#39; 优化索引 索引不会包含有NULL值的列，因此数据库设计时不要让字段的默认值为NULL 使用短索引 例如：如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是唯一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 索引列排序 MySQL查询只使用一个索引，因此如果WHERE子句中已经使用了索引的话，那么order by 中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引 不要在列上进行运算]]></content>
      <categories>
        <category>数据库集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringWebMvc小结]]></title>
    <url>%2F2016%2F10%2F13%2FSpringWebMvc%2F</url>
    <content type="text"><![CDATA[学习SpringWebMvc的个人小结 SpringWebMvc执行流程 用户请求给前端控制器DispatcherServlet 前端控制器给HandlerMapping处理器映射器 处理器映射器根据映射找到Handler处理器 处理器返回ExcutionChain数据给前端控制器 前端控制器找到HandlerAdapter处理器适配器 处理器适配器找到处理器执行后返回ModelAndView给前端控制器 前端控制器把ModelAndView解析位Model和View，把Model赋值给View HandlerMapping ExcutionChain HandlerAdapter ModelAndView 视图解析器 解析Model赋值给View SpringMvc.xml配置文件 扫描Controller &lt; mvc:annotation-driver/&gt; 视图解析器 默认支持的数据类型 request response session Model ModelMap Map 基本数据类型 pojo类(属性名相同) 自定义参数类型 Converter 时间注解方式：pojo属性@DateTimeFormat(pattern=”yyyy-MM-dd”)]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线程知识]]></title>
    <url>%2F2016%2F09%2F15%2FThread%2F</url>
    <content type="text"><![CDATA[线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对运算密集型任务提速。 线程和进程的区别进程正在运行的程序。确切的来说，当一个程序进入内存运行，即编程一个进程，进程是处于运行过程中的程序，并且具有一定独立功能 线程线程是进程中的一个执行单元，负责当前进程中程序的执行，一个进程中至少有一个线程。一个进程中是可以有多个线程的。这个应用程序也可以称之为多线程程序。 如何在java中实现线程继承Thread类 Thread类本质也实现了Runnable接口 class Thread implements Runnable 实现代码 在自己的类直接继承Thread接口，并重写run()方法 12345public class MyThread extends Thread&#123; public void run()&#123; ........ &#125;&#125; 在需要的地方启动线程 1234MyThread myThread1 = new MyThread();MyThread myThread2 = new MyThread();myThread1.start();myThread2.start(); 实现Runnable接口 自己的类实现Runnable接口，重写run()方法 12345public class MyThread implements Runnable()&#123; public void run()&#123; .....; &#125;&#125; 123MyThread myThread = new MyThread();Thread thread = new Thread(myThread);thread.start(); 使用ExecutorService、Callable、Future实现有返回结果的多线程 ExecutorService、Callable、Future这个对象实际上都是属于Executor框架中的功能类 执行Callable任务后，可以获取一个Future对象，在该对象上调用get就可以获取到Callable任务返回的Object了，再结合线程池接口ExecutorService就可以实现有返回结果的多线程了 1234567891011121314151617 class MyCallable implements Callable&lt;Object&gt;&#123; private String taskNum; MyCallable(String taskNum)&#123; this.taskNum = taskNum; &#125; public Object call() throws Exception&#123; System.out.println(&quot;&gt;&gt;&gt;&quot;+taskNum+&quot;任务开始&quot;); Date dateTpm1 = new Date(); Thread.sleep(1000); Date dateTmp2 = new Date(); long time = dateTmp2.getTime() - dateTemp1.getTime(); System.out.println(&quot;&gt;&gt;&gt;&quot;+taskNum+&quot;任务停止&quot;); return taskNum+&quot;任务返回运行结果，当前任务时间【&quot;+time+&quot;】毫秒&quot;； &#125;&#125; 123456789101112131415161718192021222324Date date = new Date();int taskSize = 5;// 创建一个线程池ExecutorService pool = Executors.newFixedThreadPool(taskSize);// 创建多个有返回值的任务List&lt;Future&gt; list = new ArrayList&lt;Future&gt;();for(int i = 0 ; i &lt; taskSize ; i++)&#123; Callable c = new MyCallable(i+&quot; &quot;); // 执行任务时获取Future对象 Future f = pool.submit(c); list.add(f);&#125;// 关闭线程池pool.shutdown();// 获取所有并发任务的运行结果 for(Future f : list)&#123; // 从Future对象上获取任务的返回键，并输出到控制台 System.out.println(&quot;&gt;&gt;&gt;&quot;+f.get().toString());&#125;Date newDate = new Date();System.out.println(&quot;---程序结束运行---，程序运行时间【&quot;+(newDate.getTime() - date.getTime())+&quot;】毫秒&quot;); 常用的创建线程的线程池线程池线程池的基本思想还是一种对象池的思想，开辟一块内存空间，里面存放了众多（未死亡）的线程，池中线程执行调度由池管理器来处理。当有线程任务时，从池中取一个，执行完成后线程对象归池，这样可以避免反复创建线程对象所带来的性能开销，节省了系统的资源。 创建方式 newCachedThreadPool(可缓存线程池) 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空间线程，若无可回收，则新建线程 特点 工作线程的创建数量几乎没有限制（其实也有娴熟，数目为Integer.MAX_VALUE），这样可灵活的往线程池中添加线程 如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间（默认为1分钟），则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。 在使用CachedThreadPool时，一定要主义控制任务的数量，否则，由于大量线程同时运行，很有可能会造成系统瘫痪 newFixedThreadPool（指定工作线程数量的线程池。 创建一个指定工作数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中 FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源。 newSingleThreadExecutor(单线程化的线程池) 创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序（FIFO，LIFO，优先级）执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的。 newScheduleThreadPool（定长的线程池） 创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。实现接口和集成线程类直接的不同 接口可以避免由于java的单继承特性带来的局限 增强程序的健壮性，代码能够被多个线程共享，代码与数据是独立的 适合多个相同程序代码的线程区处理同一资源的情况 线程的状态 新建状态（NEW） 当线程对象创建后，即进入了新建状态，如：Thread t = new MyThread(); 就绪状态（Runnable） 当调用线程对象的star()方法后，线程即进入就绪状态。处于就绪状态的线程，只是说明线程已经做好了准备，随时等待CPU调度执行，并不是说执行了star方法，此线程就会立即执行。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程想要进入运行状态执行，首先必须处于就绪状态中； 阻塞状态（Blocked） 处于运行状态中的线程由于某种原因，暂时放弃对CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才有机会再次被CPU调用以进入到运行状态 等待堵塞：运行状态中的线程执行wait()方法，使本线程进入到等待阻塞状态 同步堵塞：线程在获取synchronized同步锁失败（因为锁被其他线程所占用），它会进入同步阻塞状态； 其他堵塞：通过调用线程的sleep()或join()或发出了I/O请求时，线程会进入到阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。 死亡状态（Dead） 线程执行完了或者因异常退出了run()方法，该线程结束生命周期。 常用的一些方法 start用来启动一个线程，当调用start方法后，系统才会开启一个新的线程来执行用户定义的子任务，在这个过程中，会为相应的线程分配需要的资源 run是不需要用户来调用的，当通过start方法启动一个线程之后，当线程获得了CPU执行时间，便进入run方法提去执行具体的任务。注意，继承Thread类必须重写run方法,在run方法中定义具体要执行的任务 sleep（sleep(long millis)） sleep(long mills,int nanoseconds) // 第二个参数是纳秒sleep相当于让线程睡眠，交出CPU，让CPU去执行其他的任务。但是又一点要非常注意，sleep方法不会释放锁，也就是说如果当前线程持有对某个对象的锁，则即使调用sleep方法，其他线程也无法访问这个对象。 yield调用yield方法会让当前线程交出CPU权限，让CPU去执行其他的线程。她跟sleep方法类似，同样不会释放锁。但是yield不能控制具体的交出CPU的时间，另外，yield方法只能让拥有相同优先级的线程CPU执行时间的机会。 调用yield方法并不会让线程进入阻塞状态，而是让线程重回就绪状态，她只需要等待重新获取CPU执行时间，这一点是和sleep方法不一样的。 join中端其他线程的执行，等待调用join方法的线程结束，即使是主线程main也会被中断。join()join(long millis) //参数为毫秒join（long millis,int nanoseconds） //第二个参数是纳秒 多个线程之间的通信 多个线程在处理同一个资源，但是处理的动作（线程的任务）却不相同。通过一定的手断使各个线程能有效的利用资源。 通常情况下，一个次级线程要为主线程完成某种特定类型的任务，这就隐含着表示在主线程和次线程之间需要简历一个通信的通道。 一般情况下，又下面集中方法实现这种通信任务：使用全局变量、使用事件对象、使用消息。 线程安全问题什么时候会出现线程安全问题 在单线程中不会出现线程安全问题，而在多线程编程中，有可能会出现同时访问同一个资源的情况，这种资源可以是各种类型的资源：一个变量、一个对象、一个文件、一个数据库等。而当多个线程同时访问一个资源的时候，就会存在一个问题：由于每个线程执行的过程是不可控的，所以很可能导致最终的结果与实际上的愿望相违背或者直接导致程序出错 当多个线程执行一个方法，方法内部的局部变量并不是临界资源，因为方法是在栈上执行的，而java栈是线程私有的，因此不会产生线程安全问题。如何解决线程 序列化访问临界资源，即在同一时刻，只能有一个线程访问临界资源，也称同步互斥访问。通常来说，是在访问临界资源的代码前面加一个锁，当访问完临界资源后释放锁，让其他线程继续访问。 两种方案实现同步互斥访问。 将操作共享数据的代码行作为一个整体，同一时间只允许一个线程执行，执行过程中其他线程不能参与执行 synchronized 同步方法 有synchronized关键字修饰的方法，由于java的每个对象都有一个内置锁，当用此关键字修饰方法时，内置锁会保护整个方法，在调用该方法前，需要获得内置锁，否则就处于阻塞状态。 当两个并发线程访问同一个对象object中的这个synchronized(this)同步代码块时，一个时间内只能又一个线程得到执行。另一个线程必须等待当前执行完这个代码块以后才能执行该代码块。 然后，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。 尤其关键的是，当一个线程访问object的一个synchronized(this)同步代码块时，其他线程对object中所有其他synchronized(this)同步代码块的访问将被阻塞。（synchronized锁定所有的同步代码块，一旦阻塞，所有同步代码块都被阻塞）。 第三个例子同样使用其他同步代码块。也就是说，当一个线程访问object的一个synchronized(this)同步代码块时，它就获得了这个object的对象锁。结果，其他线程对该object对象所有同步代码部分的访问都被暂时阻塞。（synchronized获取整个对象的锁，整个对象将被锁定） 同步代码块 即又synchronized关键字修饰的语句块。被该关键字修饰的语句块会自动被加上内置锁，从而实现同步。 lock jdk1.5之后出现的，并发包中新增了Lock接口以及相关实现类用来实现锁功能，lock接口提供的功能与synchronized类似的同步功能，但需要手动释放锁和获得锁。 lock接口却提供了锁的可操作性，可中断获取锁一级超时获得锁等 有两个非常强大的实现类重入锁和读写锁 如何使用 12345678910Lock lock = new ReentrantLock();Lock.lock();try&#123; //可能会出现线程安全的操作&#125;finally&#123; //一定在finally中释放锁 //也不能吧获取锁在try中进行，因为有可能在获取锁的时候抛出异常 lock.ublock();&#125; 常用方法 void lock()：获取锁，调用该方法当前线程将会获取锁，当锁获取后，该方法将返回。 void lockInterruptibly() throws InterruptedException 可中断获取锁，与lock()方法不同之处在于该方法会响应中断，即在锁的获取过程中可以中断当前线程 boolean tryLock()尝试非阻塞的获取锁，调用该方法立即返回，true表示获取到锁 boolean tryLock(long time,TimeUnit unit)throws InterruptedException超时获取锁，以下情况会返回：时间内获取到了锁，时间内被中断，时间到了没有获取到锁。 void unlock()释放锁 实现类 ReentrantLock排它锁：该锁在同一时刻只允许一个线程来访问 ReentranReadWriteLock读写锁 在同一时刻允许可以又多个线程来访问，但在写线程访问时，所有的独显成和其他写线程被阻塞，读写锁维护了一对锁，一个读锁和一个写锁，通过读写锁分离，使得并发性相比一般的排他锁有了很大的提升。 使用特殊域变量（Volatile）实现线程同步 它的原理是每次要线程要访问volatile习俗hi的变量时都是从内存中读取，而不是缓存中读取，因此每个线程访问到的变量值都是一样的，这样就保证了同步。 volation关键字为域变量的访问提供了一种免锁机制 使用volatile修饰域相当于告诉虚拟机该域可能会被其他线程更新 因此每次使用该域就要重新计算，而不是使用寄存器中的值 volatile不会提供任何院子操作，它也不能用来修饰final类型的类 使用局部变量 如果使用ThreadLocal管理变量，则每一个使用该变量的线程都获得该变量的副本，副本之间相互独立，这样每一个线程都可以随意修改自己的变量副本，而不会对其他线程产生影响。 线程同步 只能同步方法，而不能同步变量和类 每个对象只有一个锁；当提到同步时，应该清楚在什么上同步。也就是说，在哪个对象上同步。 不必同步类中所有的方法，类可以同时拥有同步和非同步方法。 如果两个线程要执行一个类中的synchronized方法，并且两个线程使用相同的实例来调用方法，那么一次只能又一个线程能够执行方法，另一个需要等待，直到锁被释放，也就是说：如果一个线程在对象上获得一个锁，就没有任何其他线程可以进入（该对象的）类中的任何一个同步方法。 如果线程拥有同步和非同步方法，则非同步方法可以被多个线程自由访问而不受锁的限制。 线程睡眠时，它所持的任何锁都不会释放。 线程可以获得多个锁。比如，在一个对象的同步方法里面调用另外一个对象的同步方法，则获得了两个对象的同步锁。 同步损害并发性，应该尽可能缩小同步返回。同步不但可以同步整个方法，还可以同步方法中一部分代码块。 在使用同步代码块时候，应该指定在那个对象上同步，也就是说要获取哪个对象的锁。 lock接口和synchronized的区别 Lock是一个接口，而synchronized是java中的关键字，synchronized是内置的语言实现； synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock（）去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到 Lock可以提高多个线程进行读操作的效率 在性能上来说，如果竞争资源不激烈，亮着的性能是差不多的，而当竞争资源非常激烈时（即又大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。Lock的操作域synchronized相比，灵活性更高，而且Lock提供多种方式获取锁，有Lock、ReadWriteLock接口，以及实现这两个接口的ReentrantLock类、ReentrantReadWriteLock类。 java中的wait和sleep方法有何不同都能将线程状态变成等待状态 sleep()状态，是属于Thread类中的。而wait()方法，则是属于Object类中的。 wait方法必须正在同步环境下使用。比如synchronized方法或者同步代码块。如果你不在同步条件下使用，会抛出IllegalMonitorStateException异常。另外，sleep方法不需要再同步条件下调用，你可以任意正常的使用。 调用wait()的时候方法会释放当前持有的锁，而sleep方法不会释放任何锁 编写java代码，解决生产者——消费者问题有T1、T2、T3三个线程，如何保证T2在T1执行完后执行，T3在T2执行完后窒息感？什么是ThreadLocal类，怎么使用它 线程局部变量 为每一个使用变量的线程提供一个变量值的副本，是java中一种较为特殊的线程绑定机制，是每个线程都可以独立的改变自己的副本，而不会和其他线程的副本发生冲突 线程消失之后，其线程局部实例的所有副本都会被垃圾回收机制回收（除非存在对这些副本的其他引用） 实现思路：在ThreadLocal类中有一个MAP，用于存储每一个线程的变量的副本 使用方法： 在多线程的类（如ThreadDemo类）中，创建一个ThreadLocal对象threadXxx，用来保存线程间需要隔离处理的对象xxx 在ThreadDemo，创建一个获取要隔离访问的数据的方法getXxx(),在方法中判断，若ThreadLocal对象为null时，应该new一个隔离访问类型的对象，并强制转换为要应用的类型。 在ThreadDemo类的run方法中，通过getXxx方法获取要操作的数据，这样可以保证每个线程对应一个数据对象，在任何时刻都操作的是这个对象。 什么是死锁？如何解决死锁问题死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，他们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。由于资源占用是互斥的，当某个进程提出申请资源后，使得有关进程在无歪理血族下，永远分配不到必须的资源而无法继续运行，这就产生了一种特殊现象死锁。 死锁的发送具备的条件 互斥条件 请求和保持条件 不剥夺条件 环路等待条件 如何解决死锁问题 按同一顺序访问对象 如果所有并发事务按同一顺序访问对象，则发生死锁的可能性会降低。例如：如果两个并发事务获得Supplier表上的锁，然后获得Part表上的锁，则在其中一个事务完成之前，另一个事务被阻塞在Supplier表上。第一个事务提交或回滚后，第二个事务继续进行。不发生死锁。将存储过程用于所有的数据修改可以标准化访问对象的顺序。 避免事务中的用户交互避免事务中的用户交互 避免编写包含用户交互的事务，因为运行没有用户交互的批处理的速度要圆圆快于用户手动响应查询的速度，例如答复应用程序请求参数的提示。例如，如果事务正在等待用户输入，而用户去吃无参了或者甚至回家过周末了，则用户将此事务挂起使之不能完成。这样将降低系统的吞吐量，因为事务持有的任何锁只有在事务提交或回滚时才会释放，即使不出现死锁的情况，访问同一资源的其他事务也会被阻塞，等待该事务完成。 保持事务简短并在一个批处理中 在同一数据库中并发执行多个需要长时间运行的事务时通常发生死锁。事务运行时间越长，其持有排它锁或更新锁的时间也就越长，从而堵塞了其他活动并可能导致死锁。包吃事务在一个批处理中，可以最小化事务的网络通信往返量，减少完成事务可能的延迟并释放锁。 使用低隔离级别 确定事务是否能在更低的隔离级别上运行，执行提交读允许事务读取另一个事务已读取（未修改）的数据，而不必等待第一个事务完成。使用较低的隔离级别（例如隔离读）而不使用较高的隔离级别（例如可串行读）可以缩短持有共享锁的时间，从而降低了锁定争夺。 使用绑定连接 使用绑定连接同一应用程序锁打开的两个或多个连接可以相互合作，次级连接锁获得的任何锁可以象由住连接获得的锁那样持有，反之亦然，因此不会相互阻塞。]]></content>
      <categories>
        <category>基本集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[网络编程]]></title>
    <url>%2F2016%2F08%2F02%2Ftcp%26io%2F</url>
    <content type="text"><![CDATA[协议UDPUser Date Protocol（用户数据报协议） UDP是面向非连接的协议，面向非连接就是在正式通信前不必与对方先简历连接，不管对方状态就直接发送。 UDP适用于一次只传送少量数据、对可靠性要求不高的应用环境。比如，我们经常使用“ping”命令来测试两台主机之间TCP/IP通信是否正常，其实“PING”命令的原理就是向对方主机发送ICMP数据包，然后对方主机确认收到数据包，如果数据包是否到达的消息及时反馈回来，那么网络就是通的。这充分说明了UDP协议是面向非连接的协议，没有简历连接的过程。正因为UDP协议没有连接的过程，所以它的通信效率高；但是正因为如此，它的可靠性不如TCP协议高。QQ就使用UDP发消息，因此有时会出现收不到消息的情况。 TCPTransmission Control Protocol（传输控制协议） TCP是面向连接的协议，面向连接就是正式通信前必须要与对方建立起连接。比如你给别人打电话，必须等线路接通了、对方拿起话筒才能互相通话。 一个TCP连接必须要经过三次对话才能建立起来，其中的过程非常复杂。简单的说：A向B发送连接请求的数据包，这是第一次，B向A发送同一连接和要求同步（同步就是两台主机一个在发送，一个在接收，协调工作）的数据包，这是第二次对话；主机A再发出一个数据包确认主机B的要求同步，这是第三次对话，三次对话的目的是指数据包的发送和接收同步，经过三次对话之后，A才向B正式发送数据。 TCP关闭连接必须要经过四次挥手，简单的说： 流NIO IO是基于字节流和字符流实现的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区，或者从缓冲区写入到通道中。 Non-blocking IO(非阻塞式IO) 可以让你非阻塞的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Selectors（选择器） 选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 三个核心组件 IO和NIO之间的区别 IO是面向流的，NIO是面向缓冲的 Java IO面向流意味着每次从流中读一个或多个字节，直至读 取所有字节，他们没有被缓存在任何地方 NIO则能前后移动流中的数据，因为是面向缓冲区的 IO是堵塞型IO，NIO是非堵塞型IO Java IO的各种流是阻塞的。这意味着，当一个线程调用read()或write()时，该线程被阻塞，知道有一些数据背读取，或数据完全写日。该线程在此期间不能再干任何事情了 java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。NIO可让你只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去坐别的事情。 NIO有选择器 Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。]]></content>
      <categories>
        <category>基本集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于我]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[我是谁Misaniy 这个名字是因为她的名字是YUANSIYI，我从中抠了几个字符串，加上我的想念组合的。 上可陪领导逛街拎包吃喝玩乐，下可宅在家里追新番，热衷于研究新菜品和更优雅的代码，也没有忘记工作之余锻炼身体给自己未来投资，有一个非常有眼光的女朋友（这绝不是在夸我），也在努力成为一个优雅的hentai绅士。 热爱生活，热爱科技，爱小米，更爱小米的智能家居体系，有朝一日，我要让家里充满智能，充满GEEK的味道。 这是哪里misaniy.cc这是我的博客小站，也是我的黄金屋。在我的代码之夜上，地上有数不清的咯脚的石子，我摸索着前行，背后总有一个人意味深长地让我带上他们。如果你听过这个故事，你也会像我一样，写这样一个博客，把一路的石子捡起来。 我会不时更新我的博客，在我代码上遇到问题，或者我觉得需要记录的时候 怎么联系我Github:MisaniyEmail:MisaniyWeibo:为这美好的世界献上祝福Location：中国.重庆]]></content>
  </entry>
  <entry>
    <title><![CDATA[生活图]]></title>
    <url>%2Fgallery%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
