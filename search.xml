<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[浅谈数据库优化]]></title>
    <url>%2F2017%2F11%2F15%2Fsql%2F</url>
    <content type="text"><![CDATA[数据库优化分为硬优化、软优化。硬优化主要是指针对数据库本身的优化（表和库），软优化主要就是针对sql语句之类的优化。 软优化查询条件尽量加索引- 这个不是必须加的，但很大成都上能够解决一定查询效率的问题。但是需要主义的是不要在索引字段上做计算、函数等操作。 少部分关键字的使用需要减少- 避免使用Like - exit not exit来代替not in - 字段名代替* - id&gt;=3代替id&gt;2 减少子查询的使用- 复杂业务简单化，不使用子查询 - 先生成临时表，再做关联查询 其他- 减少对数据库的重复操作，能合并就合并，能一条sql就不要分多条 - 其他有的再补充 硬优化拆表 在很多情况下，某一张表某些字段经常被查询到，那么我们就有必要拆分表 正如这张图，在id\name\attr7\attr8\attr9常用的情况下，把表拆分成两张表再关联起来 分表 单表数据太多，查询的效率极慢的情况下我们可以把表里的数据分成几张表来存储 就如上图，我们每次查询一月份的数据的时候都要在全年度的表里去查，数据量太大，查询了会很慢，所以我们将每个月份的数据单独提出来做成表，这样查询就相对比较快了 读写分离 读写分离，主要针对数据库访问的量很大，导致数据库运行可能宕机的问题。原理就是添加几个数据库，形成集群将访问量平均分配到每个数据库上 如上图，我们只需要修改主库数据，通知到每个分类，从库更新之后，每次查询我们都将查询分配到每个从库就可以了]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mybatis的分页插件PageHelper]]></title>
    <url>%2F2017%2F11%2F10%2FPageHelper%2F</url>
    <content type="text"><![CDATA[逆向工程生成的代码是不支持分页处理的，如果想进行分页需要自己编写mapper，这样就失去逆向工程的意义了。为了提高开发效率可以是要弄个mybatis的分页插件PageHelper. 简介该插件目前支持Oracle，MySQL，MariaDB,SQLite,Hsqldb,PostgreSQL六种数据库分页. 使用方法 把PageHelper依赖的jar包添加到工程中。官方提供的代码对逆向工程支持的不好，使用民间修改版pagehelper-fix. 在Mybatis配置xml中配置拦截器插件 12345&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;&gt; &lt;property name=&quot;dialect&quot; value=&quot;mysql&quot;/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 代码 12345678910//获取第1页，10条内容，默认查询总数countPageHelper.startPage(1,10);//分页处理List&lt;E&gt; list = ;//取分页信息PageInfo&lt;E&gt; pageInfo = new PageInfo(list);pageInfo.getTotal();pageInfo.getPages();pageInfo.getPageNum();pageInfo.getPageSize();]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集合小结]]></title>
    <url>%2F2017%2F10%2F17%2Fcollection%2F</url>
    <content type="text"><![CDATA[计算机科学中，集合是一组可变数量的数据项(也可能是0个)的组合，这些数据项可能共享某些特征，需要以某种操作方式一起进行操作。 单例集合List特点：元素有放入顺序，元素可重复 ArrayList 长度可变的数组 有索引结构，使用索引在数组中搜索和读取数组是很快的，也就是查找快 数组这种数据结构将对象放在连续的位置中，因此增加、删除的速度比较慢 存数类型是Object，允许为null 初始大小（不设置大小的时候默认是10） 不是线程安全的，异步的 LinkedList 链表的数据结构 存储的类型是Object,允许为null 不是线程安全的，得自己实现线程安全 在创建List时构造一个同步的List List list = Collections.synchronizedList(new LinkedList(...)); Vector 底层是数组结构 同步的，线程安全 执行效率低 Vector缺省情况下自动增长原来一倍的数组长度 在集合中保存大量的数据那么使用Vector有一些有事，因为你可以通过设置集合的变化大小来避免不必要的资源开销 初始大小是10 Set特点：元素无放入顺序，且不可重复（实际存储顺序是HashCode决定的，固定的） HashSet 基于hashmap实现，底层使用hashmap保存元素 set元素中的值存放在hashmap的key中，value存放了一个模拟值persent 由于hashmap中的key是不能重复的，所以hashset就利用这个特性实现了set中的值不会重复 hashset是如何保证元素的唯一性 基于哈希表实现的。哈希表是存储一系列哈希值的表，而哈希值是由对象的hashcode()方法产生的。 确保元素唯一性的两个方法：hashcode()和equals()方法 当调用add（）方法向集合中存入对象的时候，先比较对象的哈细致有没有一样的，如果都不一样就直接存入。如果有与之相同的哈希值，则要继续比较两个对象是否是同一个对象，调用对象的equals方法TreeSet 基于treeMap实现的 有序的二叉树多例集合 HashTable 父类是Dictionary 实现一个key-value映射的哈希表。任何非空对象都可作为key或者value 同步的，线程安全 内部通过单链表解决冲突问题 效率低 有contains方法 初始大小是11 扩容时，hashTable采用的是2*old+1 hash值的计算使用的直接是对象的hashcode方法 迭代器，HashTable用Enumeration来遍历数据 只提供了遍历Vector和Hashtable类型集合元素的功能 这种类型的集合对象通过调用elements()方法获取一个Enumeration对象，然后Enumeration对象再调用一下方法来对集合中元素进行遍历。 hasMoreElements():判断Enumeration对象中是否还有数据nextElement():获取Enumeration对象中的下一个数据 HashMap 弗雷是AbstractMap 哈希表，允许key和value值为空 方法是同步的，线程不安全 为什么不安全 首先如果多个线程同时使用put方法添加元素，而且假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖 如果多个线程同时检测到元素个数超过数组大小*loadFactor，这样就会发生多个线程同时对Node数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋值给table，也就算说其他线程的都会丢失，并且各自线程put的数据也丢失 解决方案 HashtableConcurrentHashMapSynchronized Map 效率高 有containsvalue()和containsKey()方法 初始大小是16 扩容时，hashMap采用的是2*old hash值的计算没有直接使用对象的hashcode方法，为对象key的hashcode值与对象value的hashcode值按位与或操作 迭代器 hashMap的工作原理 当两个对象的hashcode值相同时会发生什么？(hashmap中的碰撞探测(collision detection)及解决方案) 因为hashcode值相同，因此他们的bucket位置相同，碰撞就会发生 因为hashMap使用链表存储对象，这个Entry(包含有监支队的Map.Entry对象)会存储在链表中 如果有两个键的hashcode相同，你如何取对象？ 当我们调用get()方法，hashMap会使用建对象的hashcode找到bucket位置，然后获取值对象 找到bucket位置之后，会调用keys,equals()方法去找到链表中正确的节点，最终找到要找的值对象 如果HashMap的大小超过了负载银子(load factor)定义的容量，怎么办？ 默认的负载银子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其他集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中，这个过程叫做rehashing，因为它调用hash方法找到新的bucket位置 重新调整HashMap大小存在什么问题吗？ 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，他们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将呀un苏放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件发生了，那么就死循环了。 为什么String、Integer这样的wrapper类适合作为键 String、Integer这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的。也是final的，而且已经重写了equals()和hashcode()方法了。其他的wrapper类也有这个特点。不可变形是必须，因为为了要计算hashCode(),就要防止键值改变，如果键值在放入时和获取是i返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性是必要的，因为为了要计算hashCode()，就要防止兼职改变。如果键值在放入和获取时返回不同的hashCode就不能从HashMap中找到想要的对象。不可变性还有其他优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法非常重要。如果两个不想等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 ConcurrentHashMap CHM不但是线程安全的，而且比HashTable和synchronizedMap的性能要好 相对于HashTable和synchronizedMap锁住了整个Map,CHM只锁住部分Map. CHM允许并发的读操作，同时通过同步锁在写操作时保证数据完整性 如何实现 CHM引入了分割，并提供了HashTable支持的所有的功能 在CHM中，支持多线程对Map做读操作，并且不需要任何的blocking CHM将MAP分割成不同的部分，在执行更新操作的时候只锁住了一部分 根据默认的并发级别，Map被分割成16个部分，并且由不同的锁控制。这样的话可以同时最多有16个写线程进行操作 但由于一些更新操作，如put()，remove(),putAll(),clear()只锁住操作的部分，所以检索操作不能保证返回的是最新的结果 在迭代遍历CHM时，keySet返回的是iterator是弱一致性和fail-safe的，可能不会返回某些最近的改变，并且在遍历过程中，如果已经遍历的数组上的内容变化了，不会抛出ConcurrentModificationException的异常 CHM默认的并发级别是16，但可以在创建CHM时通过构造函数改变 CHM不允许null的键值 应用场景 CHM适用于读数量超过写 当写数量大于读时，CHM的性能低于Hashtable的synchronized Map的。这是因为当锁住了整个Map时，读操作要等待对统一部分执行写操作的线程结束 CHM适用于cache,在程序启动时初始化，之后可以被多个请求线程访问。]]></content>
      <categories>
        <category>基本集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java7的新特性Paths,Files]]></title>
    <url>%2F2017%2F09%2F25%2FJAVA7PathsFiles%2F</url>
    <content type="text"><![CDATA[在使用lucene 5的时候，发现在lucene4时file的地方用到了Path,发现这是JAVA7的新特性，于是查找相关文档，发现在IO方面,java7新增了Paths,Files工具类，发现异常强大，小结一下。 PathsPath是用来表示文件路径和文件，可以有多种方法来构造一个Path对象来表示一个文件路径或一个文件 在Paths类里有两个static方法 12345public static Path get(String first,String...more) &#123; return FileSystems.getDefault().getPath(first,more);&#125;public static Path get(Url url) 得到三种构造方式(以源文件d:/demo.txt为例) 1234Path path1 = Paths.get(&quot;d:/&quot;,&quot;demo.txt&quot;);Path path2 = Paths.get(&quot;d:/demo.txt&quot;);Path path3 = Paths.get(URI.create(&quot;file:///d:/demo.txt&quot;));Path path4 = FileSystems.getDefault().getPath(&quot;d:/&quot;,&quot;demo.txt&quot;); File和Path、File和URI之间的转换 12345File file = new File(&quot;d:/demo.txt&quot;);Path path = file.toPath();File file2 = path.toFile();URI uri = file.toURI(); 读取文件属性 1234567Path path = Paths.get(url);path.getFileName();path.getParent();//根目录path.getRoot();//目录级数(D:\xxx\xxx\xxx\demo.txt 4)path.getNameCount(); 创建一个文件 123Path path = Paths.get(&quot;C:\demo.txt&quot;);if(Files.exists(path)) Files.createFile(path); Files.newBufferWriter写入文件 1234BufferedWriter writer = Files.newBufferedWriter(Paths.get(&quot;D:\\demo.txt&quot;),Charset.forName(&quot;UTF-8&quot;)); writer.write(&quot;测试中文&quot;); writer.flush(); writer.close(); Files.newBufferWriter读取文件 123456BufferedReader reader = Files.newBufferedReader(Paths.get(&quot;D:\\demo.txt&quot;), Charset.forName(&quot;UTF-8&quot;)); String str = null; while((str = reader.readLine())!=null)&#123; System.out.println(str); &#125; reader.close(); 遍历文件夹,这里只遍历当前目录，不遍历子目录 12345678910111213Path path = Paths.get(&quot;D:\\dir&quot;); DirectoryStream&lt;Path&gt; paths = Files.newDirectoryStream(path); for(Path p : paths)&#123; System.out.println(p.getFileName()); &#125; DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(Paths.get(&quot;D:\\dir&quot;)); Iterator&lt;Path&gt; ite = stream.iterator(); while (ite.hasNext())&#123; Path path = ite.next(); System.out.println(path.getFileName()); &#125; 要遍历子目录，在java7前需要用递归，而java7的Files提供了walkFileTree()方法，这个在另一篇文章写到 Files 创建目录和文件 123Files.createDirectories(Paths.get(&quot;D://dir&quot;));if(!Files.exists(Paths.get(&quot;D://dir&quot;))) Files.createFile(Paths.get(&quot;D://dir/demo.txt&quot;)) 文件复制 123456789101112//Files.copy(Source,Target,CopyOptions) //StandardCopyOption //REPLACE_EXISTING 如果存在替换 //COPY_ATTRIBUTES 复制 //ATOMIC_MOVE Move the file as an atomic file system operation.//Files.copy(Source,OutputStream)//Files.copy(InputStream,Target,CopOption)Files.copy(Paths.get(&quot;C://Source.txt&quot;,Paths.get(&quot;D://Target.txt&quot;,StandardCopyOption.COPY_ATTRIBUTES))); 读取文件属性 123456789Path path = Paths.get(url);//最后一次修改时间 System.out.println(Files.getLastModifiedTime(path)); System.out.println(Files.size(path));//是否为一个连接 System.out.println(Files.isSymbolicLink(path)); System.out.println(Files.isDirectory(path));//指定属性，*表全部 System.out.println(Files.readAttributes(path,&quot;*&quot;));]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux误删除了备份的数据库]]></title>
    <url>%2F2017%2F09%2F13%2FOracleBankDelete%2F</url>
    <content type="text"><![CDATA[延迟加载异常：failed to lazily initialize a collection of role: com.misaniy.bos.domain.base.Courier.fixedAreas, could not initialize proxy - no Session reason：误删除了备份的数据库resolve：12345678910111213141516sqlplus /nolog//使用数据库命令模式connect system/root as sysdba//连接数据库SQL&gt;shutdown normal//关闭数据库oracle服务SQL&gt;startup mount//重新启动Oracle服务SQL&gt;alter database open;//打开数据库//SQL&gt;alter database datafile 5 offline drop 若出现错误SQL&gt;alter database open;//重新更改数据库的openSQL&gt;startup]]></content>
      <categories>
        <category>报错集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[could not initialize proxy - noSession]]></title>
    <url>%2F2017%2F09%2F13%2Flazily%2F</url>
    <content type="text"><![CDATA[failed to lazily initialize a collection of role: com.misaniy.xxx, could not initialize proxy - no Session延迟加载异常 解决方案有两种其一，在web.xml中配置Spring的OpenSessionInViewFilter，确保服务器端的逻辑执行完后再关闭session，这是针对hibernate的支持类12345678910111213&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.jpa.support.OpenEntityManagerInviewFilter&lt;/filter-class&gt; &lt;!-- 如果你的sessionFactory不是叫sessionFactory，需要配置如下--&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt; &lt;param-value&gt;&#123;Your Session Factory Name&#125;&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 其二，上面方法是hibernate的支持类，如果你配置的不是sessionFactory,比如我用的SPRING DATA JPA，就用如下方法1234@JSON(serialize = false)public XXX getXXX()&#123; return XXX;&#125; 在Bean类找到你的延迟加载的数据，没有使用到就使用该注解]]></content>
      <categories>
        <category>报错集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Oracle和tomcat端口冲突]]></title>
    <url>%2F2017%2F09%2F12%2FPortException%2F</url>
    <content type="text"><![CDATA[Oracle XE http与tomcat端口冲突8080 reason：oracle与tomcat端口8080冲突，我们可以修改任意一个端口； resolve：修改oracle123sqlplus system/rootSQL&gt;call dbms_xdb.sethttpport(&apos;8082&apos;); 修改tomcat，这里由于用了maven,所以直接安装tomcat7插件Maven —&gt; build plugin —-&gt;tomcat712345678&lt;plugin&gt;&lt;gourpId&gt;org.apache.tomact.maven&lt;/groupId&gt;&lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;&lt;version&gt;2.2&lt;/version&gt;&lt;configuration&gt; &lt;port&gt;8081&lt;/port&gt; &lt;path&gt;/&lt;/path&gt;&lt;/configuration&gt; Run As —&gt; Maven InstallRun As —&gt; Maven Build… tomcat7:run]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[全文检索和Lucene]]></title>
    <url>%2F2017%2F04%2F27%2FFull-Context%2F</url>
    <content type="text"><![CDATA[SQL语句的like会搜索大量不相关的内容，不走索引，且，存在资源浪费。这时我们就需要用到全文检索。 全文检索和Lucene全文检索了解Lucene之前，我们需要清楚全文检索的概念。 生活中的数据分为三种。 结构化数据：具有固定格式或有限长度的数据，如数据库，元数据等。 非结构化数据：不定长度或无固定格式的数据，如邮件，word文档等。 半结构化数据：根据需要可以按结构化数据处理，也可抽取出纯文本按非结构化数据来处理。 对应的搜索分为两种。 对结构化数据的搜索：sql语句、windows搜索文件名、类型、修改时间等 对非结构化数据的搜索：windwos搜索文件内容、linux的grep,搜索引擎的搜索等 对非结构化数据搜索即对全文数据的搜索分为两种： 顺序扫描法：假设寻找某个字符串的文件，就是一个文档一个文档读，然后每个文档从头读到尾，Linux下的grep就是这种方式，小数据量可以使用，但对于大量数据，就很慢了。 全文检索：将非结构化数据中的一部分信息提取出来，重新组织，使其变为结构化数据，我们称之为索引，而这种先建立索引，再搜索的过程就叫全文检索。 对应的创建索引方式分三种。 索引：加快数据搜索的一种数据结构 I/O流：对于本地文件创建索引。 爬虫：模拟访问URL，获取网页数据，搜索引擎使用。 SQL搜索：对于存放在数据库的数据使用。 如何创建索引 源文档Document 文档中包括一个一个域(Field)[file_name,file_path,file_size,file_content等] 分词组件Tokenizer得到词元Token 将源文档分词 去除标点 去除停词 处理组件LinguisticProcessor得到词Term 变为小写 缩减位词根 转变为词根 索引组件Indexer 用词Term创建字典 对字典按字母顺序排序 合并相同的词Term成为倒排索引(Posting List) 倒排索引：从字符串到文件的映射是文件到字符串映射的反向过程，所以这种索引称为倒排索引 如何对索引进行搜索 用户输入查询语句 对查询语句进行分析处理 搜索索引，得到符合语法树的文档 根据得到的文档和查询语句的相关性排序 LuceneLucene实现全文检索 获得原始文档 创建文档对象 分析文档 创建索引 查询索引库 常用域Field 创建索引库12 索引库的维护索引库的添加123456789//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//Document.add(Field)Document document = new Document();document.add(new TextField(&quot;name&quot;,&quot;文档&quot;.Store.YES));document.add(new TextFiled(&quot;content&quot;,&quot;内容&quot;,Store.YES));IndexWriter.addDocument(document);IndexWriter.close(); 索引库的删除 删除全部 123456//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//删除全部索引indexWriter.deleteAll();indexWriter.close(); 指定条件删除 12345678//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//QueryQuery query = new TermQuery(new Term(&quot;name&quot;,&quot;文档&quot;));//指定条件删除indexWriter.deleteDocuments(query);indexWriter.close(); 索引库的修改123456789//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//修改后的DocumentDocument document = new Document();document.add(new TextField(&quot;name&quot;,&quot;新文档&quot;,Store.YES));//updateDocumentindexWriter.updateDocument(new Term(&quot;content&quot;,&quot;文档&quot;),document);indexWriter.close(); 查询索引Query的子类查询TermQuery 精确查找123456789101112131415//IndexSearcherIndexSearcher indexSearcher = new IndexSearcher(Directory.open(FSDirectory.open(new File(Path))));Query query = new TermQuery(new Term(FieldName,keyStr));TopDocs topDocs = indexSearcher.searcher(query,100);//topDocs.scoreDocs存储了document的idfor(ScoreDoc scoreDoc : topDocs.scoreDocs)&#123; //scoreDoc.doc就是document的id Document document = indexSearcher.doc(scoreDoc.doc); document.get(keyStr);&#125;indexSearcher.getindexReader.close(); MatchAllDocsQuery 所有文档NumericRangeQuery 数值范围查找BooleanQuery 组合条件查找IndexSearcher搜索方法queryparser 查询QueryParserMulitFieldQueryParser TopDocs]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用Redis实现缓存]]></title>
    <url>%2F2017%2F04%2F13%2FRedis%2F</url>
    <content type="text"><![CDATA[实现缓存的工具有很多，现在比较流行的是Redis。Redis(remote dictionary server)缓存存放在内存，数据库存放在磁盘，访问内存更快。频繁访问而不频繁修改的数据放在缓存里，能带来更好的体验和更小的服务器压力。 Redis安装Redis即便是高并发也是单线程的，不适合保存内容大的数据。 Linux下安装由于Redis是C语言开发的，安装Redis前需要安装c语言的编译环境。如果没有gcc需要在线安装。Yum install gcc-c++解压后编译make安装make install PREFIX=/usr/local/redis 连接Redis 启动Redis服务 前端启动 bin]./redis-server 后台启动 把redis3.0/redis.conf复制到redis/bin 目录下 修改配置文件daemonize yes 启动服务bin]./redis-server redis.conf 启动客户端 bin]./redis-cli默认连接localhost:6379 bin]./redis-cli -h 192.168.25.130 -p 6379 -h:Path -p:Port Redis五中数据类型 String:key-value(做缓存) get/incr/decr key set key value Hash:key-fields-values(做缓存) hget key field hset key field value hincrby key field int List:有顺序可重复 （类似堆） rpush key values往右添加 lpush key values 往左添加（倒序） lrange 0 -1 查询所有 lpop 从左取 rpop 从右取 Set:无顺序，不能重复 sadd key values smembers key srem key 删除 SortedSet(zset):有顺序，不能重复 zadd key n valueszadd zset1 2 a 5 b 1 c 6 d zrange key 0 -1 c a b d zrevrange key 0 -1 倒序查看 Key命令 expire key time 设置失效时间 ttl key 查看有效期 persist key 解除失效（持久化） Redis的持久化方案Redis所有数据都是保存在内存中的。 Rdb:快照形式。定期把内存中当前时刻的数据保存到磁盘。Redis默认支持的持久化方案。一直开启 Aof:Append Only File。把所有对Redis数据库操作的命令，增删改命令保存到一个文件中。数据库恢复时，把所有命令执行一遍就可恢复。开启后会降低一点性能 在Redis.conf配置文件中配置。 默认RBD123save 900 1save 300 10save 60 10000 - 备份文件 dump.rdb - AOF.每秒执行一次 `appendonly yes` Redis集群搭建架构细节 所有Redis节点彼此互联(PING-PONG机制)，内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连，不需要中间proxy层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 redis-cluster把所有物理节点映射到[0-16383]slot上，cluster负责维护node&lt;-&gt;slot&lt;-&gt;value Redis集群中内置了16384个哈希槽，当需要在Redis集群中防止一个key-value时，redis先对key使用crc16算法算出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，redis会根据节点数量大致均等的将哈希槽映射到不同的节点 集群搭建Redis集群中至少应该有三个节点，为了保证集群的高可用，需要每个节点有一个备份机。所以至少需要6台服务器。 搭建伪集群 复制单机版redis复制5份redis-cluster 修改每一个redis节点的端口port以及启用集群cluster-enabled yes 启动每一个节点./redis-server redis.conf 创建集群 安装Ruby的运行环境yum install ruby 安装Ruby的包管理器yum install rubygems 安装脚本gem install redis-3.0.0.gem 复制redis-3.0.0/src/redis-trib.rb到redis 使用ruby脚本搭建集群./redis-trib.rb create --replicase 1 192.168.XX.XX:7001 1992.168.XX.XX:7002 ...... 创建关闭集群的脚本 redis-cluster]vim shutdown-all.sh 123redis01/redis-cli -h address -p 7001 shutdownredis01/redis-cli -h address -p 7002 shutdownredis01/redis-cli -h address -p 7003 shutdown chmod u+x shutdown-all.sh这里这个redis-cli可以是任意一个redis集群的客户端 集群的使用方法Redis-cli 连接集群redis-cluster]redis01/redis-cli -p 7002 -c-c:代表连接的redis集群 Jedis连接单机版1234Jedis jedis = new Jedis(ipaddress,port);jedis.set(key,value);String result = jedis.get(key);System.out.print(result); 连接单机版使用连接池1234567JedisPool jedisPool = new JedisPool(ipaddress,port);Jedis jedis = jedisPool.getResource();jedis.set(key,value);String result = jedis.get(key);System.out.print(result);jedis.close();jedisPool.close(); 连接集群版1234567891011121314//第一步：使用JedisCluster对象，需要一个Set&lt;HostAndPort&gt;参数，Redis节点的列表Set&lt;HostAndPort&gt; nodes = new HashSet&lt;&gt;();nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));JedisCluster jedisCluster = new JedisCluster(nodes);//第二部：直接使用JedisCluster对象操作redis,在系统中举例存在。jedisCluster.set(key,value);String result = jedisCluster.get(key);//关闭JedisCluster对象。jedisCluster.close();]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Solr安装与介绍]]></title>
    <url>%2F2017%2F04%2F02%2Fsolr%2F</url>
    <content type="text"><![CDATA[Solr是基于Lucene的全文搜索服务器 Solr的安装和配置Solr配置到tomcat 把Solr的war包赋值到tomcat的webapp下并解压 把Solr/example/lib/ext目录下的jar包添加到solr工程中. 配置SolrHome和SolrCore Solr/example/solr文件就是一个标准SolrHome，复制出来命名solrHome solrHome中的collection1就是一个SolrCore solrCore下有一个目录conf，conf下的solrconfig.xml可以配置相关信息。 solrconfig.xml env-entry-value:配置solrhome的绝对路径 Lib:solr服务依赖的拓展包 dataDir:配置索引库存放路径 requestHandler:查询时使用的url 打开Schema.xml可以看到Solr默认的FieldType class:Solr提供的包solr.TextField，solr.TextField允许用户通过分析器来定制索引和查询，分析器包括一个分词器(tokenizer)和多个过滤器(filter) positionIncrementGap:可选属性，定义在同一个文档中此类型数据的空白间隔，避免短语匹配错误，此值相当于Lucene的短语查询设置slop值，根据经验设置位100 analyzer 搜索分析器 solr.StandardTokenizerFactory 标准分词器 solr.StopFilterFactory 停用词过滤器 solr.LowerCaseFilterFactory 小写过滤器 索引分析器 solr.StandardTokenizerFactory 标准分词器 solr.StopFilterFactory 停用词过滤器 solr.SynonymFilterFactory 同义词过滤器 Field 定义 name 域名 type FieldType indexed 是否索引 stored 是否存储 multiValued 是否存储多个值 uniqueKey 默认定义唯一主键key为id域 &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; copyField 复制域 将多个Field复制到一个Field中，进行统一检索 比如，输入关键字搜索name/description 1234567&lt;!-- 定义name/description/keywords的Field --&gt;&lt;field name=&quot;keywords&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot;/&gt;&lt;field name=&quot;name&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;description&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;false&quot; /&gt;&lt;!-- 只搜索keywords就相当于搜索了name和description&lt;copyField source=&quot;name&quot; dest=&quot;keywords&quot;&gt;&lt;copyField source=&quot;description&quot; dest=&quot;keywords&quot;&gt; dynamicField(动态字段)自定义Field为:product_title_t&lt;dynamicField name=&quot;*_t&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; 配置中文分析器 将IKAnalyzer2012FF_ul.jar添加到solr/WEB-INF/lib 赋值IKAnalyzer的配置文件和自定义词典到solr的classpath下 在schema.xml中添加一个自定义的filedType 123&lt;fieldType name=&quot;text_ik&quot; class=&quot;solr.TextFiled&quot;&gt; &lt;analyzer class=&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;/&gt;&lt;/fieldType&gt; 定义field，指定field的type属性为text_ik Solr管理索引库维护索引 添加/更新文档 使用Dataimport批量导入数据 导入solr-dataimporthandler.jar、solr-dataimporthandler-extras.jar、mysql数据库jar包 配置solrconfig.xml，添加一个requestHandler 12345&lt;requestHandler name=&quot;/dateimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 创建一个data-config.xml，保存到solrcore\conf\目录下 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;dataConfig&gt;&lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver url=&quot;jdbc:mysql://localhost:3306/database&quot; user=&quot;root&quot; password=&quot;root&quot;/&gt;&lt;document&gt; &lt;entity name=&quot;product&quot; query=&quot;SELECT pid,name,catalog_name,price,description,picture FROM products&quot;&gt; &lt;field column=&quot;pid&quot; name=&quot;id&quot;/&gt; &lt;field column=&quot;name&quot; name=&quot;product_name&quot;/&gt; &lt;field column=&quot;catalog_name&quot; name=&quot;product_catalog_name&quot;/&gt; &lt;field column=&quot;description&quot; name=&quot;product_price&quot;/&gt; &lt;field column=&quot;picture&quot; name=&quot;product_picture&quot;/&gt; &lt;/entity&gt;&lt;/document&gt;&lt;/dataConfig&gt; 重启tomcat Excute导入数据 删除文档- 删除指定ID的索引 123&lt;delete&gt; &lt;id&gt;1&lt;/id&gt;&lt;/delete&gt; - 删除查询到的索引数据&lt;*:*表示全部&gt; 123&lt;delete&gt; &lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt; 查询索引 q- 查询字符串 域名:条件 fq- filter query 过滤查询 sort- 排序 start,rows- 分页查询 fl- 返回指定字段内容，用逗号或空格分割 df- default Feild 默认域 wt- writer type 指定输出格式 hl- 是否高亮 使用SolrJ管理索引库使用客户端操作Solr比较繁琐低效，于是有SolrJ通过JAVA来访问Solr客户端 添加/更新文档 solrJ、slf4j-log4j12、jul-to-slf4j、jcl-over-slf4j1234567891011SolrServer solerServer = new HttpSolrServer(&quot;https://localhost:8080/solr&quot;);SolrInputDocument document = new SolrInputDocument();//这里FieldName在schema.xml已定义//当id已存在，相当于更新document.addField(FieldName,Value);solrServer.add(document);solrServer.commit(); 删除文档12345678SolrServer solrServer = new HttpSolrServer(&quot;http://localhost:8080/solr&quot;);//根据条件删除solrServer.deleteByQuery(&quot;*:*&quot;);solrServer.deleteById(id);solrServer.commit(); 查询文档查询所有12345678910111213141516SolrServer solrServer = new HttpSolrServer(&quot;https://localhost:8080/solr&quot;);SolrQuery query = new SolrQuery();query.setQuery(&quot;*:*&quot;);QueryResponse queryResponse = solrQuery.query(query);SolrDocumentList solrDocumentList = queryResponse.getResults();//查询到的数量solrDocumentList.getNumFound();//遍历查询结果for(SolrDocument solrDocument : solrDocumentList) &#123; solrDocument.get(fieldName);&#125; 复杂查询123456789101112131415161718192021222324252627282930313233343536373839404142SolrServer solrServer = new HttpSolrServer(&quot;http://localhost:8080/solr&quot;);SolrQuery query = new SolrQuery();//查询条件query.setQuery(KeyStr);//过滤条件query.setFilterQueries();//排序条件query.setSort(FieldNmae,ORDER.asc);//分页处理query.setStart(num);query.setRows(num);//结果中域的列表query.setFields(FiledNames);//默认搜索域query.set(FieldName);//高亮显示query.setHighlightField(true);//高亮显示前缀query.setHighlightSimplePre();//高亮显示后缀query.setHighlightSimplePost();//执行查询QueryResponse queryResponse = solrServer.query(query);//取查询结果SolrDocumentList solrDocumentList = queryResponse.getResults();//遍历查询结果for(SolrDocument solrDocument:solrDocumentList)&#123; //取高亮显示 String productName=&quot;&quot;; Map&lt;String,Map&lt;String,List&lt;String&gt;&gt;&gt; highlighting = queryResponse.getHighlighting(); List&lt;String&gt; list = highlighting.get(solrDocument.get(&quot;id&quot;)).get(product_name); //判断是否有高亮内容 if(list!=null)&#123; productName=list.get(0); &#125;else&#123; productName=(String)solrDocument.get(&quot;product_name&quot;); &#125;&#125;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式文件系统FastDFS]]></title>
    <url>%2F2017%2F01%2F11%2FFastDFS%2F</url>
    <content type="text"><![CDATA[传统图片上传不能有效应对集群的方式，单独搭建一个图片服务器。可以使用分布式文件系统FastDFS方式。实现服务器的高可用 什么是FastDFSFastDFS是用C语言编写的一款开源的分布式文件系统。FastDFS位互联网量身定制，充分考虑了冗杂备份，负载均衡，线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务集群提供文件上传、下载等服务 FastDFS架构 FastDFS架构包括Tracker server和Storage server。客户端请求Tracker server进行文件上传、下载，通过tracker server调度最终由Storage server完成文件上传和下载。 Tracker server作用是负载均衡和调度，通过Tracker server在文件上传时可以根据一些策略找到Storage server提供文件上传服务。可以将Tracker称为追踪服务器或调度服务器。 Storage server作用是文件存储，客户端上传的文件最终存储在Storage服务器上，Storage server没有实现自己的文件系统而是利用操作西戎的文件系统来管理文件。可以将storage称为存储服务器. 服务端两个角色： Tracker：管理集群，tracker也可以实现集群。每个tracker节点地位平等。 Storage:实际保存文件。Storage氛围多个组，每个组之间保存的文件是不同的。每个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有主从概念。 文件上传的流程图客户端上传文件后存储服务器将文件ID返回给客户端，此文件ID用于以后访问该文件的索引信息。文件索引信息包括：组名，虚拟磁盘路径，数据两级目录，文件名。 组名：文件上传后所在的storage组名称，在文件上传成功后有storage服务器返回，需要客户端自行保存。 虚拟磁盘路径:storage配置的虚拟路径，与磁盘选项store_path*对应。如果配置了store_path0则是M00，如果配置了store_path1则是M01,以此类推。 数据两级目录：storage服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件。 文件名：与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含：源存储服务器IP地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。 文件下载 FastDFS使用上传文件123456789101112//加载配置文件，配置文件内容就是tracker服务的地址ClientGlobal.init(AbsoluteAddress);//创建一个TrackerClient对象TrakcerClient trackerClient = new TrackerClient();//使用TrackerClient对象创建连接，获得一个TrackerServer对象TrackerServer trackerServer = trackerClient.getConnection();//创建一个StorageServer的引用StorageServer storageServer = null;//创建一个StorageClient对象，提供两个Server对象StorageClient storageClient = new StorageClient(trackerServer,storageServer);//使用StorageClient对象上传图片String[] result = storageClient.upload_file(&quot;D:/a.jpg&quot;,&quot;jpg&quot;,null); 使用工具类上传1234//加载配置文件FastDFSClient fastDFSClient = new FastDFSClient(AbsoluteAddress);//上传文件String file = fastDFSClient.uploadFile(AbsoluteAddress); 整合SpringSpringMvc.xml设定文件上传解析器1234567&lt;!-- 定义文件上传解析器 --&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- 设定默认编码 --&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;!-- 设定文件上传的最大值,这里设5MB --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;5242888&quot;/&gt; &lt;/bean&gt;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dubbo]]></title>
    <url>%2F2016%2F11%2F09%2FDubbo%2F</url>
    <content type="text"><![CDATA[Dubbo是阿里的开源分布式服务架构，可通过高性能的RPC实现服务的输出和输入功能。 系统通信如何实现远程通信 WebService：效率不高基于soap协议。 使用Restful形式的服务：http+json。很多项目中应用。如果服务太多，服务之间调用关系混乱，需要治疗服务。 dubbo。使用rpc协议进行远程调用，直接使用socket通信。传输效率高，并且可以统计出系统之间的调用关系、调用次数。 规模架构 单一应用架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 此时，用于简化增删改差工作量的数据访问框架(ORM)是关键。 垂直应用架构 当访问量逐渐增大，单一应用增加极其带来的加速度越来越小，将应用拆成无不想干的几个应用，以提升效率 此时，用于加速前端页面开发的Web框架(MVC)是关键 分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 此时，用于提高业务服用及整合的分布式服务架构(RPC)是关键。 流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。 此时，用于提高极其利用率的**资源调度和治理中心(SOA)是关键。 Dubbo 节点角色说明： Provider:暴露服务的服务提供方 Consumer:调用远程服务的服务消费方 Registry:服务注册与发现的注册中心 Monitor:统计服务的调用次数调和调用时间的监控中心 Container:服务运行容器 调用关系说明： 0.服务容器(Container)负责启动、加载、运行服务提供方(Provider) 1.服务提供方(Provider)在启动时，向注册中心(Registry)注册自己提供的服务。 2.服务消费方(Consumer)在启动时，向注册中心(Registry)订阅自己所需的服务。 3.注册中心(Registry)返回服务提供方(Provider)地址列表给消费者，如果有变化，注册中心将基于长连接推送变更数据给消费方(Consumer). ４。服务消费方(Consumer)，从提供者地址列表中，基于软负载均衡算法，选一台提供方(Provider)进行调用，如果调用失败，再选另一台调用. 5.服务消费者(Consumer)和提供方(Provider)，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心(Monitor)使用方法 Spring配置Dubbo采用全Spring配置方式，透明化接入应用，对应用没有任何API侵入，只需用Spring加载Dubbo的配置即可，Dubbo基于Spring的Schema拓展进行加载。 单一工程中spring的配置 1234&lt;bean id=&quot;xxxService&quot; class=&quot;com.xxxServiceImpl&quot; /&gt;&lt;bean id=&quot;xxxAction&quot; class=&quot;com.xxx.xxxAction&quot;&gt; &lt;property name=&quot;xxxService&quot; ref=&quot;xxxService&quot;&gt;&lt;/bean&gt; 远程服务 将上述local.xml配置拆分成两份，将服务定义部分放在服务提供方remote-provier.xml,将服务引用部分放在服务消费方remote-consumer.xml并在provider增加暴露服务配置&lt;dubbo:service&gt;,在consumer增加引用服务配置&lt;dubbo:reference&gt; 发布服务1234&lt;!-- 和本地服务一样实现远程服务 --&gt;&lt;bean id = &quot;xxxService&quot; class = &quot;com.xxx.xxxServiceImpl&quot;/&gt;&lt;!-- 增加暴露远程服务配置 --&gt;&lt;dubbo:service interface=&quot;com.xxx.xxxService&quot; ref=&quot;xxxService&quot;&gt; 调用服务123456&lt;!-- 增加引用远程服务配置 --&gt;&lt;dubbo:reference id =&quot;xxxService&quot; interface=&quot;com.xxx.xxxService&quot;/&gt;&lt;!-- 和本地服务一样使用远程服务&gt;&lt;bean id =&quot;xxxAction&quot; class=&quot;com.xxx.xxxAction&quot;&gt; &lt;property name=&quot;xxxService&quot; ref=&quot;xxxService&quot;&gt;&lt;/bean&gt; 注册中心(Registry)注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力娇小，使用dubbo2.3.3以上版本，建议使用zookeeper注册中心zookeeper是Apache Hadoop的紫霞公募，是一个属性的目录服务，支持变更推送，适合作为Dubbo服务的注册中心 Zookeeper 安装 安装jdk 上传并解压zookeeper 将conf文件夹下的zoo-sample.cfg复制一份，改名位zoo.cfg 修改zoo.cfg的dataDir属性，指定zookeeper的真实目录 修改zoo.cfg的clientport属性，指定该服务器的zookeeper端口 启动zookeeper:zookeeper/bin/zkServer.sh start 关闭zookeeper:zookeeper/bin/zkServer.sh stop 查看zookeeper:zookeeper/bin/zkServer.sh status 框架整合Dubbo监控中心 https://github.com/alibaba/dubbo下载dubbo将dubbo-admin打成war包 1mvn package -Dmaven.test.skip=true 将war包复制到tomcat/webapps/中。修改/WEB-INF/dubbo.properties。包括地址和用户密码]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis框架小结]]></title>
    <url>%2F2016%2F10%2F31%2FMyBatis%2F</url>
    <content type="text"><![CDATA[学习MyBatis的一些个人总结 MyBatis环境配置 1 导入jar包 2 log4j.properties 3 sqlMapConfig.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- 先加载property，再加载properties,后加载覆盖--&gt;&lt;properties resource=&quot;db.properties&quot;&gt; &lt;property name=&quot;jdbc.driver&quot; value=&quot;AAA&quot;/&gt;&lt;/properties&gt;&lt;!-- 配置pojo别名--&gt; &lt;typeAliases&gt; &lt;!--&lt;typeAlias type=&quot;com.misaniy.po.User&quot; alias=&quot;user&quot; /&gt;--&gt; &lt;!-- 单个配置typeAlias 扫描包package，不区分大小写 --&gt; &lt;package name=&quot;com.misaniy.po&quot; /&gt;&lt;/typeAliases&gt;&lt;enviroments default=&quot;development&quot;&gt; &lt;enviroment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;/dataSource&gt; &lt;/enviroment&gt;&lt;/enviroments&gt;&lt;mappers&gt; &lt;!-- resource根据classpath查找，url绝对路径,class，接口全限定名（目录相同、名称一致、）、package扫描包 --&gt; &lt;mapper class=&quot;com.misaniy.mapper.UserMapper&quot; /&gt; &lt;mapper resource=&quot;XXX.xml&quot;&gt; &lt;package name=&quot;com.misaniy.mapper&quot; /&gt; &lt;mappers&gt;&lt;/configuration&gt; ####Config.xml配置内容和顺序 properties(属性) settings(全局配置参数) typeAliases(类型别名) typeHandlers(类型处理器) objectFactory(对象工厂) plugins(插件) enviroments（环境集合属性对象） enviroments(环境子属性对象) -transactionManager -dataSource mappers（映射器） 4 PO类5 映射文件xxx.xml 1234567891011121314&lt;!-- &lt;mapper namespace=&quot;test&quot;&gt; 传统--&gt;&lt;mapper namespace=&quot;com.misaniy.UserMapper&quot;/&gt;&lt;!-- 使用Mapper不需要DaoImpl --&gt;&lt;!-- 这里的Type使用的别名，类型由mybatis内部定制，pojo由config配置文件定制--&gt;&lt;select id=&quot;findUserByName&quot; parameterType=&quot;String&quot; resultType=&quot;user&quot;&gt; select * from user where name like &apos;%$&#123;value&#125;%&apos;&lt;/select&gt;&lt;insert id=&quot;insertUser&quot; parameterType=&quot;user&quot;&gt; &lt;selectKey keyProperty=&quot;id&quot; order=&quot;After&quot; resultType=&quot;Integer&quot;&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; insert into user(name) values(#&#123;name&#125;)&lt;/insert&gt;&lt;/mapper&gt; Mybatis解决jdbc的问题数据库链接和创建的频繁操作、sql硬编码、结果集封装到pojo、parameterType对输入参数的规范]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringWebMvc小结]]></title>
    <url>%2F2016%2F10%2F13%2FSpringWebMvc%2F</url>
    <content type="text"><![CDATA[学习SpringWebMvc的个人小结 SpringWebMvc执行流程 用户请求给前端控制器DispatcherServlet 前端控制器给HandlerMapping处理器映射器 处理器映射器根据映射找到Handler处理器 处理器返回ExcutionChain数据给前端控制器 前端控制器找到HandlerAdapter处理器适配器 处理器适配器找到处理器执行后返回ModelAndView给前端控制器 前端控制器把ModelAndView解析位Model和View，把Model赋值给View HandlerMapping ExcutionChain HandlerAdapter ModelAndView 视图解析器 解析Model赋值给View SpringMvc.xml配置文件 扫描Controller &lt; mvc:annotation-driver/&gt; 视图解析器 默认支持的数据类型 request response session Model ModelMap Map 基本数据类型 pojo类(属性名相同) 自定义参数类型 Converter 时间注解方式：pojo属性@DateTimeFormat(pattern=”yyyy-MM-dd”)]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于我]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[我是谁Misaniy 这个名字是因为她的名字是YUANSIYI，我从中抠了几个字符串，加上我的想念组合的。 上可陪领导逛街拎包吃喝玩乐，下可宅在家里追新番，热衷于研究新菜品和更优雅的代码，也没有忘记工作之余锻炼身体给自己未来投资，有一个非常有眼光的女朋友（这绝不是在夸我），也在努力成为一个优雅的hentai绅士。 热爱生活，热爱科技，爱小米，更爱小米的智能家居体系，有朝一日，我要让家里充满智能，充满GEEK的味道。 这是哪里misaniy.cc这是我的博客小站，也是我的黄金屋。在我的代码之夜上，地上有数不清的咯脚的石子，我摸索着前行，背后总有一个人意味深长地让我带上他们。如果你听过这个故事，你也会像我一样，写这样一个博客，把一路的石子捡起来。 我会不时更新我的博客，在我代码上遇到问题，或者我觉得需要记录的时候 怎么联系我Github:MisaniyEmail:MisaniyWeibo:为这美好的世界献上祝福Location：中国.重庆]]></content>
  </entry>
  <entry>
    <title><![CDATA[生活图]]></title>
    <url>%2Fgallery%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
