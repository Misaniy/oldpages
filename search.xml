<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据库基础小结]]></title>
    <url>%2F2017%2F11%2F19%2Fdatebase%2F</url>
    <content type="text"><![CDATA[数据库是入门Coder的基础。工具用太久，有时候甚至忘了一些基础，小结一下。 基本语法聚合函数 AVG COUNT MIN MAX SUM GROUP_CONCAT 返回由属于一组的列值连接组合而成的结果 表连接内连接From table1 a**inner join** table2 b **on** a.id=b.cid &gt; 亦可写成`From table1 a , table2 b where a.id=b.cid` &gt; 在连接条件中还有（&gt;、&lt;、&lt;&gt;、&gt;=、&lt;=、！&gt;和!&lt;） 外连接 左外连接 From table1 a **left join** table2 b **on** a.id=b.cid 左连接显示左表全部行 右外连接 From table1 a **right join** table2 b **on** a.id=b.cid 右连接显示右表全部行 全连接 From table1 a **full join** table2 b **on** a.id=b.cid 交叉连接（CROSS JOIN）也称笛卡尔积：不带WHERE条件子句，它将会返回被连接的两个表的笛卡尔积，返回结果的行数等于两个表行数的乘积,如果带WHERE，返回的是匹配的行数。 不带Where From table1 cross join table2 &gt;亦可写成`From table1 ,table2` 带Where From table1 a cross **join table2** b where a.id=b.cid 注意：cross join 后加条件只能用where ，不用用on 自连接连接的表都是同一个表，同样可以由内连接，外连接各种组合 From table1 a ,table1 b where a.id = b.id 子查询 为了给住查询（外部查询）提供数据而首先执行的查询（内部查询）被叫做子查询。也就是说，先执行子查询，根据子查询的结构，再执行主查询 IN、NOT IN、EXIST、NOT EXIST、=、&lt;&gt; 子查询的效率低于连接诶查询 事务事务的四个属性 原子性 事务是由一个或一组相互关联的SQL语句组成，这些语句被认为是一个不可分割的单元 一致性 对于数据库的修改是一致的，即多个用户查的数据是一样的。一致性主要由mysql的日志机制处理，他记录数据的变化，为事务恢复提供跟踪记录。 隔离性 每个事务都有自己的空间，和其他发生在系统中的事务隔离开来，而且事务的结果只在他完全被执行时才能看到 持久性 提交了这个事务之后对数据的修改更新就是永久的。当一个事务完成，数据库的日志已经被更新时，持久性即可发挥其特有的功效，在mysql中，如果系统崩溃或数据戒指被破坏，通过日志，系统能够恢复在重启前进行的最后一次成功更新，可以反应系统崩溃时处于执行过程的事物的变化 事务的四种隔离级别READ UNCOMMITTED(未提交读)事务A对数据做的修改，即使没有提交，对于事务B来说也是可见的，这种问题叫脏读 READ COMMITTED（提交读）事务A对数据做的修改，提交之后会对事务B可见。 &gt; 举例：事务B开启时独到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，会独到最新的数据2 REPEATABLE READ(可重复读)事务A对数据做的修改，提交之后，对于先于事务A开启的事务是不可见的。 &gt; 举例：事务B开启时读到数据1，接下来事务A开启，把这个数据改成2，提交，B再次读取这个数据，仍然独到数据1 SERIALIZABLE（可串行化）可串行化是最高的隔离级别。这种隔离级别强制要求所有事物串行执行，在这种隔离级别下，读取的每行数据都加锁，会导致大量的锁征用问题，性能最差 &gt; 随着隔离级别的增高，并发性能也会降低 mysql中的事务 事务的实现是基于数据库的存储引擎的。不同的存储引擎对事务的支持成都不一样 mysql支持的存储引擎中支持事务的InnoDB 事务的隔离级别是通过锁实现的，而事务的原子性、一致性和持久性是通过事务日志实现的 事务日志 redo——保障了事务的持久性和一致性 在InnoDB的存储引擎中，事务日志通过重做(redo)日志和innoDB存储引擎的日志缓冲(InnoDB Log Buffer)实现。事务开启时，事务中的操作，都会先写入存储引擎的日志缓冲中，在事务提交之前，这些缓冲的日志都需要提前刷新到磁盘上持久化，这就是DBA们口中常说的“日志现形”。当事务提交之后，在BUFFER POOL中映射的数据文件才会慢慢刷新到磁盘。此时如果数据库崩溃或者宕机，那么当系统进行恢复时，就可以根据redo log中记录的日志，把数据库恢复到崩溃前的一个状态，未完成的事务，可以继续提交，也可以选择回滚，这是基于恢复的策略而定 undo——保障了事务的原子性 undo log主要为事务的回滚服务。在事务执行的过程中，除了记录redo log，还会记录一定量的undo log。undo log记录数据在每个操作前的状态，如果事务执行过程中需要回滚，就可以根据undo log进行回滚操作。每个事务的回滚，只会当回滚当前事务做的操作，并不会印象到其他的事务做的操作 索引索引概述 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，他们包含着数据表里所有文件的引用指针。可以类比为书的目录，可以加快数据库的查询速度。 索引是创建在数据表对象上的。由表中的一个字段或多个字段生成的键组成，这些键存储在数据结构（B-树或哈希表）中，通过mysql可以快速有效的查找与键值相关联的字段 根据存储类型，分为B型树索引和哈希索引 InnoDB和MyISAM支持BTREE类型索引，默认 Memoery支持HASH类型的索引 索引分类普通索引最基本的索引，没有任何限制。MyIASM中默认的BTREE类型的索引，经常用 唯一索引与普通索引类是，不同的是：索引的列必须唯一，但允许有空值 主键索引它是一种特殊的唯一索引，不允许有空值 全文索引 FULLTEXT索引仅仅可用于MYISAM表 他们可以从CHAR、VARCHAR或TEXT列中作为CREATE TABLE语句的一部分被创建，或是随后使用ALTER TABLE 或CREATE INDEX被添加 注意：大容量的表，使用全文索引虽然速度更快，但是生产全文检索是一个非常消耗时间消耗磁盘空间的做法 组合索引 所谓多列索引，是指在创建索引的时候，锁关联的字段不是一个字段，而是多个字段。 虽然可以通过所关联的字段进行查询，但是只有查询条件中使用了所关联字段中的第一个字段，多列索引才会被使用 应用场景什么情况适合创建索引 经常被查询的字段，即在WHERE子句中出现的字段 在分组的字段，即在GROUP BY子句中出现的字段 存在依赖关系的子表和父表之间的联合查询，即主键或外键字段 设置唯一完整性约束的字段 什么情况不适合创建索引 在查询中很少使用的字段 拥有许多重复值的字段 索引失效 使用or关键字的时候索引失效，要想使用or,又想使用索引，只能将or条件中的每个列都加上索引 对于多列索引，不是使用的第一部分，则不会使用索引 like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来，否则不使用索引 如果mysql估计使用全表扫描要比使用索引快，则不使用索引 对索引列进行运算导致索引失效(+，-，*，/，！等) 独立的列（对列变量需要计算（聚合运算、类型转换等）） 在JOIN操作中（需要从多个数据表提取数据时），MYSQL只在主键和外键的数据类型相同时才能使用索引，否则即使建立了索引也不会使用 不使用NOT IN和&lt;&gt;操作，不会使用索引将进行全表扫描，NOT IN 可以NOT EXISTS代替，ID&lt;&gt;3则可以用id&gt;3 or id &lt; 3来代替 索引不会包含有NULL值的列，只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 查看索引和优化索引 查看索引show status like &#39;Handler_read%&#39; 优化索引 索引不会包含有NULL值的列，因此数据库设计时不要让字段的默认值为NULL 使用短索引 例如：如果有一个CHAR(255)的列，如果在前10个或20个字符内，多数值是唯一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 索引列排序 MySQL查询只使用一个索引，因此如果WHERE子句中已经使用了索引的话，那么order by 中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引 不要在列上进行运算 存储过程、存储函数什么是 一组预编译的SQL语句集 优点 只需要一次创建过程，以后在程序中就可以调用该过程任意次 允许更快执行，如果某操作需要执行大量SQL语句或重复执行，存储过程比SQL语句执行的要快 减少网络流量，例如一个需要数百行的SQL代码的操作有一条执行语句完成，不需要在网络中发送数百行代码 更好的安全机制，对于没有权限执行存储过程的用户，也可以授权他们执行存储过程 缺点 可移植性差，多个类型的数据库 学习成本高 如果存储过程中有复杂运算的话，会增加一些数据库服务端的处理成本，对于集中式数据库可能会导致系统可扩展性问题。 如何用存储过程1234CREATE PROCEDURE Proc()BEGINSELECT * tb_person;END; 存储函数123CREATE FUNCTION Query_score(classID INT,studentID INT)RETURNS INT RETURN(SELECT grade FROM tb_score WHERE cID=classID AND sID=studentID); 两者之间的区别 存储过程的功能更加复杂，而存储函数的功能针对性更强； 存储过程可以返回参数（通过OUT|INPUT），而函数只能返回单一值或者表对象； 存储过程作为一个独立的部分来执行，而函数可以可以作为查询语句的一部分来调用，由于函数可以返回一个表对象，因此它可以在查询语句中位于FROM关键字之后； 存储过程是通过关键字CALL来调用，作为一个独立的执行部分。而存储函数则可作为SELECT语句的一部分调用，嵌入到SQL语句中； 当存储过程和函数被执行的时候，SQLManager会到procedure cache中去取相应的查询语句，如果在procedure cache里没有相应的查询语句，SQLManager就会对存储过程和函数进行编译。 应用场景 复杂的数据处理用存储过程，如有些报表处理 多条件多表联合查询，并做分页处理，用存储过程也比较适合 优化SQL语句优化 对查询进行优化，尽量避免全表查询，首先考虑在where以及order by 涉及的列上建立索引。 应尽量避免在where子句中字段进行null值判断，否则将导致引擎放弃使用索引而进行全表扫描 应尽量避免在where子句中使用!=或&lt;&gt;操作符，否则将导致引擎放弃使用索引而进行全表扫描。 应尽量避免在where子句中使用or来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，可以使用union all来代替 in 和not in 也要慎用，否则会导致全表扫描，对连续的数据用between 代替，也可以用exists 代替in like ‘%a%’这种，索引失效，可以使用全文索引代替 对于多张大数据量（几百条就算）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差 少使用* 日期使用mysql自带的，少使用字符串存储，字符串的比较复杂；IP也使用int类型，不要使用字符串 尽可能的使用varchar/nvarchar代替char/nchar，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对娇小的字段内搜索效率显然要高些。 数据库优化主从复制（读写分离）使用spring可以实现读写分离 分库分表 分表 垂直分表 将表按照功能模块、关系密切程度划分出来，部署到不同的库上，比如我们会建立定义数据库workDB、商品数据库payDB、用户数据库userDB、日志数据库logDB等，分别用于存储项目数据定义表、商品定义表、用户数据表、日志数据表等 水平分表 指定自己的规则：1、求余。2、哈希。3、时间 分库 分库就是把一张表的数据分成N多个区块，这些区块可以在同一个磁盘上，也可以在不同的磁盘上 应用场景 表太多，海量数据，各项业务划分清除，低耦合的话，用垂直拆分比较号 表不多，单表数据量大的话，选水平拆分比较号 存储引擎查看mysql支持的存储引擎（SHOW ENGINES） myisam和innodb的区别 存储结构 MyISAM每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，拓展名指出文件类型。.frm文件存储表定义。数据文件的拓展名为.MYD(MYDate)。索引文件的拓展名是.MYI(MYIndex) InnoDB所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB 存储空间 MyISAM可被压缩，存储空间较小。支持三种不同的存储格式：静态表（默认，但是主义数据末尾不能有空格，会被去掉）、动态表、压缩表 innoDB需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于告诉缓冲数据和索引 可移植性、备份及恢复 MyISAM数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个进行操作。 innoDB免费的方案可以是拷贝数据文件、备份binlog，或用mysqldump，在数据量达到几十GB的时候就相对痛苦了。 事务支持 MyISAM强调的是性能，每次查询具有原子性，其执行速度比InnoDB类型要快，但是不提供事务支持。 innoDB提供事务支持，外部键等高级数据库功能。具有事务（commit），回滚（rollback）和崩溃修复能力（crash recovery capablities）的事务安全（transaction=safe(ACID compliant)）型表 表锁差异 MyISAM只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据 innoDB支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的性能，但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的 AUTO-INCREMENT MyISAM可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 innoDBInnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引。如果是组合索引也必须是组合索引的第一列。 全文索引 MyISAM支持FULLTEXT类型的全文索引。 innoDB不支持FULLTEXT类型的全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好 表主键 MyISAM允许没有任何索引和主键的表存在，索引都是保存行的地址。 innoDB如果没有设定主键或空孔唯一索引，就会自动生成一个6字节的主键（用户不可见），数据是主索引的一部分，附加索引保存的是主索引的值。 表的具体行数 MyISAM保存表的总行数，如果select count(*) from table会直接取出该值 innoDB没有保存表的总行数，如果使用select count(*) from table；会遍历整个表，消耗相当大，但是在加了where条件后，myisam和innodb处理的方式都一样。 CRUD操作 MyISAM如果执行大量的select，MyISAM是更好的选择 innoDB如果你的数据执行大量的insert或update，出于性能方面的考虑，应该使用InnoDB表。DELETE从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在InnoDB上如果要清空保存有大量数据的表，最好使用truncate table这个命令 外键 MyISAM不支持 innoDB支持 锁表锁行锁（InnoDB）共享锁又成为读锁，多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改 &gt; SELECT ... LOCK IN SHARE MODE 排他锁又称为写锁。排它锁就是不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据就行数据和修改。排他锁指的是一个事务在一行数据加上排它锁后，其他事务不能再加其他的锁。updata,delete,isnert都会自动给设计到的数据加上排他锁，select语句默认不会加任何锁类型 &gt; SELECT ... FRO UPDATE 乐观锁 乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行就爱南侧，如果发现冲突了，则返回用户错误的信息，让用户决定如何去做 实现方式 版本（Version）记录机制 一般是通过为数据库表增加一个数字类型的”version”字段来实现。当读取数据时，将version字段的值一同独处，数据每更新一次，对此version值加一。当我们提交更新的时候，判断数据库表对应记录的当前版本信息与第一次取出来的version值进行比较。如果数据库表当前版本号与第一次取出来的version值相等，则予以更新，否则认为是过期数据 时间戳（timestamp） 悲观锁它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守状态（悲观），因此，在整个数据处理过程中，将数据处于锁定状态。 &gt; SELECT ... FOR UPDATE 或 LOCK IN SHARE MODE select语句完整的执行顺序from → where → group by → having →表达式 → group by → select 输出]]></content>
      <categories>
        <category>DB集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[集合小结]]></title>
    <url>%2F2017%2F10%2F17%2Fcollection%2F</url>
    <content type="text"><![CDATA[计算机科学中，集合是一组可变数量的数据项(也可能是0个)的组合，这些数据项可能共享某些特征，需要以某种操作方式一起进行操作。 单例集合List特点：元素有放入顺序，元素可重复 ArrayList 长度可变的数组 有索引结构，使用索引在数组中搜索和读取数组是很快的，也就是查找快 数组这种数据结构将对象放在连续的位置中，因此增加、删除的速度比较慢 存数类型是Object，允许为null 初始大小（不设置大小的时候默认是10） 不是线程安全的，异步的 LinkedList 链表的数据结构 存储的类型是Object,允许为null 不是线程安全的，得自己实现线程安全 在创建List时构造一个同步的List List list = Collections.synchronizedList(new LinkedList(...)); Vector 底层是数组结构 同步的，线程安全 执行效率低 Vector缺省情况下自动增长原来一倍的数组长度 在集合中保存大量的数据那么使用Vector有一些有事，因为你可以通过设置集合的变化大小来避免不必要的资源开销 初始大小是10 Set特点：元素无放入顺序，且不可重复（实际存储顺序是HashCode决定的，固定的） HashSet 基于hashmap实现，底层使用hashmap保存元素 set元素中的值存放在hashmap的key中，value存放了一个模拟值persent 由于hashmap中的key是不能重复的，所以hashset就利用这个特性实现了set中的值不会重复 hashset是如何保证元素的唯一性 基于哈希表实现的。哈希表是存储一系列哈希值的表，而哈希值是由对象的hashcode()方法产生的。 确保元素唯一性的两个方法：hashcode()和equals()方法 当调用add（）方法向集合中存入对象的时候，先比较对象的哈细致有没有一样的，如果都不一样就直接存入。如果有与之相同的哈希值，则要继续比较两个对象是否是同一个对象，调用对象的equals方法TreeSet 基于treeMap实现的 有序的二叉树多例集合 HashTable 父类是Dictionary 实现一个key-value映射的哈希表。任何非空对象都可作为key或者value 同步的，线程安全 内部通过单链表解决冲突问题 效率低 有contains方法 初始大小是11 扩容时，hashTable采用的是2*old+1 hash值的计算使用的直接是对象的hashcode方法 迭代器，HashTable用Enumeration来遍历数据 只提供了遍历Vector和Hashtable类型集合元素的功能 这种类型的集合对象通过调用elements()方法获取一个Enumeration对象，然后Enumeration对象再调用一下方法来对集合中元素进行遍历。 hasMoreElements():判断Enumeration对象中是否还有数据nextElement():获取Enumeration对象中的下一个数据 HashMap 弗雷是AbstractMap 哈希表，允许key和value值为空 方法是同步的，线程不安全 为什么不安全 首先如果多个线程同时使用put方法添加元素，而且假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖 如果多个线程同时检测到元素个数超过数组大小*loadFactor，这样就会发生多个线程同时对Node数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋值给table，也就算说其他线程的都会丢失，并且各自线程put的数据也丢失 解决方案 HashtableConcurrentHashMapSynchronized Map 效率高 有containsvalue()和containsKey()方法 初始大小是16 扩容时，hashMap采用的是2*old hash值的计算没有直接使用对象的hashcode方法，为对象key的hashcode值与对象value的hashcode值按位与或操作 迭代器 hashMap的工作原理 当两个对象的hashcode值相同时会发生什么？(hashmap中的碰撞探测(collision detection)及解决方案) 因为hashcode值相同，因此他们的bucket位置相同，碰撞就会发生 因为hashMap使用链表存储对象，这个Entry(包含有监支队的Map.Entry对象)会存储在链表中 如果有两个键的hashcode相同，你如何取对象？ 当我们调用get()方法，hashMap会使用建对象的hashcode找到bucket位置，然后获取值对象 找到bucket位置之后，会调用keys,equals()方法去找到链表中正确的节点，最终找到要找的值对象 如果HashMap的大小超过了负载银子(load factor)定义的容量，怎么办？ 默认的负载银子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其他集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中，这个过程叫做rehashing，因为它调用hash方法找到新的bucket位置 重新调整HashMap大小存在什么问题吗？ 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，他们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将呀un苏放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件发生了，那么就死循环了。 为什么String、Integer这样的wrapper类适合作为键 String、Integer这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的。也是final的，而且已经重写了equals()和hashcode()方法了。其他的wrapper类也有这个特点。不可变形是必须，因为为了要计算hashCode(),就要防止键值改变，如果键值在放入时和获取是i返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性是必要的，因为为了要计算hashCode()，就要防止兼职改变。如果键值在放入和获取时返回不同的hashCode就不能从HashMap中找到想要的对象。不可变性还有其他优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法非常重要。如果两个不想等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 ConcurrentHashMap CHM不但是线程安全的，而且比HashTable和synchronizedMap的性能要好 相对于HashTable和synchronizedMap锁住了整个Map,CHM只锁住部分Map. CHM允许并发的读操作，同时通过同步锁在写操作时保证数据完整性 如何实现 CHM引入了分割，并提供了HashTable支持的所有的功能 在CHM中，支持多线程对Map做读操作，并且不需要任何的blocking CHM将MAP分割成不同的部分，在执行更新操作的时候只锁住了一部分 根据默认的并发级别，Map被分割成16个部分，并且由不同的锁控制。这样的话可以同时最多有16个写线程进行操作 但由于一些更新操作，如put()，remove(),putAll(),clear()只锁住操作的部分，所以检索操作不能保证返回的是最新的结果 在迭代遍历CHM时，keySet返回的是iterator是弱一致性和fail-safe的，可能不会返回某些最近的改变，并且在遍历过程中，如果已经遍历的数组上的内容变化了，不会抛出ConcurrentModificationException的异常 CHM默认的并发级别是16，但可以在创建CHM时通过构造函数改变 CHM不允许null的键值 应用场景 CHM适用于读数量超过写 当写数量大于读时，CHM的性能低于Hashtable的synchronized Map的。这是因为当锁住了整个Map时，读操作要等待对统一部分执行写操作的线程结束 CHM适用于cache,在程序启动时初始化，之后可以被多个请求线程访问。]]></content>
      <categories>
        <category>基本集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java7的新特性Paths,Files]]></title>
    <url>%2F2017%2F09%2F25%2FJAVA7PathsFiles%2F</url>
    <content type="text"><![CDATA[在使用lucene 5的时候，发现在lucene4时file的地方用到了Path,发现这是JAVA7的新特性，于是查找相关文档，发现在IO方面,java7新增了Paths,Files工具类，发现异常强大，小结一下。 PathsPath是用来表示文件路径和文件，可以有多种方法来构造一个Path对象来表示一个文件路径或一个文件 在Paths类里有两个static方法 12345public static Path get(String first,String...more) &#123; return FileSystems.getDefault().getPath(first,more);&#125;public static Path get(Url url) 得到三种构造方式(以源文件d:/demo.txt为例) 1234Path path1 = Paths.get(&quot;d:/&quot;,&quot;demo.txt&quot;);Path path2 = Paths.get(&quot;d:/demo.txt&quot;);Path path3 = Paths.get(URI.create(&quot;file:///d:/demo.txt&quot;));Path path4 = FileSystems.getDefault().getPath(&quot;d:/&quot;,&quot;demo.txt&quot;); File和Path、File和URI之间的转换 12345File file = new File(&quot;d:/demo.txt&quot;);Path path = file.toPath();File file2 = path.toFile();URI uri = file.toURI(); 读取文件属性 1234567Path path = Paths.get(url);path.getFileName();path.getParent();//根目录path.getRoot();//目录级数(D:\xxx\xxx\xxx\demo.txt 4)path.getNameCount(); 创建一个文件 123Path path = Paths.get(&quot;C:\demo.txt&quot;);if(Files.exists(path)) Files.createFile(path); Files.newBufferWriter写入文件 1234BufferedWriter writer = Files.newBufferedWriter(Paths.get(&quot;D:\\demo.txt&quot;),Charset.forName(&quot;UTF-8&quot;)); writer.write(&quot;测试中文&quot;); writer.flush(); writer.close(); Files.newBufferWriter读取文件 123456BufferedReader reader = Files.newBufferedReader(Paths.get(&quot;D:\\demo.txt&quot;), Charset.forName(&quot;UTF-8&quot;)); String str = null; while((str = reader.readLine())!=null)&#123; System.out.println(str); &#125; reader.close(); 遍历文件夹,这里只遍历当前目录，不遍历子目录 12345678910111213Path path = Paths.get(&quot;D:\\dir&quot;); DirectoryStream&lt;Path&gt; paths = Files.newDirectoryStream(path); for(Path p : paths)&#123; System.out.println(p.getFileName()); &#125; DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(Paths.get(&quot;D:\\dir&quot;)); Iterator&lt;Path&gt; ite = stream.iterator(); while (ite.hasNext())&#123; Path path = ite.next(); System.out.println(path.getFileName()); &#125; 要遍历子目录，在java7前需要用递归，而java7的Files提供了walkFileTree()方法，这个在另一篇文章写到 Files 创建目录和文件 123Files.createDirectories(Paths.get(&quot;D://dir&quot;));if(!Files.exists(Paths.get(&quot;D://dir&quot;))) Files.createFile(Paths.get(&quot;D://dir/demo.txt&quot;)) 文件复制 123456789101112//Files.copy(Source,Target,CopyOptions) //StandardCopyOption //REPLACE_EXISTING 如果存在替换 //COPY_ATTRIBUTES 复制 //ATOMIC_MOVE Move the file as an atomic file system operation.//Files.copy(Source,OutputStream)//Files.copy(InputStream,Target,CopOption)Files.copy(Paths.get(&quot;C://Source.txt&quot;,Paths.get(&quot;D://Target.txt&quot;,StandardCopyOption.COPY_ATTRIBUTES))); 读取文件属性 123456789Path path = Paths.get(url);//最后一次修改时间 System.out.println(Files.getLastModifiedTime(path)); System.out.println(Files.size(path));//是否为一个连接 System.out.println(Files.isSymbolicLink(path)); System.out.println(Files.isDirectory(path));//指定属性，*表全部 System.out.println(Files.readAttributes(path,&quot;*&quot;));]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JavaWeb]]></title>
    <url>%2F2017%2F09%2F17%2Fjavaweb%2F</url>
    <content type="text"><![CDATA[Java Web，是用Java技术来解决相关web互联网领域的技术总和。web包括：web服务器和web客户端两部分。Java在客户端的应用有java applet，不过使用得很少，Java在服务器端的应用非常的丰富，比如Servlet，JSP和第三方框架等等。 Http协议 是一个基于请求与响应模式的、无状态的、应用层的协议，基于TCP的连接方式 请求： 请求行、消息报头、请求正文 响应：状态行、消息报头、响应正文 状态码 1XX：提示信息——表示请求已解说，继续处理 2XX：成功——表示请求已被成功接收、理解、接受 3XX：重定向——要完成请求必须进行更进一步的操作 4XX：客户端错误——请求有语法错误或请求无法实现 5XX：服务器端错误——服务器未能实现合法的请求 TomcatServletServlet是一个web容器，我们通常用的servlet是httpservlet，而httpservlet又是继承于genericservlet，而genericservlet又实现了servlet接口 生命周期 实例化——init() 初始化 提高服务——service() 销毁 不可用 应用场景 在调用service()方法中，因为我们继承了httpservlet，其实就是对应了doGet(),doPost()方法。 常用的MVC框架，strus,spring这些框架都是基于servlet发展二来的，比如struts1的核心控制器是ActionServlet，而MVC的前端控制器是dispatchServlet Servlet的由于是单例的，所有请求都使用一个实例，所以如果有全局变量被多线程使用的时候，就会出现线程安全问题。解决方案如下： 实现singleThreadModel接口，这样对于每次请求都会创建一个新的servlet实例，这样会消耗服务器内存，且已经过时。 通过加锁(synchronized)来避免线程安全问题。这个时候虽然是单例，但是对于多线程的访问，每次只能有一个请求进行方法体内执行，只有执行完毕后，其他线程才允许访问，降低吞吐量。 避免使用全局变量，使用局部变量可以避免线程安全问题，强烈推荐使用此方法来解决servlet线程安全问题 12345678&lt;servlet&gt; &lt;servlet-name&gt;唯一性 &lt;servlet-class&gt;提高的服务&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt; &lt;url-pattern&gt;&lt;/servlet&gt; Listener一个Java类，用来监听其他的Java的状态的变化 生命周期应用场景 监听其他对象的变化 应用在图形化界面中比较多FilterFilter是一个过滤器，用来在请求前和响应后进行数据的处理生命周期 实例化 初始化——init 进行过滤——doFilter 销毁——destroy 释放资源 应用场景 编码转换 123456789101112131415//实现Filter类public class CharactorFilter implements Filter &#123; //定义转换字符集类型 String encoding = utf-8; public void DoFilter(ServletRequest request,ServletResponse response,FilterChain chain) throws Exception&#123; if(encoding!=null)&#123; //设置request字符编码 request.setCharacterEncoding(encoding); //设置response字符编码 response.setContentType(&quot;text/html;charset=&quot;+encoding); &#125; chain.doFilter(request,response); &#125;&#125; web.xml 12345678&lt;filter&gt; &lt;filter-name&gt; &lt;filter-class&gt;上文过滤器的全限定名&lt;/fitler&gt;&lt;filter&gt; &lt;filter-name&gt; &lt;url-pattern&gt;&lt;/filter&gt; 安全验证 123456789101112//实现Filter类public LoginFilter implements Filter&#123; public void doFilter(ServletRequest request,ServletResponse response,FilterChain chain) throws Exception&#123; HttpServletRequest httpRequest = (HttpRequest) request; HttpServletResponse httpResponse = (HttpResponse) response; HttpSession session = httpRequest.getSession(); if(session.getAttribute(&quot;username&quot;)!=null)&#123; chain.doFilter(request,response); &#125; &#125; 重复提交的判断 1234567891011121314public class HttpRequestContentFilter implements FIlter &#123; private Servlet Context context; public void init(FilterConfig filterConfig) throws Exception&#123; context = filterConfig.getServletContext(); &#125; public void doFilter(ServletRequest request,ServletResponse response,FilterChain chain) throws Exception&#123; //在ThreadLocal中共享本次请求、响应对象 HttpRequestContext.setHttpRequestContext((HttpServletRequest)request,(HttpServletResponse)response,context); chain.doFilter(request,response); &#125;&#125;]]></content>
      <categories>
        <category>Web集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux误删除了备份的数据库]]></title>
    <url>%2F2017%2F09%2F13%2FOracleBankDelete%2F</url>
    <content type="text"><![CDATA[延迟加载异常：failed to lazily initialize a collection of role: com.misaniy.bos.domain.base.Courier.fixedAreas, could not initialize proxy - no Session reason：误删除了备份的数据库resolve：12345678910111213141516sqlplus /nolog//使用数据库命令模式connect system/root as sysdba//连接数据库SQL&gt;shutdown normal//关闭数据库oracle服务SQL&gt;startup mount//重新启动Oracle服务SQL&gt;alter database open;//打开数据库//SQL&gt;alter database datafile 5 offline drop 若出现错误SQL&gt;alter database open;//重新更改数据库的openSQL&gt;startup]]></content>
      <categories>
        <category>报错集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[could not initialize proxy - noSession]]></title>
    <url>%2F2017%2F09%2F13%2Flazily%2F</url>
    <content type="text"><![CDATA[failed to lazily initialize a collection of role: com.misaniy.xxx, could not initialize proxy - no Session延迟加载异常 解决方案有两种其一，在web.xml中配置Spring的OpenSessionInViewFilter，确保服务器端的逻辑执行完后再关闭session，这是针对hibernate的支持类12345678910111213&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.jpa.support.OpenEntityManagerInviewFilter&lt;/filter-class&gt; &lt;!-- 如果你的sessionFactory不是叫sessionFactory，需要配置如下--&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt; &lt;param-value&gt;&#123;Your Session Factory Name&#125;&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 其二，上面方法是hibernate的支持类，如果你配置的不是sessionFactory,比如我用的SPRING DATA JPA，就用如下方法1234@JSON(serialize = false)public XXX getXXX()&#123; return XXX;&#125; 在Bean类找到你的延迟加载的数据，没有使用到就使用该注解]]></content>
      <categories>
        <category>报错集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Oracle和tomcat端口冲突]]></title>
    <url>%2F2017%2F09%2F12%2FPortException%2F</url>
    <content type="text"><![CDATA[Oracle XE http与tomcat端口冲突8080 reason：oracle与tomcat端口8080冲突，我们可以修改任意一个端口； resolve：修改oracle123sqlplus system/rootSQL&gt;call dbms_xdb.sethttpport(&apos;8082&apos;); 修改tomcat，这里由于用了maven,所以直接安装tomcat7插件Maven —&gt; build plugin —-&gt;tomcat712345678&lt;plugin&gt;&lt;gourpId&gt;org.apache.tomact.maven&lt;/groupId&gt;&lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt;&lt;version&gt;2.2&lt;/version&gt;&lt;configuration&gt; &lt;port&gt;8081&lt;/port&gt; &lt;path&gt;/&lt;/path&gt;&lt;/configuration&gt; Run As —&gt; Maven InstallRun As —&gt; Maven Build… tomcat7:run]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Mybatis的分页插件PageHelper]]></title>
    <url>%2F2017%2F08%2F10%2FPageHelper%2F</url>
    <content type="text"><![CDATA[逆向工程生成的代码是不支持分页处理的，如果想进行分页需要自己编写mapper，这样就失去逆向工程的意义了。为了提高开发效率可以是要弄个mybatis的分页插件PageHelper. 简介该插件目前支持Oracle，MySQL，MariaDB,SQLite,Hsqldb,PostgreSQL六种数据库分页. 使用方法 把PageHelper依赖的jar包添加到工程中。官方提供的代码对逆向工程支持的不好，使用民间修改版pagehelper-fix. 在Mybatis配置xml中配置拦截器插件 12345&lt;plugins&gt; &lt;plugin interceptor=&quot;com.github.pagehelper.PageHelper&quot;&gt; &lt;property name=&quot;dialect&quot; value=&quot;mysql&quot;/&gt; &lt;/plugin&gt;&lt;/plugins&gt; 代码 12345678910//获取第1页，10条内容，默认查询总数countPageHelper.startPage(1,10);//分页处理List&lt;E&gt; list = ;//取分页信息PageInfo&lt;E&gt; pageInfo = new PageInfo(list);pageInfo.getTotal();pageInfo.getPages();pageInfo.getPageNum();pageInfo.getPageSize();]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[浅谈数据库优化]]></title>
    <url>%2F2017%2F05%2F15%2Fsql%2F</url>
    <content type="text"><![CDATA[数据库优化分为硬优化、软优化。硬优化主要是指针对数据库本身的优化（表和库），软优化主要就是针对sql语句之类的优化。 软优化查询条件尽量加索引- 这个不是必须加的，但很大成都上能够解决一定查询效率的问题。但是需要主义的是不要在索引字段上做计算、函数等操作。 少部分关键字的使用需要减少- 避免使用Like - exit not exit来代替not in - 字段名代替* - id&gt;=3代替id&gt;2 减少子查询的使用- 复杂业务简单化，不使用子查询 - 先生成临时表，再做关联查询 其他- 减少对数据库的重复操作，能合并就合并，能一条sql就不要分多条 - 其他有的再补充 硬优化拆表 在很多情况下，某一张表某些字段经常被查询到，那么我们就有必要拆分表 正如这张图，在id\name\attr7\attr8\attr9常用的情况下，把表拆分成两张表再关联起来 分表 单表数据太多，查询的效率极慢的情况下我们可以把表里的数据分成几张表来存储 就如上图，我们每次查询一月份的数据的时候都要在全年度的表里去查，数据量太大，查询了会很慢，所以我们将每个月份的数据单独提出来做成表，这样查询就相对比较快了 读写分离 读写分离，主要针对数据库访问的量很大，导致数据库运行可能宕机的问题。原理就是添加几个数据库，形成集群将访问量平均分配到每个数据库上 如上图，我们只需要修改主库数据，通知到每个分类，从库更新之后，每次查询我们都将查询分配到每个从库就可以了]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[全文检索和Lucene]]></title>
    <url>%2F2017%2F04%2F27%2FFull-Context%2F</url>
    <content type="text"><![CDATA[SQL语句的like会搜索大量不相关的内容，不走索引，且，存在资源浪费。这时我们就需要用到全文检索。 全文检索和Lucene全文检索了解Lucene之前，我们需要清楚全文检索的概念。 生活中的数据分为三种。 结构化数据：具有固定格式或有限长度的数据，如数据库，元数据等。 非结构化数据：不定长度或无固定格式的数据，如邮件，word文档等。 半结构化数据：根据需要可以按结构化数据处理，也可抽取出纯文本按非结构化数据来处理。 对应的搜索分为两种。 对结构化数据的搜索：sql语句、windows搜索文件名、类型、修改时间等 对非结构化数据的搜索：windwos搜索文件内容、linux的grep,搜索引擎的搜索等 对非结构化数据搜索即对全文数据的搜索分为两种： 顺序扫描法：假设寻找某个字符串的文件，就是一个文档一个文档读，然后每个文档从头读到尾，Linux下的grep就是这种方式，小数据量可以使用，但对于大量数据，就很慢了。 全文检索：将非结构化数据中的一部分信息提取出来，重新组织，使其变为结构化数据，我们称之为索引，而这种先建立索引，再搜索的过程就叫全文检索。 对应的创建索引方式分三种。 索引：加快数据搜索的一种数据结构 I/O流：对于本地文件创建索引。 爬虫：模拟访问URL，获取网页数据，搜索引擎使用。 SQL搜索：对于存放在数据库的数据使用。 如何创建索引 源文档Document 文档中包括一个一个域(Field)[file_name,file_path,file_size,file_content等] 分词组件Tokenizer得到词元Token 将源文档分词 去除标点 去除停词 处理组件LinguisticProcessor得到词Term 变为小写 缩减位词根 转变为词根 索引组件Indexer 用词Term创建字典 对字典按字母顺序排序 合并相同的词Term成为倒排索引(Posting List) 倒排索引：从字符串到文件的映射是文件到字符串映射的反向过程，所以这种索引称为倒排索引 如何对索引进行搜索 用户输入查询语句 对查询语句进行分析处理 搜索索引，得到符合语法树的文档 根据得到的文档和查询语句的相关性排序 LuceneLucene实现全文检索 获得原始文档 创建文档对象 分析文档 创建索引 查询索引库 常用域Field 创建索引库12 索引库的维护索引库的添加123456789//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//Document.add(Field)Document document = new Document();document.add(new TextField(&quot;name&quot;,&quot;文档&quot;.Store.YES));document.add(new TextFiled(&quot;content&quot;,&quot;内容&quot;,Store.YES));IndexWriter.addDocument(document);IndexWriter.close(); 索引库的删除 删除全部 123456//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//删除全部索引indexWriter.deleteAll();indexWriter.close(); 指定条件删除 12345678//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//QueryQuery query = new TermQuery(new Term(&quot;name&quot;,&quot;文档&quot;));//指定条件删除indexWriter.deleteDocuments(query);indexWriter.close(); 索引库的修改123456789//IndexWriterIndexWriter indexWriter = new IndexWriter(FSDirectory.open(new File(Path)),new IndexWriterConfig(Version.LATEST,new IKAnalyzer()));//修改后的DocumentDocument document = new Document();document.add(new TextField(&quot;name&quot;,&quot;新文档&quot;,Store.YES));//updateDocumentindexWriter.updateDocument(new Term(&quot;content&quot;,&quot;文档&quot;),document);indexWriter.close(); 查询索引Query的子类查询TermQuery 精确查找123456789101112131415//IndexSearcherIndexSearcher indexSearcher = new IndexSearcher(Directory.open(FSDirectory.open(new File(Path))));Query query = new TermQuery(new Term(FieldName,keyStr));TopDocs topDocs = indexSearcher.searcher(query,100);//topDocs.scoreDocs存储了document的idfor(ScoreDoc scoreDoc : topDocs.scoreDocs)&#123; //scoreDoc.doc就是document的id Document document = indexSearcher.doc(scoreDoc.doc); document.get(keyStr);&#125;indexSearcher.getindexReader.close(); MatchAllDocsQuery 所有文档NumericRangeQuery 数值范围查找BooleanQuery 组合条件查找IndexSearcher搜索方法queryparser 查询QueryParserMulitFieldQueryParser TopDocs]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[利用Redis实现缓存]]></title>
    <url>%2F2017%2F04%2F13%2FRedis%2F</url>
    <content type="text"><![CDATA[实现缓存的工具有很多，现在比较流行的是Redis。Redis(remote dictionary server)缓存存放在内存，数据库存放在磁盘，访问内存更快。频繁访问而不频繁修改的数据放在缓存里，能带来更好的体验和更小的服务器压力。 Redis安装Redis即便是高并发也是单线程的，不适合保存内容大的数据。 Linux下安装由于Redis是C语言开发的，安装Redis前需要安装c语言的编译环境。如果没有gcc需要在线安装。Yum install gcc-c++解压后编译make安装make install PREFIX=/usr/local/redis 连接Redis 启动Redis服务 前端启动 bin]./redis-server 后台启动 把redis3.0/redis.conf复制到redis/bin 目录下 修改配置文件daemonize yes 启动服务bin]./redis-server redis.conf 启动客户端 bin]./redis-cli默认连接localhost:6379 bin]./redis-cli -h 192.168.25.130 -p 6379 -h:Path -p:Port Redis五中数据类型 String:key-value(做缓存) get/incr/decr key set key value Hash:key-fields-values(做缓存) hget key field hset key field value hincrby key field int List:有顺序可重复 （类似堆） rpush key values往右添加 lpush key values 往左添加（倒序） lrange 0 -1 查询所有 lpop 从左取 rpop 从右取 Set:无顺序，不能重复 sadd key values smembers key srem key 删除 SortedSet(zset):有顺序，不能重复 zadd key n valueszadd zset1 2 a 5 b 1 c 6 d zrange key 0 -1 c a b d zrevrange key 0 -1 倒序查看 Key命令 expire key time 设置失效时间 ttl key 查看有效期 persist key 解除失效（持久化） Redis的持久化方案Redis所有数据都是保存在内存中的。 Rdb:快照形式。定期把内存中当前时刻的数据保存到磁盘。Redis默认支持的持久化方案。一直开启 Aof:Append Only File。把所有对Redis数据库操作的命令，增删改命令保存到一个文件中。数据库恢复时，把所有命令执行一遍就可恢复。开启后会降低一点性能 在Redis.conf配置文件中配置。 默认RBD123save 900 1save 300 10save 60 10000 - 备份文件 dump.rdb - AOF.每秒执行一次 `appendonly yes` Redis集群搭建架构细节 所有Redis节点彼此互联(PING-PONG机制)，内部使用二进制协议优化传输速度和带宽。 节点的fail是通过集群中超过半数的节点检测失效时才生效。 客户端与redis节点直连，不需要中间proxy层，客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 redis-cluster把所有物理节点映射到[0-16383]slot上，cluster负责维护node&lt;-&gt;slot&lt;-&gt;value Redis集群中内置了16384个哈希槽，当需要在Redis集群中防止一个key-value时，redis先对key使用crc16算法算出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，redis会根据节点数量大致均等的将哈希槽映射到不同的节点 集群搭建Redis集群中至少应该有三个节点，为了保证集群的高可用，需要每个节点有一个备份机。所以至少需要6台服务器。 搭建伪集群 复制单机版redis复制5份redis-cluster 修改每一个redis节点的端口port以及启用集群cluster-enabled yes 启动每一个节点./redis-server redis.conf 创建集群 安装Ruby的运行环境yum install ruby 安装Ruby的包管理器yum install rubygems 安装脚本gem install redis-3.0.0.gem 复制redis-3.0.0/src/redis-trib.rb到redis 使用ruby脚本搭建集群./redis-trib.rb create --replicase 1 192.168.XX.XX:7001 1992.168.XX.XX:7002 ...... 创建关闭集群的脚本 redis-cluster]vim shutdown-all.sh 123redis01/redis-cli -h address -p 7001 shutdownredis01/redis-cli -h address -p 7002 shutdownredis01/redis-cli -h address -p 7003 shutdown chmod u+x shutdown-all.sh这里这个redis-cli可以是任意一个redis集群的客户端 集群的使用方法Redis-cli 连接集群redis-cluster]redis01/redis-cli -p 7002 -c-c:代表连接的redis集群 Jedis连接单机版1234Jedis jedis = new Jedis(ipaddress,port);jedis.set(key,value);String result = jedis.get(key);System.out.print(result); 连接单机版使用连接池1234567JedisPool jedisPool = new JedisPool(ipaddress,port);Jedis jedis = jedisPool.getResource();jedis.set(key,value);String result = jedis.get(key);System.out.print(result);jedis.close();jedisPool.close(); 连接集群版1234567891011121314//第一步：使用JedisCluster对象，需要一个Set&lt;HostAndPort&gt;参数，Redis节点的列表Set&lt;HostAndPort&gt; nodes = new HashSet&lt;&gt;();nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));nodes.add(new HostAndPort(ipaddress,port));JedisCluster jedisCluster = new JedisCluster(nodes);//第二部：直接使用JedisCluster对象操作redis,在系统中举例存在。jedisCluster.set(key,value);String result = jedisCluster.get(key);//关闭JedisCluster对象。jedisCluster.close();]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Solr安装与介绍]]></title>
    <url>%2F2017%2F04%2F02%2Fsolr%2F</url>
    <content type="text"><![CDATA[Solr是基于Lucene的全文搜索服务器 Solr的安装和配置Solr配置到tomcat 把Solr的war包赋值到tomcat的webapp下并解压 把Solr/example/lib/ext目录下的jar包添加到solr工程中. 配置SolrHome和SolrCore Solr/example/solr文件就是一个标准SolrHome，复制出来命名solrHome solrHome中的collection1就是一个SolrCore solrCore下有一个目录conf，conf下的solrconfig.xml可以配置相关信息。 solrconfig.xml env-entry-value:配置solrhome的绝对路径 Lib:solr服务依赖的拓展包 dataDir:配置索引库存放路径 requestHandler:查询时使用的url 打开Schema.xml可以看到Solr默认的FieldType class:Solr提供的包solr.TextField，solr.TextField允许用户通过分析器来定制索引和查询，分析器包括一个分词器(tokenizer)和多个过滤器(filter) positionIncrementGap:可选属性，定义在同一个文档中此类型数据的空白间隔，避免短语匹配错误，此值相当于Lucene的短语查询设置slop值，根据经验设置位100 analyzer 搜索分析器 solr.StandardTokenizerFactory 标准分词器 solr.StopFilterFactory 停用词过滤器 solr.LowerCaseFilterFactory 小写过滤器 索引分析器 solr.StandardTokenizerFactory 标准分词器 solr.StopFilterFactory 停用词过滤器 solr.SynonymFilterFactory 同义词过滤器 Field 定义 name 域名 type FieldType indexed 是否索引 stored 是否存储 multiValued 是否存储多个值 uniqueKey 默认定义唯一主键key为id域 &lt;uniqueKey&gt;id&lt;/uniqueKey&gt; copyField 复制域 将多个Field复制到一个Field中，进行统一检索 比如，输入关键字搜索name/description 1234567&lt;!-- 定义name/description/keywords的Field --&gt;&lt;field name=&quot;keywords&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot;/&gt;&lt;field name=&quot;name&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;&lt;field name=&quot;description&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;false&quot; /&gt;&lt;!-- 只搜索keywords就相当于搜索了name和description&lt;copyField source=&quot;name&quot; dest=&quot;keywords&quot;&gt;&lt;copyField source=&quot;description&quot; dest=&quot;keywords&quot;&gt; dynamicField(动态字段)自定义Field为:product_title_t&lt;dynamicField name=&quot;*_t&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt; 配置中文分析器 将IKAnalyzer2012FF_ul.jar添加到solr/WEB-INF/lib 赋值IKAnalyzer的配置文件和自定义词典到solr的classpath下 在schema.xml中添加一个自定义的filedType 123&lt;fieldType name=&quot;text_ik&quot; class=&quot;solr.TextFiled&quot;&gt; &lt;analyzer class=&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;/&gt;&lt;/fieldType&gt; 定义field，指定field的type属性为text_ik Solr管理索引库维护索引 添加/更新文档 使用Dataimport批量导入数据 导入solr-dataimporthandler.jar、solr-dataimporthandler-extras.jar、mysql数据库jar包 配置solrconfig.xml，添加一个requestHandler 12345&lt;requestHandler name=&quot;/dateimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; 创建一个data-config.xml，保存到solrcore\conf\目录下 1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;dataConfig&gt;&lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver url=&quot;jdbc:mysql://localhost:3306/database&quot; user=&quot;root&quot; password=&quot;root&quot;/&gt;&lt;document&gt; &lt;entity name=&quot;product&quot; query=&quot;SELECT pid,name,catalog_name,price,description,picture FROM products&quot;&gt; &lt;field column=&quot;pid&quot; name=&quot;id&quot;/&gt; &lt;field column=&quot;name&quot; name=&quot;product_name&quot;/&gt; &lt;field column=&quot;catalog_name&quot; name=&quot;product_catalog_name&quot;/&gt; &lt;field column=&quot;description&quot; name=&quot;product_price&quot;/&gt; &lt;field column=&quot;picture&quot; name=&quot;product_picture&quot;/&gt; &lt;/entity&gt;&lt;/document&gt;&lt;/dataConfig&gt; 重启tomcat Excute导入数据 删除文档- 删除指定ID的索引 123&lt;delete&gt; &lt;id&gt;1&lt;/id&gt;&lt;/delete&gt; - 删除查询到的索引数据&lt;*:*表示全部&gt; 123&lt;delete&gt; &lt;query&gt;*:*&lt;/query&gt;&lt;/delete&gt; 查询索引 q- 查询字符串 域名:条件 fq- filter query 过滤查询 sort- 排序 start,rows- 分页查询 fl- 返回指定字段内容，用逗号或空格分割 df- default Feild 默认域 wt- writer type 指定输出格式 hl- 是否高亮 使用SolrJ管理索引库使用客户端操作Solr比较繁琐低效，于是有SolrJ通过JAVA来访问Solr客户端 添加/更新文档 solrJ、slf4j-log4j12、jul-to-slf4j、jcl-over-slf4j1234567891011SolrServer solerServer = new HttpSolrServer(&quot;https://localhost:8080/solr&quot;);SolrInputDocument document = new SolrInputDocument();//这里FieldName在schema.xml已定义//当id已存在，相当于更新document.addField(FieldName,Value);solrServer.add(document);solrServer.commit(); 删除文档12345678SolrServer solrServer = new HttpSolrServer(&quot;http://localhost:8080/solr&quot;);//根据条件删除solrServer.deleteByQuery(&quot;*:*&quot;);solrServer.deleteById(id);solrServer.commit(); 查询文档查询所有12345678910111213141516SolrServer solrServer = new HttpSolrServer(&quot;https://localhost:8080/solr&quot;);SolrQuery query = new SolrQuery();query.setQuery(&quot;*:*&quot;);QueryResponse queryResponse = solrQuery.query(query);SolrDocumentList solrDocumentList = queryResponse.getResults();//查询到的数量solrDocumentList.getNumFound();//遍历查询结果for(SolrDocument solrDocument : solrDocumentList) &#123; solrDocument.get(fieldName);&#125; 复杂查询123456789101112131415161718192021222324252627282930313233343536373839404142SolrServer solrServer = new HttpSolrServer(&quot;http://localhost:8080/solr&quot;);SolrQuery query = new SolrQuery();//查询条件query.setQuery(KeyStr);//过滤条件query.setFilterQueries();//排序条件query.setSort(FieldNmae,ORDER.asc);//分页处理query.setStart(num);query.setRows(num);//结果中域的列表query.setFields(FiledNames);//默认搜索域query.set(FieldName);//高亮显示query.setHighlightField(true);//高亮显示前缀query.setHighlightSimplePre();//高亮显示后缀query.setHighlightSimplePost();//执行查询QueryResponse queryResponse = solrServer.query(query);//取查询结果SolrDocumentList solrDocumentList = queryResponse.getResults();//遍历查询结果for(SolrDocument solrDocument:solrDocumentList)&#123; //取高亮显示 String productName=&quot;&quot;; Map&lt;String,Map&lt;String,List&lt;String&gt;&gt;&gt; highlighting = queryResponse.getHighlighting(); List&lt;String&gt; list = highlighting.get(solrDocument.get(&quot;id&quot;)).get(product_name); //判断是否有高亮内容 if(list!=null)&#123; productName=list.get(0); &#125;else&#123; productName=(String)solrDocument.get(&quot;product_name&quot;); &#125;&#125;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分布式文件系统FastDFS]]></title>
    <url>%2F2017%2F01%2F11%2FFastDFS%2F</url>
    <content type="text"><![CDATA[传统图片上传不能有效应对集群的方式，单独搭建一个图片服务器。可以使用分布式文件系统FastDFS方式。实现服务器的高可用 什么是FastDFSFastDFS是用C语言编写的一款开源的分布式文件系统。FastDFS位互联网量身定制，充分考虑了冗杂备份，负载均衡，线性扩容等机制，并注重高可用、高性能等指标，使用FastDFS很容易搭建一套高性能的文件服务集群提供文件上传、下载等服务 FastDFS架构 FastDFS架构包括Tracker server和Storage server。客户端请求Tracker server进行文件上传、下载，通过tracker server调度最终由Storage server完成文件上传和下载。 Tracker server作用是负载均衡和调度，通过Tracker server在文件上传时可以根据一些策略找到Storage server提供文件上传服务。可以将Tracker称为追踪服务器或调度服务器。 Storage server作用是文件存储，客户端上传的文件最终存储在Storage服务器上，Storage server没有实现自己的文件系统而是利用操作西戎的文件系统来管理文件。可以将storage称为存储服务器. 服务端两个角色： Tracker：管理集群，tracker也可以实现集群。每个tracker节点地位平等。 Storage:实际保存文件。Storage氛围多个组，每个组之间保存的文件是不同的。每个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有主从概念。 文件上传的流程图客户端上传文件后存储服务器将文件ID返回给客户端，此文件ID用于以后访问该文件的索引信息。文件索引信息包括：组名，虚拟磁盘路径，数据两级目录，文件名。 组名：文件上传后所在的storage组名称，在文件上传成功后有storage服务器返回，需要客户端自行保存。 虚拟磁盘路径:storage配置的虚拟路径，与磁盘选项store_path*对应。如果配置了store_path0则是M00，如果配置了store_path1则是M01,以此类推。 数据两级目录：storage服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据文件。 文件名：与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含：源存储服务器IP地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。 文件下载 FastDFS使用上传文件123456789101112//加载配置文件，配置文件内容就是tracker服务的地址ClientGlobal.init(AbsoluteAddress);//创建一个TrackerClient对象TrakcerClient trackerClient = new TrackerClient();//使用TrackerClient对象创建连接，获得一个TrackerServer对象TrackerServer trackerServer = trackerClient.getConnection();//创建一个StorageServer的引用StorageServer storageServer = null;//创建一个StorageClient对象，提供两个Server对象StorageClient storageClient = new StorageClient(trackerServer,storageServer);//使用StorageClient对象上传图片String[] result = storageClient.upload_file(&quot;D:/a.jpg&quot;,&quot;jpg&quot;,null); 使用工具类上传1234//加载配置文件FastDFSClient fastDFSClient = new FastDFSClient(AbsoluteAddress);//上传文件String file = fastDFSClient.uploadFile(AbsoluteAddress); 整合SpringSpringMvc.xml设定文件上传解析器1234567&lt;!-- 定义文件上传解析器 --&gt; &lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- 设定默认编码 --&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;!-- 设定文件上传的最大值,这里设5MB --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;5242888&quot;/&gt; &lt;/bean&gt;]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dubbo]]></title>
    <url>%2F2016%2F11%2F09%2FDubbo%2F</url>
    <content type="text"><![CDATA[Dubbo是阿里的开源分布式服务架构，可通过高性能的RPC实现服务的输出和输入功能。 系统通信如何实现远程通信 WebService：效率不高基于soap协议。 使用Restful形式的服务：http+json。很多项目中应用。如果服务太多，服务之间调用关系混乱，需要治疗服务。 dubbo。使用rpc协议进行远程调用，直接使用socket通信。传输效率高，并且可以统计出系统之间的调用关系、调用次数。 规模架构 单一应用架构 当网站流量很小时，只需一个应用，将所有功能都部署在一起，以减少部署节点和成本。 此时，用于简化增删改差工作量的数据访问框架(ORM)是关键。 垂直应用架构 当访问量逐渐增大，单一应用增加极其带来的加速度越来越小，将应用拆成无不想干的几个应用，以提升效率 此时，用于加速前端页面开发的Web框架(MVC)是关键 分布式服务架构 当垂直应用越来越多，应用之间交互不可避免，将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，使前端应用能更快速的响应多变的市场需求。 此时，用于提高业务服用及整合的分布式服务架构(RPC)是关键。 流动计算架构 当服务越来越多，容量的评估，小服务资源的浪费等问题逐渐显现，此时需增加一个调度中心基于访问压力实时管理集群容量，提高集群利用率。 此时，用于提高极其利用率的**资源调度和治理中心(SOA)是关键。 Dubbo 节点角色说明： Provider:暴露服务的服务提供方 Consumer:调用远程服务的服务消费方 Registry:服务注册与发现的注册中心 Monitor:统计服务的调用次数调和调用时间的监控中心 Container:服务运行容器 调用关系说明： 0.服务容器(Container)负责启动、加载、运行服务提供方(Provider) 1.服务提供方(Provider)在启动时，向注册中心(Registry)注册自己提供的服务。 2.服务消费方(Consumer)在启动时，向注册中心(Registry)订阅自己所需的服务。 3.注册中心(Registry)返回服务提供方(Provider)地址列表给消费者，如果有变化，注册中心将基于长连接推送变更数据给消费方(Consumer). ４。服务消费方(Consumer)，从提供者地址列表中，基于软负载均衡算法，选一台提供方(Provider)进行调用，如果调用失败，再选另一台调用. 5.服务消费者(Consumer)和提供方(Provider)，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心(Monitor)使用方法 Spring配置Dubbo采用全Spring配置方式，透明化接入应用，对应用没有任何API侵入，只需用Spring加载Dubbo的配置即可，Dubbo基于Spring的Schema拓展进行加载。 单一工程中spring的配置 1234&lt;bean id=&quot;xxxService&quot; class=&quot;com.xxxServiceImpl&quot; /&gt;&lt;bean id=&quot;xxxAction&quot; class=&quot;com.xxx.xxxAction&quot;&gt; &lt;property name=&quot;xxxService&quot; ref=&quot;xxxService&quot;&gt;&lt;/bean&gt; 远程服务 将上述local.xml配置拆分成两份，将服务定义部分放在服务提供方remote-provier.xml,将服务引用部分放在服务消费方remote-consumer.xml并在provider增加暴露服务配置&lt;dubbo:service&gt;,在consumer增加引用服务配置&lt;dubbo:reference&gt; 发布服务1234&lt;!-- 和本地服务一样实现远程服务 --&gt;&lt;bean id = &quot;xxxService&quot; class = &quot;com.xxx.xxxServiceImpl&quot;/&gt;&lt;!-- 增加暴露远程服务配置 --&gt;&lt;dubbo:service interface=&quot;com.xxx.xxxService&quot; ref=&quot;xxxService&quot;&gt; 调用服务123456&lt;!-- 增加引用远程服务配置 --&gt;&lt;dubbo:reference id =&quot;xxxService&quot; interface=&quot;com.xxx.xxxService&quot;/&gt;&lt;!-- 和本地服务一样使用远程服务&gt;&lt;bean id =&quot;xxxAction&quot; class=&quot;com.xxx.xxxAction&quot;&gt; &lt;property name=&quot;xxxService&quot; ref=&quot;xxxService&quot;&gt;&lt;/bean&gt; 注册中心(Registry)注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力娇小，使用dubbo2.3.3以上版本，建议使用zookeeper注册中心zookeeper是Apache Hadoop的紫霞公募，是一个属性的目录服务，支持变更推送，适合作为Dubbo服务的注册中心 Zookeeper 安装 安装jdk 上传并解压zookeeper 将conf文件夹下的zoo-sample.cfg复制一份，改名位zoo.cfg 修改zoo.cfg的dataDir属性，指定zookeeper的真实目录 修改zoo.cfg的clientport属性，指定该服务器的zookeeper端口 启动zookeeper:zookeeper/bin/zkServer.sh start 关闭zookeeper:zookeeper/bin/zkServer.sh stop 查看zookeeper:zookeeper/bin/zkServer.sh status 框架整合Dubbo监控中心 https://github.com/alibaba/dubbo下载dubbo将dubbo-admin打成war包 1mvn package -Dmaven.test.skip=true 将war包复制到tomcat/webapps/中。修改/WEB-INF/dubbo.properties。包括地址和用户密码]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis框架小结]]></title>
    <url>%2F2016%2F10%2F31%2FMyBatis%2F</url>
    <content type="text"><![CDATA[学习MyBatis的一些个人总结 MyBatis环境配置 1 导入jar包 2 log4j.properties 3 sqlMapConfig.xml 12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- 先加载property，再加载properties,后加载覆盖--&gt;&lt;properties resource=&quot;db.properties&quot;&gt; &lt;property name=&quot;jdbc.driver&quot; value=&quot;AAA&quot;/&gt;&lt;/properties&gt;&lt;!-- 配置pojo别名--&gt; &lt;typeAliases&gt; &lt;!--&lt;typeAlias type=&quot;com.misaniy.po.User&quot; alias=&quot;user&quot; /&gt;--&gt; &lt;!-- 单个配置typeAlias 扫描包package，不区分大小写 --&gt; &lt;package name=&quot;com.misaniy.po&quot; /&gt;&lt;/typeAliases&gt;&lt;enviroments default=&quot;development&quot;&gt; &lt;enviroment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot; /&gt; &lt;dataSource type=&quot;POOLED&quot;&gt; &lt;/dataSource&gt; &lt;/enviroment&gt;&lt;/enviroments&gt;&lt;mappers&gt; &lt;!-- resource根据classpath查找，url绝对路径,class，接口全限定名（目录相同、名称一致、）、package扫描包 --&gt; &lt;mapper class=&quot;com.misaniy.mapper.UserMapper&quot; /&gt; &lt;mapper resource=&quot;XXX.xml&quot;&gt; &lt;package name=&quot;com.misaniy.mapper&quot; /&gt; &lt;mappers&gt;&lt;/configuration&gt; ####Config.xml配置内容和顺序 properties(属性) settings(全局配置参数) typeAliases(类型别名) typeHandlers(类型处理器) objectFactory(对象工厂) plugins(插件) enviroments（环境集合属性对象） enviroments(环境子属性对象) -transactionManager -dataSource mappers（映射器） 4 PO类5 映射文件xxx.xml 1234567891011121314&lt;!-- &lt;mapper namespace=&quot;test&quot;&gt; 传统--&gt;&lt;mapper namespace=&quot;com.misaniy.UserMapper&quot;/&gt;&lt;!-- 使用Mapper不需要DaoImpl --&gt;&lt;!-- 这里的Type使用的别名，类型由mybatis内部定制，pojo由config配置文件定制--&gt;&lt;select id=&quot;findUserByName&quot; parameterType=&quot;String&quot; resultType=&quot;user&quot;&gt; select * from user where name like &apos;%$&#123;value&#125;%&apos;&lt;/select&gt;&lt;insert id=&quot;insertUser&quot; parameterType=&quot;user&quot;&gt; &lt;selectKey keyProperty=&quot;id&quot; order=&quot;After&quot; resultType=&quot;Integer&quot;&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; insert into user(name) values(#&#123;name&#125;)&lt;/insert&gt;&lt;/mapper&gt; Mybatis解决jdbc的问题数据库链接和创建的频繁操作、sql硬编码、结果集封装到pojo、parameterType对输入参数的规范]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringWebMvc小结]]></title>
    <url>%2F2016%2F10%2F13%2FSpringWebMvc%2F</url>
    <content type="text"><![CDATA[学习SpringWebMvc的个人小结 SpringWebMvc执行流程 用户请求给前端控制器DispatcherServlet 前端控制器给HandlerMapping处理器映射器 处理器映射器根据映射找到Handler处理器 处理器返回ExcutionChain数据给前端控制器 前端控制器找到HandlerAdapter处理器适配器 处理器适配器找到处理器执行后返回ModelAndView给前端控制器 前端控制器把ModelAndView解析位Model和View，把Model赋值给View HandlerMapping ExcutionChain HandlerAdapter ModelAndView 视图解析器 解析Model赋值给View SpringMvc.xml配置文件 扫描Controller &lt; mvc:annotation-driver/&gt; 视图解析器 默认支持的数据类型 request response session Model ModelMap Map 基本数据类型 pojo类(属性名相同) 自定义参数类型 Converter 时间注解方式：pojo属性@DateTimeFormat(pattern=”yyyy-MM-dd”)]]></content>
      <categories>
        <category>分享集</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于我]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[我是谁Misaniy 这个名字是因为她的名字是YUANSIYI，我从中抠了几个字符串，加上我的想念组合的。 上可陪领导逛街拎包吃喝玩乐，下可宅在家里追新番，热衷于研究新菜品和更优雅的代码，也没有忘记工作之余锻炼身体给自己未来投资，有一个非常有眼光的女朋友（这绝不是在夸我），也在努力成为一个优雅的hentai绅士。 热爱生活，热爱科技，爱小米，更爱小米的智能家居体系，有朝一日，我要让家里充满智能，充满GEEK的味道。 这是哪里misaniy.cc这是我的博客小站，也是我的黄金屋。在我的代码之夜上，地上有数不清的咯脚的石子，我摸索着前行，背后总有一个人意味深长地让我带上他们。如果你听过这个故事，你也会像我一样，写这样一个博客，把一路的石子捡起来。 我会不时更新我的博客，在我代码上遇到问题，或者我觉得需要记录的时候 怎么联系我Github:MisaniyEmail:MisaniyWeibo:为这美好的世界献上祝福Location：中国.重庆]]></content>
  </entry>
  <entry>
    <title><![CDATA[生活图]]></title>
    <url>%2Fgallery%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
